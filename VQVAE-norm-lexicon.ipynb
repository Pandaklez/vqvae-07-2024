{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/annkle/Documents/KTH/Github/DWPose/conda_torch_env/bin/python\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import utils\n",
    "import pandas as pd\n",
    "import utils\n",
    "import importlib\n",
    "utils = importlib.reload(utils)\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "import acrh_from_norm_meanpose_nowarmup as arch\n",
    "arch = importlib.reload(arch)\n",
    "import vqvae_run\n",
    "vqvae_run = importlib.reload(vqvae_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.is_available()\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "torch.set_default_device(device)\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 60\n",
    "img_size = (32, 32)  # (width, height) # NOT USED\n",
    "input_dim = 112\n",
    "hidden_dim = 512\n",
    "latent_dim = 16\n",
    "n_embeddings= 512\n",
    "output_dim = 112\n",
    "commitment_beta = 0.30\n",
    "lr = 0.002\n",
    "epochs = 50\n",
    "print_step = 50\n",
    "sequence_length = 30\n",
    "down_t = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import importlib\n",
    "import utils\n",
    "utils = importlib.reload(utils)\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_lexicon_data(sslc_pose, path_to_data, output_dim=112, normalize_by_mean_pose=True, num_workers=0, batch_size=256, sequence_length=30, jsons_path='lexicon_jsons/out', pin_memory=False, shuffle\n",
    "=True, train=False):\n",
    "    \"\"\"\n",
    "    sslc_pose: pandas dataframe; df_train_sign, df_val_sign, df_test_sign\n",
    "    # Loading only corpus data (from zip file)\n",
    "    load up to torch.utils.data.DataLoader\n",
    "    \"\"\"\n",
    "    # for corpus: sslc_pose = pd.read_csv('./SSL_video_eaf/sslc_pose_2.csv', encoding='utf-8')  # sslc_pose_2.csv\n",
    "    len_data = len(sslc_pose)\n",
    "\n",
    "    if path_to_data.endswith(\".zip\"):\n",
    "        print(\"Loading data from zip file\")\n",
    "        dataset = utils.ZipPoseDataset(\n",
    "            path_to_data, in_memory=True,\n",
    "            dtype=torch.float32,\n",
    "            max_length=sequence_length,\n",
    "            df=sslc_pose,\n",
    "            normalize_by_mean_pose=normalize_by_mean_pose\n",
    "        )\n",
    "        #threshold_10 = round(len_data * 0.1)\n",
    "        #training_dataset = dataset.slice(threshold_10, None)  \n",
    "        #test_dataset = dataset.slice(0, threshold_10)\n",
    "        # Shuffle is only slow without in_memory since the zip file is read sequentially\n",
    "        # Reading from multiple workers errors out since the zip file is read sequentially\n",
    "        #if train:\n",
    "        #    dataset = utils.PackedDataset(dataset, max_length=sequence_length, shuffle=shuffle)\n",
    "        loader = DataLoader(\n",
    "            dataset, batch_size=batch_size,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory,\n",
    "            shuffle=True,\n",
    "            generator=torch.Generator(device='mps')\n",
    "        )\n",
    "        #test_loader = DataLoader(\n",
    "        #    test_dataset, batch_size=batch_size,\n",
    "        #    shuffle=False, num_workers=num_workers\n",
    "        #)\n",
    "    else:\n",
    "        train_df, test_df = train_test_split(sslc_pose, test_size=0.2, random_state=42)\n",
    "        training_dataset = utils.PoseDataset2(\n",
    "            df=train_df, root_dir=jsons_path,   # ./SSL_video_eaf/SSLC_poses/\n",
    "            sequence_length=sequence_length, normalize_by_mean_pose=normalize_by_mean_pose\n",
    "        )\n",
    "        test_dataset = utils.PoseDataset2(\n",
    "            df=test_df, root_dir=jsons_path,   # ./SSL_video_eaf/SSLC_poses/\n",
    "            sequence_length=sequence_length, normalize_by_mean_pose=normalize_by_mean_pose\n",
    "        )\n",
    "        kwargs = {'num_workers': num_workers, 'pin_memory': True} \n",
    "        train_loader = DataLoader(dataset=training_dataset, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "        test_loader  = DataLoader(dataset=test_dataset,  batch_size=batch_size, shuffle=False,  **kwargs)\n",
    "    return dataset, loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading DF\n",
    "with open('./lexicon_jsons/sign_data_frames_info_train.pkl', 'rb') as file:  # '../../fmalmb/data/signs_and_phrases/sign_data_frames_info_train.pkl'\n",
    "    df_train_sign = pickle.load(file) \n",
    "with open('./lexicon_jsons/sign_data_frames_info_val.pkl', 'rb') as file:  # '../../fmalmb/data/signs_and_phrases/sign_data_frames_info_val.pkl'\n",
    "    df_val_sign = pickle.load(file) \n",
    "with open('./lexicon_jsons//sign_data_frames_info_test.pkl', 'rb') as file:  # '../../fmalmb/data/signs_and_phrases/sign_data_frames_info_test.pkl'\n",
    "    df_test_sign = pickle.load(file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from zip file\n",
      "ZipPoseDataset @ lexicon_jsons/lexicon_poses_norm_train2.zip with max_length=30, in_memory=True\n",
      "Total files 17129\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n",
      "1550\n",
      "1600\n",
      "1650\n",
      "1700\n",
      "1750\n",
      "1800\n",
      "1850\n",
      "1900\n",
      "1950\n",
      "2000\n",
      "2050\n",
      "2100\n",
      "2150\n",
      "2200\n",
      "2250\n",
      "2300\n",
      "2350\n",
      "2400\n",
      "2450\n",
      "2500\n",
      "2550\n",
      "2600\n",
      "2650\n",
      "2700\n",
      "2750\n",
      "2800\n",
      "2850\n",
      "2900\n",
      "2950\n",
      "3000\n",
      "3050\n",
      "3100\n",
      "3150\n",
      "3200\n",
      "3250\n",
      "3300\n",
      "3350\n",
      "3400\n",
      "3450\n",
      "3500\n",
      "3550\n",
      "3600\n",
      "3650\n",
      "3700\n",
      "3750\n",
      "3800\n",
      "3850\n",
      "3900\n",
      "3950\n",
      "4000\n",
      "4050\n",
      "4100\n",
      "4150\n",
      "4200\n",
      "4250\n",
      "4300\n",
      "4350\n",
      "4400\n",
      "4450\n",
      "4500\n",
      "4550\n",
      "4600\n",
      "4650\n",
      "4700\n",
      "4750\n",
      "4800\n",
      "4850\n",
      "4900\n",
      "4950\n",
      "5000\n",
      "5050\n",
      "5100\n",
      "5150\n",
      "5200\n",
      "5250\n",
      "5300\n",
      "5350\n",
      "5400\n",
      "5450\n",
      "5500\n",
      "5550\n",
      "5600\n",
      "5650\n",
      "5700\n",
      "5750\n",
      "5800\n",
      "5850\n",
      "5900\n",
      "5950\n",
      "6000\n",
      "6050\n",
      "6100\n",
      "6150\n",
      "6200\n",
      "6250\n",
      "6300\n",
      "6350\n",
      "6400\n",
      "6450\n",
      "6500\n",
      "6550\n",
      "6600\n",
      "6650\n",
      "6700\n",
      "6750\n",
      "6800\n",
      "6850\n",
      "6900\n",
      "6950\n",
      "7000\n",
      "7050\n",
      "7100\n",
      "7150\n",
      "7200\n",
      "7250\n",
      "7300\n",
      "7350\n",
      "7400\n",
      "7450\n",
      "7500\n",
      "7550\n",
      "7600\n",
      "7650\n",
      "7700\n",
      "7750\n",
      "7800\n",
      "7850\n",
      "7900\n",
      "7950\n",
      "8000\n",
      "8050\n",
      "8100\n",
      "8150\n",
      "8200\n",
      "8250\n",
      "8300\n",
      "8350\n",
      "8400\n",
      "8450\n",
      "8500\n",
      "8550\n",
      "8600\n",
      "8650\n",
      "8700\n",
      "8750\n",
      "8800\n",
      "8850\n",
      "8900\n",
      "8950\n",
      "9000\n",
      "9050\n",
      "9100\n",
      "9150\n",
      "9200\n",
      "9250\n",
      "9300\n",
      "9350\n",
      "9400\n",
      "9450\n",
      "9500\n",
      "9550\n",
      "9600\n",
      "9650\n",
      "9700\n",
      "9750\n",
      "9800\n",
      "9850\n",
      "9900\n",
      "9950\n",
      "10000\n",
      "Memory used in GB: RSS=1.87, VMS=399.12\n",
      "10050\n",
      "10100\n",
      "10150\n",
      "10200\n",
      "10250\n",
      "10300\n",
      "10350\n",
      "10400\n",
      "10450\n",
      "10500\n",
      "10550\n",
      "10600\n",
      "10650\n",
      "10700\n",
      "10750\n",
      "10800\n",
      "10850\n",
      "10900\n",
      "10950\n",
      "11000\n",
      "11050\n",
      "11100\n",
      "11150\n",
      "11200\n",
      "11250\n",
      "11300\n",
      "11350\n",
      "11400\n",
      "11450\n",
      "11500\n",
      "11550\n",
      "11600\n",
      "11650\n",
      "11700\n",
      "11750\n",
      "11800\n",
      "11850\n",
      "11900\n",
      "11950\n",
      "12000\n",
      "12050\n",
      "12100\n",
      "12150\n",
      "12200\n",
      "12250\n",
      "12300\n",
      "12350\n",
      "12400\n",
      "12450\n",
      "12500\n",
      "12550\n",
      "12600\n",
      "12650\n",
      "12700\n",
      "12750\n",
      "12800\n",
      "12850\n",
      "12900\n",
      "12950\n",
      "13000\n",
      "13050\n",
      "13100\n",
      "13150\n",
      "13200\n",
      "13250\n",
      "13300\n",
      "13350\n",
      "13400\n",
      "13450\n",
      "13500\n",
      "13550\n",
      "13600\n",
      "13650\n",
      "13700\n",
      "13750\n",
      "13800\n",
      "13850\n",
      "13900\n",
      "13950\n",
      "14000\n",
      "14050\n",
      "14100\n",
      "14150\n",
      "14200\n",
      "14250\n",
      "14300\n",
      "14350\n",
      "14400\n",
      "14450\n",
      "14500\n",
      "14550\n",
      "14600\n",
      "14650\n",
      "14700\n",
      "14750\n",
      "14800\n",
      "14850\n",
      "14900\n",
      "14950\n",
      "15000\n",
      "15050\n",
      "15100\n",
      "15150\n",
      "15200\n",
      "15250\n",
      "15300\n",
      "15350\n",
      "15400\n",
      "15450\n",
      "15500\n",
      "15550\n",
      "15600\n",
      "15650\n",
      "15700\n",
      "15750\n",
      "15800\n",
      "15850\n",
      "15900\n",
      "15950\n",
      "16000\n",
      "16050\n",
      "16100\n",
      "16150\n",
      "16200\n",
      "16250\n",
      "16300\n",
      "16350\n",
      "16400\n",
      "16450\n",
      "16500\n",
      "16550\n",
      "16600\n",
      "16650\n",
      "16700\n",
      "16750\n",
      "16800\n",
      "16850\n",
      "16900\n",
      "16950\n",
      "17000\n",
      "17050\n",
      "17100\n",
      "torch.Size([60, 112, 30])\n",
      "torch.Size([60, 112, 30])\n",
      "torch.Size([60, 112, 30])\n",
      "torch.Size([60, 112, 30])\n"
     ]
    }
   ],
   "source": [
    "kwargs = {'num_workers': 0, 'pin_memory': False, 'shuffle': True} \n",
    "\n",
    "sequence_length = 30\n",
    "\n",
    "# corpus jsons path jsons_path='./SSL_video_eaf/SSLC_poses/'\n",
    "# lexicon zip path: lexicon_jsons/lexicon_poses_norm_train.zip\n",
    "\n",
    "training_dataset, train_loader = load_lexicon_data(\n",
    "    df_train_sign, 'lexicon_jsons/lexicon_poses_norm_train2.zip',\n",
    "    normalize_by_mean_pose=True,\n",
    "    batch_size=60,\n",
    "    train=False,\n",
    "    **kwargs\n",
    ")\n",
    "i = 0\n",
    "for batch in train_loader:\n",
    "    print(batch.shape)\n",
    "    i += 1\n",
    "    if i == 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vqvae_arch_MGPTmse_misc_norm_oldquant_1024_32.pth\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "from MotionGPT.mGPT.archs import mgpt_vq\n",
    "\n",
    "import importlib\n",
    "\n",
    "# Reload the module\n",
    "importlib.reload(mgpt_vq)\n",
    "\n",
    "# Re-import the class\n",
    "from MotionGPT.mGPT.archs.mgpt_vq import VQVae, Decoder, Encoder\n",
    "\n",
    "\n",
    "batch_size = 60\n",
    "img_size = (32, 32)  # (width, height) # NOT USED\n",
    "input_dim = 112\n",
    "hidden_dim = 512\n",
    "latent_dim = 32  # 16\n",
    "n_embeddings = 1024  # 512\n",
    "output_dim = 112\n",
    "commitment_beta = 0.30\n",
    "lr = 2e-4\n",
    "epochs = 100\n",
    "print_step = 50\n",
    "sequence_length = 30\n",
    "down_t = 1\n",
    "\n",
    "model = VQVae(down_t=down_t, nfeats=input_dim, code_num=n_embeddings, code_dim=latent_dim, output_emb_width=512).to(device)\n",
    "#quantizer = VQEmbeddingEMA(n_embeddings=512, embedding_dim=512).to(device)\n",
    "#quantizer = arch.QuantizeEMAResetCounter(nb_code=n_embeddings, code_dim=latent_dim, mu=0.99).to(device)\n",
    "quantizer = arch.VQEmbeddingEMA(n_embeddings=n_embeddings, embedding_dim=latent_dim).to(device)\n",
    "model.quantizer = quantizer\n",
    "\n",
    "hyperparameters = {\"arch\": \"MGPTmse\", \"misc\": \"norm_oldquant_1024_32\"}\n",
    "log_dir = \"runs/\" + \"_\".join([f\"{key}={value}\" for key, value in hyperparameters.items()])\n",
    "writer = SummaryWriter(log_dir)\n",
    "model_name = 'vqvae_' + '_'.join([f\"{key}_{value}\" for key, value in hyperparameters.items()])+'.pth'\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from zip file\n",
      "ZipPoseDataset @ lexicon_jsons/lexicon_poses_norm_val2.zip with max_length=30, in_memory=True\n",
      "Total files 2141\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n",
      "1550\n",
      "1600\n",
      "1650\n",
      "1700\n",
      "1750\n",
      "1800\n",
      "1850\n",
      "1900\n",
      "1950\n",
      "2000\n",
      "2050\n",
      "2100\n",
      "torch.Size([60, 112, 30])\n",
      "torch.Size([60, 112, 30])\n",
      "torch.Size([60, 112, 30])\n",
      "torch.Size([60, 112, 30])\n"
     ]
    }
   ],
   "source": [
    "# validation loader\n",
    "val_dataset, val_loader = load_lexicon_data(\n",
    "    df_val_sign, 'lexicon_jsons/lexicon_poses_norm_val2.zip',\n",
    "    normalize_by_mean_pose=True, num_workers=0,\n",
    "    batch_size=60\n",
    ")\n",
    "i = 0\n",
    "for batch in val_loader:\n",
    "    print(batch.shape)\n",
    "    i += 1\n",
    "    if i == 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam, AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "def euclidean_loss(output, target):\n",
    "    # print(\"output.shape, target.shape \", output.shape, target.shape)\n",
    "    x = output.transpose(2,1)\n",
    "    y = target.transpose(2,1)\n",
    "    return torch.mean(torch.sqrt(torch.sum((x - y) ** 2, dim=-1)))\n",
    "\n",
    "#optimizer = Adam(model.parameters(), lr=lr)\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "def lr_lambda(epoch):\n",
    "    # Warm-up period (10 epochs) followed by cosine annealing (40 epochs)\n",
    "    if epoch < 10:\n",
    "        # Linearly increase the learning rate during warm-up\n",
    "        return epoch / 10\n",
    "    else:\n",
    "        # Cosine decay\n",
    "        return 0.5 * (1 + torch.cos(torch.tensor((epoch - 10) / (50 - 10) * torch.pi)))\n",
    "    \n",
    "def lr_lambda_nowarmup(epoch):\n",
    "    # Cosine annealing\n",
    "    return 0.5 * (1 + torch.cos(torch.tensor((epoch) / (50) * torch.pi)))\n",
    "\n",
    "scheduler = LambdaLR(optimizer, lr_lambda_nowarmup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training VQ-VAE...\n",
      "epoch: 1 ( batch:  1 / 286 ) recon_loss: 949.17919921875  perplexity:  117.97272491455078  codebook loss:  0.2024366706609726 \n",
      " total_loss:  949.199462890625 \n",
      "\n",
      "epoch: 1 ( batch:  51 / 286 ) recon_loss: 433.789306640625  perplexity:  89.18970489501953  codebook loss:  7.848525524139404 \n",
      " total_loss:  434.57415771484375 \n",
      "\n",
      "epoch: 1 ( batch:  101 / 286 ) recon_loss: 258.5171813964844  perplexity:  108.028076171875  codebook loss:  14.296819686889648 \n",
      " total_loss:  259.9468688964844 \n",
      "\n",
      "epoch: 1 ( batch:  151 / 286 ) recon_loss: 328.41064453125  perplexity:  170.2467803955078  codebook loss:  66.25802612304688 \n",
      " total_loss:  335.03643798828125 \n",
      "\n",
      "epoch: 1 ( batch:  201 / 286 ) recon_loss: 287.8243713378906  perplexity:  185.11793518066406  codebook loss:  59.26470184326172 \n",
      " total_loss:  293.7508544921875 \n",
      "\n",
      "epoch: 1 ( batch:  251 / 286 ) recon_loss: 270.1352233886719  perplexity:  259.2598571777344  codebook loss:  127.83909606933594 \n",
      " total_loss:  282.91912841796875 \n",
      "\n",
      "Validation - epoch: 1  average recon_loss: 2.7326486110687256\n",
      "Are there any dead codes on this epoch?  69\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 2 ( batch:  1 / 286 ) recon_loss: 310.31146240234375  perplexity:  342.0474548339844  codebook loss:  316.64306640625 \n",
      " total_loss:  341.97576904296875 \n",
      "\n",
      "epoch: 2 ( batch:  51 / 286 ) recon_loss: 238.54928588867188  perplexity:  335.1485290527344  codebook loss:  196.05459594726562 \n",
      " total_loss:  258.1547546386719 \n",
      "\n",
      "epoch: 2 ( batch:  101 / 286 ) recon_loss: 315.72979736328125  perplexity:  407.8111572265625  codebook loss:  459.5703430175781 \n",
      " total_loss:  361.68682861328125 \n",
      "\n",
      "epoch: 2 ( batch:  151 / 286 ) recon_loss: 234.36016845703125  perplexity:  365.46893310546875  codebook loss:  255.660400390625 \n",
      " total_loss:  259.92620849609375 \n",
      "\n",
      "epoch: 2 ( batch:  201 / 286 ) recon_loss: 238.7288055419922  perplexity:  406.52081298828125  codebook loss:  287.87615966796875 \n",
      " total_loss:  267.51641845703125 \n",
      "\n",
      "epoch: 2 ( batch:  251 / 286 ) recon_loss: 248.02024841308594  perplexity:  384.0827941894531  codebook loss:  490.08013916015625 \n",
      " total_loss:  297.02825927734375 \n",
      "\n",
      "Validation - epoch: 2  average recon_loss: 2.5658045642905765\n",
      "Are there any dead codes on this epoch?  26\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 3 ( batch:  1 / 286 ) recon_loss: 248.14825439453125  perplexity:  409.2156066894531  codebook loss:  501.29254150390625 \n",
      " total_loss:  298.2774963378906 \n",
      "\n",
      "epoch: 3 ( batch:  51 / 286 ) recon_loss: 202.07846069335938  perplexity:  417.0905456542969  codebook loss:  314.4668884277344 \n",
      " total_loss:  233.525146484375 \n",
      "\n",
      "epoch: 3 ( batch:  101 / 286 ) recon_loss: 239.96475219726562  perplexity:  460.43585205078125  codebook loss:  491.9979248046875 \n",
      " total_loss:  289.16455078125 \n",
      "\n",
      "epoch: 3 ( batch:  151 / 286 ) recon_loss: 276.475830078125  perplexity:  477.5791015625  codebook loss:  445.45672607421875 \n",
      " total_loss:  321.0215148925781 \n",
      "\n",
      "epoch: 3 ( batch:  201 / 286 ) recon_loss: 273.8876037597656  perplexity:  433.1685791015625  codebook loss:  538.0719604492188 \n",
      " total_loss:  327.6947937011719 \n",
      "\n",
      "epoch: 3 ( batch:  251 / 286 ) recon_loss: 178.55308532714844  perplexity:  386.5494079589844  codebook loss:  355.61181640625 \n",
      " total_loss:  214.11427307128906 \n",
      "\n",
      "Validation - epoch: 3  average recon_loss: 2.21754072772132\n",
      "Are there any dead codes on this epoch?  36\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 4 ( batch:  1 / 286 ) recon_loss: 197.76683044433594  perplexity:  428.2159118652344  codebook loss:  470.23809814453125 \n",
      " total_loss:  244.7906494140625 \n",
      "\n",
      "epoch: 4 ( batch:  51 / 286 ) recon_loss: 210.5247802734375  perplexity:  421.2322082519531  codebook loss:  383.254150390625 \n",
      " total_loss:  248.85018920898438 \n",
      "\n",
      "epoch: 4 ( batch:  101 / 286 ) recon_loss: 229.9225616455078  perplexity:  461.73193359375  codebook loss:  535.449462890625 \n",
      " total_loss:  283.4674987792969 \n",
      "\n",
      "epoch: 4 ( batch:  151 / 286 ) recon_loss: 201.7627716064453  perplexity:  444.28936767578125  codebook loss:  437.483642578125 \n",
      " total_loss:  245.51113891601562 \n",
      "\n",
      "epoch: 4 ( batch:  201 / 286 ) recon_loss: 177.78451538085938  perplexity:  447.21728515625  codebook loss:  431.19219970703125 \n",
      " total_loss:  220.9037322998047 \n",
      "\n",
      "epoch: 4 ( batch:  251 / 286 ) recon_loss: 186.40884399414062  perplexity:  459.53350830078125  codebook loss:  410.2266845703125 \n",
      " total_loss:  227.4315185546875 \n",
      "\n",
      "Validation - epoch: 4  average recon_loss: 2.1123721069759793\n",
      "Are there any dead codes on this epoch?  42\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 5 ( batch:  1 / 286 ) recon_loss: 216.19911193847656  perplexity:  475.8074951171875  codebook loss:  563.3095703125 \n",
      " total_loss:  272.5300598144531 \n",
      "\n",
      "epoch: 5 ( batch:  51 / 286 ) recon_loss: 248.24266052246094  perplexity:  508.6645202636719  codebook loss:  627.3465576171875 \n",
      " total_loss:  310.9773254394531 \n",
      "\n",
      "epoch: 5 ( batch:  101 / 286 ) recon_loss: 217.53994750976562  perplexity:  513.580078125  codebook loss:  543.38916015625 \n",
      " total_loss:  271.8788757324219 \n",
      "\n",
      "epoch: 5 ( batch:  151 / 286 ) recon_loss: 239.45639038085938  perplexity:  542.7847900390625  codebook loss:  581.852294921875 \n",
      " total_loss:  297.6416320800781 \n",
      "\n",
      "epoch: 5 ( batch:  201 / 286 ) recon_loss: 159.9630889892578  perplexity:  443.6955261230469  codebook loss:  410.44476318359375 \n",
      " total_loss:  201.007568359375 \n",
      "\n",
      "epoch: 5 ( batch:  251 / 286 ) recon_loss: 276.415283203125  perplexity:  545.6385498046875  codebook loss:  699.8809204101562 \n",
      " total_loss:  346.40338134765625 \n",
      "\n",
      "Validation - epoch: 5  average recon_loss: 2.040891253285938\n",
      "Are there any dead codes on this epoch?  37\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 6 ( batch:  1 / 286 ) recon_loss: 217.81597900390625  perplexity:  471.8609924316406  codebook loss:  485.9332275390625 \n",
      " total_loss:  266.4093017578125 \n",
      "\n",
      "epoch: 6 ( batch:  51 / 286 ) recon_loss: 216.54541015625  perplexity:  492.8004150390625  codebook loss:  598.8529052734375 \n",
      " total_loss:  276.4306945800781 \n",
      "\n",
      "epoch: 6 ( batch:  101 / 286 ) recon_loss: 172.489501953125  perplexity:  483.9954528808594  codebook loss:  432.53857421875 \n",
      " total_loss:  215.7433624267578 \n",
      "\n",
      "epoch: 6 ( batch:  151 / 286 ) recon_loss: 197.29478454589844  perplexity:  520.8768920898438  codebook loss:  588.5215454101562 \n",
      " total_loss:  256.1469421386719 \n",
      "\n",
      "epoch: 6 ( batch:  201 / 286 ) recon_loss: 201.04830932617188  perplexity:  552.9473876953125  codebook loss:  549.6006469726562 \n",
      " total_loss:  256.00836181640625 \n",
      "\n",
      "epoch: 6 ( batch:  251 / 286 ) recon_loss: 171.38299560546875  perplexity:  508.19317626953125  codebook loss:  499.2271728515625 \n",
      " total_loss:  221.3057098388672 \n",
      "\n",
      "Validation - epoch: 6  average recon_loss: 1.9963532818688288\n",
      "Are there any dead codes on this epoch?  29\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 7 ( batch:  1 / 286 ) recon_loss: 238.43910217285156  perplexity:  549.9754638671875  codebook loss:  602.0068359375 \n",
      " total_loss:  298.6397705078125 \n",
      "\n",
      "epoch: 7 ( batch:  51 / 286 ) recon_loss: 243.68862915039062  perplexity:  553.0204467773438  codebook loss:  588.688720703125 \n",
      " total_loss:  302.5574951171875 \n",
      "\n",
      "epoch: 7 ( batch:  101 / 286 ) recon_loss: 236.4183349609375  perplexity:  559.3623657226562  codebook loss:  603.6251220703125 \n",
      " total_loss:  296.7808532714844 \n",
      "\n",
      "epoch: 7 ( batch:  151 / 286 ) recon_loss: 212.6943817138672  perplexity:  575.2518310546875  codebook loss:  654.4420776367188 \n",
      " total_loss:  278.1385803222656 \n",
      "\n",
      "epoch: 7 ( batch:  201 / 286 ) recon_loss: 157.30137634277344  perplexity:  520.4768676757812  codebook loss:  529.5313720703125 \n",
      " total_loss:  210.2545166015625 \n",
      "\n",
      "epoch: 7 ( batch:  251 / 286 ) recon_loss: 197.98828125  perplexity:  520.6478881835938  codebook loss:  494.9246520996094 \n",
      " total_loss:  247.48074340820312 \n",
      "\n",
      "Validation - epoch: 7  average recon_loss: 1.899267892042796\n",
      "Are there any dead codes on this epoch?  33\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 8 ( batch:  1 / 286 ) recon_loss: 181.26414489746094  perplexity:  587.3309326171875  codebook loss:  645.3502807617188 \n",
      " total_loss:  245.79916381835938 \n",
      "\n",
      "epoch: 8 ( batch:  51 / 286 ) recon_loss: 216.23287963867188  perplexity:  592.2172241210938  codebook loss:  648.6363525390625 \n",
      " total_loss:  281.0965270996094 \n",
      "\n",
      "epoch: 8 ( batch:  101 / 286 ) recon_loss: 194.3709716796875  perplexity:  594.3880004882812  codebook loss:  620.201416015625 \n",
      " total_loss:  256.39111328125 \n",
      "\n",
      "epoch: 8 ( batch:  151 / 286 ) recon_loss: 146.63926696777344  perplexity:  522.8455200195312  codebook loss:  454.5699462890625 \n",
      " total_loss:  192.0962677001953 \n",
      "\n",
      "epoch: 8 ( batch:  201 / 286 ) recon_loss: 137.0748748779297  perplexity:  523.8631591796875  codebook loss:  479.30316162109375 \n",
      " total_loss:  185.00518798828125 \n",
      "\n",
      "epoch: 8 ( batch:  251 / 286 ) recon_loss: 135.61370849609375  perplexity:  513.8845825195312  codebook loss:  427.4309997558594 \n",
      " total_loss:  178.3568115234375 \n",
      "\n",
      "Validation - epoch: 8  average recon_loss: 1.804323371913698\n",
      "Are there any dead codes on this epoch?  29\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 9 ( batch:  1 / 286 ) recon_loss: 192.56411743164062  perplexity:  563.2510986328125  codebook loss:  551.7650146484375 \n",
      " total_loss:  247.74061584472656 \n",
      "\n",
      "epoch: 9 ( batch:  51 / 286 ) recon_loss: 170.79171752929688  perplexity:  550.5503540039062  codebook loss:  565.6434936523438 \n",
      " total_loss:  227.35606384277344 \n",
      "\n",
      "epoch: 9 ( batch:  101 / 286 ) recon_loss: 142.52694702148438  perplexity:  538.7960815429688  codebook loss:  522.5205688476562 \n",
      " total_loss:  194.7790069580078 \n",
      "\n",
      "epoch: 9 ( batch:  151 / 286 ) recon_loss: 231.5399169921875  perplexity:  613.2871704101562  codebook loss:  754.744873046875 \n",
      " total_loss:  307.014404296875 \n",
      "\n",
      "epoch: 9 ( batch:  201 / 286 ) recon_loss: 143.16592407226562  perplexity:  572.7293701171875  codebook loss:  546.4179077148438 \n",
      " total_loss:  197.80770874023438 \n",
      "\n",
      "epoch: 9 ( batch:  251 / 286 ) recon_loss: 194.55809020996094  perplexity:  607.19287109375  codebook loss:  660.4530029296875 \n",
      " total_loss:  260.6033935546875 \n",
      "\n",
      "Validation - epoch: 9  average recon_loss: 1.7622113327185314\n",
      "Are there any dead codes on this epoch?  31\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 10 ( batch:  1 / 286 ) recon_loss: 191.655517578125  perplexity:  593.843505859375  codebook loss:  712.6976318359375 \n",
      " total_loss:  262.92529296875 \n",
      "\n",
      "epoch: 10 ( batch:  51 / 286 ) recon_loss: 212.5947723388672  perplexity:  620.364013671875  codebook loss:  748.3615112304688 \n",
      " total_loss:  287.430908203125 \n",
      "\n",
      "epoch: 10 ( batch:  101 / 286 ) recon_loss: 154.6295623779297  perplexity:  571.5947265625  codebook loss:  628.7662353515625 \n",
      " total_loss:  217.50619506835938 \n",
      "\n",
      "epoch: 10 ( batch:  151 / 286 ) recon_loss: 209.55235290527344  perplexity:  640.3281860351562  codebook loss:  819.4332275390625 \n",
      " total_loss:  291.49566650390625 \n",
      "\n",
      "epoch: 10 ( batch:  201 / 286 ) recon_loss: 146.11837768554688  perplexity:  562.599853515625  codebook loss:  545.3491821289062 \n",
      " total_loss:  200.65328979492188 \n",
      "\n",
      "epoch: 10 ( batch:  251 / 286 ) recon_loss: 177.59014892578125  perplexity:  584.4238891601562  codebook loss:  615.267578125 \n",
      " total_loss:  239.11691284179688 \n",
      "\n",
      "Validation - epoch: 10  average recon_loss: 1.7011138366328344\n",
      "Are there any dead codes on this epoch?  22\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 11 ( batch:  1 / 286 ) recon_loss: 188.80311584472656  perplexity:  617.2581176757812  codebook loss:  758.1636962890625 \n",
      " total_loss:  264.6194763183594 \n",
      "\n",
      "epoch: 11 ( batch:  51 / 286 ) recon_loss: 199.1337127685547  perplexity:  640.2451782226562  codebook loss:  832.1905517578125 \n",
      " total_loss:  282.352783203125 \n",
      "\n",
      "epoch: 11 ( batch:  101 / 286 ) recon_loss: 194.42703247070312  perplexity:  601.5562133789062  codebook loss:  684.5999755859375 \n",
      " total_loss:  262.88702392578125 \n",
      "\n",
      "epoch: 11 ( batch:  151 / 286 ) recon_loss: 157.99508666992188  perplexity:  611.7476806640625  codebook loss:  651.1175537109375 \n",
      " total_loss:  223.10684204101562 \n",
      "\n",
      "epoch: 11 ( batch:  201 / 286 ) recon_loss: 137.6345977783203  perplexity:  591.7867431640625  codebook loss:  584.3412475585938 \n",
      " total_loss:  196.0687255859375 \n",
      "\n",
      "epoch: 11 ( batch:  251 / 286 ) recon_loss: 157.8807373046875  perplexity:  613.8263549804688  codebook loss:  723.4963989257812 \n",
      " total_loss:  230.23037719726562 \n",
      "\n",
      "Validation - epoch: 11  average recon_loss: 1.6734239723947313\n",
      "Are there any dead codes on this epoch?  18\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 12 ( batch:  1 / 286 ) recon_loss: 200.43386840820312  perplexity:  596.537353515625  codebook loss:  681.3724365234375 \n",
      " total_loss:  268.57110595703125 \n",
      "\n",
      "epoch: 12 ( batch:  51 / 286 ) recon_loss: 201.7134246826172  perplexity:  649.01904296875  codebook loss:  775.472900390625 \n",
      " total_loss:  279.2607116699219 \n",
      "\n",
      "epoch: 12 ( batch:  101 / 286 ) recon_loss: 188.64724731445312  perplexity:  634.672607421875  codebook loss:  817.1748046875 \n",
      " total_loss:  270.36474609375 \n",
      "\n",
      "epoch: 12 ( batch:  151 / 286 ) recon_loss: 169.8133087158203  perplexity:  614.8704223632812  codebook loss:  699.5614624023438 \n",
      " total_loss:  239.7694549560547 \n",
      "\n",
      "epoch: 12 ( batch:  201 / 286 ) recon_loss: 203.42071533203125  perplexity:  618.930419921875  codebook loss:  717.2073974609375 \n",
      " total_loss:  275.1414489746094 \n",
      "\n",
      "epoch: 12 ( batch:  251 / 286 ) recon_loss: 186.8895721435547  perplexity:  667.566650390625  codebook loss:  763.2696533203125 \n",
      " total_loss:  263.216552734375 \n",
      "\n",
      "Validation - epoch: 12  average recon_loss: 1.6106121705638037\n",
      "Are there any dead codes on this epoch?  8\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 13 ( batch:  1 / 286 ) recon_loss: 187.0294952392578  perplexity:  692.9210205078125  codebook loss:  812.0599365234375 \n",
      " total_loss:  268.2354736328125 \n",
      "\n",
      "epoch: 13 ( batch:  51 / 286 ) recon_loss: 161.9077606201172  perplexity:  672.8743286132812  codebook loss:  727.4752807617188 \n",
      " total_loss:  234.65528869628906 \n",
      "\n",
      "epoch: 13 ( batch:  101 / 286 ) recon_loss: 141.22396850585938  perplexity:  622.0892333984375  codebook loss:  620.9611206054688 \n",
      " total_loss:  203.32008361816406 \n",
      "\n",
      "epoch: 13 ( batch:  151 / 286 ) recon_loss: 144.10362243652344  perplexity:  617.84765625  codebook loss:  739.294189453125 \n",
      " total_loss:  218.03305053710938 \n",
      "\n",
      "epoch: 13 ( batch:  201 / 286 ) recon_loss: 136.9658660888672  perplexity:  613.5460205078125  codebook loss:  593.207275390625 \n",
      " total_loss:  196.28659057617188 \n",
      "\n",
      "epoch: 13 ( batch:  251 / 286 ) recon_loss: 163.0286407470703  perplexity:  640.4671630859375  codebook loss:  646.68505859375 \n",
      " total_loss:  227.6971435546875 \n",
      "\n",
      "Validation - epoch: 13  average recon_loss: 1.601715948846605\n",
      "Are there any dead codes on this epoch?  6\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 14 ( batch:  1 / 286 ) recon_loss: 198.18597412109375  perplexity:  688.0333251953125  codebook loss:  1005.204833984375 \n",
      " total_loss:  298.7064514160156 \n",
      "\n",
      "epoch: 14 ( batch:  51 / 286 ) recon_loss: 158.2811279296875  perplexity:  644.3970947265625  codebook loss:  745.9644165039062 \n",
      " total_loss:  232.8775634765625 \n",
      "\n",
      "epoch: 14 ( batch:  101 / 286 ) recon_loss: 177.25924682617188  perplexity:  678.1895751953125  codebook loss:  922.3408203125 \n",
      " total_loss:  269.49334716796875 \n",
      "\n",
      "epoch: 14 ( batch:  151 / 286 ) recon_loss: 173.2097930908203  perplexity:  672.1348266601562  codebook loss:  637.890380859375 \n",
      " total_loss:  236.99884033203125 \n",
      "\n",
      "epoch: 14 ( batch:  201 / 286 ) recon_loss: 179.35704040527344  perplexity:  678.0657348632812  codebook loss:  761.728515625 \n",
      " total_loss:  255.52989196777344 \n",
      "\n",
      "epoch: 14 ( batch:  251 / 286 ) recon_loss: 124.34065246582031  perplexity:  648.81103515625  codebook loss:  661.04052734375 \n",
      " total_loss:  190.4447021484375 \n",
      "\n",
      "Validation - epoch: 14  average recon_loss: 1.5477370056841109\n",
      "Are there any dead codes on this epoch?  2\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 15 ( batch:  1 / 286 ) recon_loss: 151.4922637939453  perplexity:  654.3089599609375  codebook loss:  749.1156616210938 \n",
      " total_loss:  226.40383911132812 \n",
      "\n",
      "epoch: 15 ( batch:  51 / 286 ) recon_loss: 182.77850341796875  perplexity:  695.6207275390625  codebook loss:  991.1703491210938 \n",
      " total_loss:  281.8955383300781 \n",
      "\n",
      "epoch: 15 ( batch:  101 / 286 ) recon_loss: 177.1737060546875  perplexity:  682.8809204101562  codebook loss:  777.2402954101562 \n",
      " total_loss:  254.89773559570312 \n",
      "\n",
      "epoch: 15 ( batch:  151 / 286 ) recon_loss: 171.33001708984375  perplexity:  667.135498046875  codebook loss:  796.9244995117188 \n",
      " total_loss:  251.0224609375 \n",
      "\n",
      "epoch: 15 ( batch:  201 / 286 ) recon_loss: 160.7598114013672  perplexity:  679.8994750976562  codebook loss:  750.7548828125 \n",
      " total_loss:  235.83529663085938 \n",
      "\n",
      "epoch: 15 ( batch:  251 / 286 ) recon_loss: 125.83344268798828  perplexity:  635.88037109375  codebook loss:  613.8489990234375 \n",
      " total_loss:  187.2183380126953 \n",
      "\n",
      "Validation - epoch: 15  average recon_loss: 1.4860845175054338\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 16 ( batch:  1 / 286 ) recon_loss: 133.41726684570312  perplexity:  646.34814453125  codebook loss:  635.1393432617188 \n",
      " total_loss:  196.9311981201172 \n",
      "\n",
      "epoch: 16 ( batch:  51 / 286 ) recon_loss: 150.68675231933594  perplexity:  654.33740234375  codebook loss:  673.68408203125 \n",
      " total_loss:  218.05516052246094 \n",
      "\n",
      "epoch: 16 ( batch:  101 / 286 ) recon_loss: 121.60604858398438  perplexity:  674.5943603515625  codebook loss:  738.3416748046875 \n",
      " total_loss:  195.44021606445312 \n",
      "\n",
      "epoch: 16 ( batch:  151 / 286 ) recon_loss: 132.90267944335938  perplexity:  664.1617431640625  codebook loss:  727.0074462890625 \n",
      " total_loss:  205.60342407226562 \n",
      "\n",
      "epoch: 16 ( batch:  201 / 286 ) recon_loss: 158.7534637451172  perplexity:  681.7890014648438  codebook loss:  764.4517822265625 \n",
      " total_loss:  235.19863891601562 \n",
      "\n",
      "epoch: 16 ( batch:  251 / 286 ) recon_loss: 125.52413940429688  perplexity:  679.9569091796875  codebook loss:  671.9849853515625 \n",
      " total_loss:  192.72264099121094 \n",
      "\n",
      "Validation - epoch: 16  average recon_loss: 1.431922670867708\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 17 ( batch:  1 / 286 ) recon_loss: 120.87986755371094  perplexity:  647.7001342773438  codebook loss:  633.9439697265625 \n",
      " total_loss:  184.27426147460938 \n",
      "\n",
      "epoch: 17 ( batch:  51 / 286 ) recon_loss: 143.02642822265625  perplexity:  694.4080810546875  codebook loss:  773.4734497070312 \n",
      " total_loss:  220.373779296875 \n",
      "\n",
      "epoch: 17 ( batch:  101 / 286 ) recon_loss: 191.0705108642578  perplexity:  754.99267578125  codebook loss:  1007.506103515625 \n",
      " total_loss:  291.82110595703125 \n",
      "\n",
      "epoch: 17 ( batch:  151 / 286 ) recon_loss: 143.19435119628906  perplexity:  663.8922729492188  codebook loss:  693.9925537109375 \n",
      " total_loss:  212.59359741210938 \n",
      "\n",
      "epoch: 17 ( batch:  201 / 286 ) recon_loss: 122.83909606933594  perplexity:  651.1681518554688  codebook loss:  607.146484375 \n",
      " total_loss:  183.55374145507812 \n",
      "\n",
      "epoch: 17 ( batch:  251 / 286 ) recon_loss: 135.28973388671875  perplexity:  677.2960205078125  codebook loss:  679.24169921875 \n",
      " total_loss:  203.21389770507812 \n",
      "\n",
      "Validation - epoch: 17  average recon_loss: 1.400235073433982\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 18 ( batch:  1 / 286 ) recon_loss: 125.6962661743164  perplexity:  670.3053588867188  codebook loss:  650.6597900390625 \n",
      " total_loss:  190.76223754882812 \n",
      "\n",
      "epoch: 18 ( batch:  51 / 286 ) recon_loss: 159.1237335205078  perplexity:  720.6083984375  codebook loss:  814.7674560546875 \n",
      " total_loss:  240.60047912597656 \n",
      "\n",
      "epoch: 18 ( batch:  101 / 286 ) recon_loss: 145.44456481933594  perplexity:  681.3882446289062  codebook loss:  789.3472900390625 \n",
      " total_loss:  224.37930297851562 \n",
      "\n",
      "epoch: 18 ( batch:  151 / 286 ) recon_loss: 161.35108947753906  perplexity:  725.0521850585938  codebook loss:  994.8324584960938 \n",
      " total_loss:  260.8343505859375 \n",
      "\n",
      "epoch: 18 ( batch:  201 / 286 ) recon_loss: 110.17452239990234  perplexity:  666.7137451171875  codebook loss:  640.9111328125 \n",
      " total_loss:  174.26564025878906 \n",
      "\n",
      "epoch: 18 ( batch:  251 / 286 ) recon_loss: 143.95541381835938  perplexity:  679.2630615234375  codebook loss:  709.1828002929688 \n",
      " total_loss:  214.87368774414062 \n",
      "\n",
      "Validation - epoch: 18  average recon_loss: 1.3987810479270086\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 19 ( batch:  1 / 286 ) recon_loss: 137.68247985839844  perplexity:  684.5863647460938  codebook loss:  714.5321655273438 \n",
      " total_loss:  209.1356964111328 \n",
      "\n",
      "epoch: 19 ( batch:  51 / 286 ) recon_loss: 135.38705444335938  perplexity:  683.9351196289062  codebook loss:  779.4601440429688 \n",
      " total_loss:  213.33306884765625 \n",
      "\n",
      "epoch: 19 ( batch:  101 / 286 ) recon_loss: 110.09404754638672  perplexity:  671.4429321289062  codebook loss:  655.5856323242188 \n",
      " total_loss:  175.65261840820312 \n",
      "\n",
      "epoch: 19 ( batch:  151 / 286 ) recon_loss: 108.44237518310547  perplexity:  656.1051025390625  codebook loss:  700.68701171875 \n",
      " total_loss:  178.51107788085938 \n",
      "\n",
      "epoch: 19 ( batch:  201 / 286 ) recon_loss: 137.94215393066406  perplexity:  700.098388671875  codebook loss:  775.56005859375 \n",
      " total_loss:  215.4981689453125 \n",
      "\n",
      "epoch: 19 ( batch:  251 / 286 ) recon_loss: 160.93458557128906  perplexity:  752.6182250976562  codebook loss:  891.3021240234375 \n",
      " total_loss:  250.06478881835938 \n",
      "\n",
      "Validation - epoch: 19  average recon_loss: 1.3654972314834595\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 20 ( batch:  1 / 286 ) recon_loss: 116.29629516601562  perplexity:  666.3060302734375  codebook loss:  691.3896484375 \n",
      " total_loss:  185.43527221679688 \n",
      "\n",
      "epoch: 20 ( batch:  51 / 286 ) recon_loss: 147.67222595214844  perplexity:  737.6988525390625  codebook loss:  783.983154296875 \n",
      " total_loss:  226.07054138183594 \n",
      "\n",
      "epoch: 20 ( batch:  101 / 286 ) recon_loss: 187.46343994140625  perplexity:  721.0204467773438  codebook loss:  954.6282958984375 \n",
      " total_loss:  282.92626953125 \n",
      "\n",
      "epoch: 20 ( batch:  151 / 286 ) recon_loss: 145.2601318359375  perplexity:  712.1810913085938  codebook loss:  865.7613525390625 \n",
      " total_loss:  231.83627319335938 \n",
      "\n",
      "epoch: 20 ( batch:  201 / 286 ) recon_loss: 141.8871307373047  perplexity:  679.3408203125  codebook loss:  787.1484375 \n",
      " total_loss:  220.6019744873047 \n",
      "\n",
      "epoch: 20 ( batch:  251 / 286 ) recon_loss: 143.23365783691406  perplexity:  709.2756958007812  codebook loss:  882.0762939453125 \n",
      " total_loss:  231.4412841796875 \n",
      "\n",
      "Validation - epoch: 20  average recon_loss: 1.3632468316290114\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 21 ( batch:  1 / 286 ) recon_loss: 161.70852661132812  perplexity:  729.5951538085938  codebook loss:  904.2413940429688 \n",
      " total_loss:  252.13265991210938 \n",
      "\n",
      "epoch: 21 ( batch:  51 / 286 ) recon_loss: 124.82384490966797  perplexity:  703.3407592773438  codebook loss:  786.6305541992188 \n",
      " total_loss:  203.48690795898438 \n",
      "\n",
      "epoch: 21 ( batch:  101 / 286 ) recon_loss: 136.19253540039062  perplexity:  706.2716674804688  codebook loss:  792.0755615234375 \n",
      " total_loss:  215.40008544921875 \n",
      "\n",
      "epoch: 21 ( batch:  151 / 286 ) recon_loss: 125.69390869140625  perplexity:  719.7577514648438  codebook loss:  812.3159790039062 \n",
      " total_loss:  206.92550659179688 \n",
      "\n",
      "epoch: 21 ( batch:  201 / 286 ) recon_loss: 162.01393127441406  perplexity:  757.6585083007812  codebook loss:  959.9305419921875 \n",
      " total_loss:  258.0069885253906 \n",
      "\n",
      "epoch: 21 ( batch:  251 / 286 ) recon_loss: 113.25367736816406  perplexity:  696.3345947265625  codebook loss:  755.7317504882812 \n",
      " total_loss:  188.82684326171875 \n",
      "\n",
      "Validation - epoch: 21  average recon_loss: 1.3478239095873303\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 22 ( batch:  1 / 286 ) recon_loss: 95.4699478149414  perplexity:  657.7069702148438  codebook loss:  699.1234130859375 \n",
      " total_loss:  165.38229370117188 \n",
      "\n",
      "epoch: 22 ( batch:  51 / 286 ) recon_loss: 138.6339111328125  perplexity:  728.221923828125  codebook loss:  950.2826538085938 \n",
      " total_loss:  233.66217041015625 \n",
      "\n",
      "epoch: 22 ( batch:  101 / 286 ) recon_loss: 126.22949981689453  perplexity:  670.0350341796875  codebook loss:  664.685546875 \n",
      " total_loss:  192.69805908203125 \n",
      "\n",
      "epoch: 22 ( batch:  151 / 286 ) recon_loss: 101.84661102294922  perplexity:  658.470458984375  codebook loss:  686.8283081054688 \n",
      " total_loss:  170.52944946289062 \n",
      "\n",
      "epoch: 22 ( batch:  201 / 286 ) recon_loss: 111.003173828125  perplexity:  704.6512451171875  codebook loss:  796.4229736328125 \n",
      " total_loss:  190.64547729492188 \n",
      "\n",
      "epoch: 22 ( batch:  251 / 286 ) recon_loss: 112.76679992675781  perplexity:  698.8369750976562  codebook loss:  737.0252075195312 \n",
      " total_loss:  186.46932983398438 \n",
      "\n",
      "Validation - epoch: 22  average recon_loss: 1.349737223651674\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 23 ( batch:  1 / 286 ) recon_loss: 141.7831268310547  perplexity:  756.7876586914062  codebook loss:  1028.3665771484375 \n",
      " total_loss:  244.61978149414062 \n",
      "\n",
      "epoch: 23 ( batch:  51 / 286 ) recon_loss: 136.04771423339844  perplexity:  728.9949340820312  codebook loss:  950.3824462890625 \n",
      " total_loss:  231.08596801757812 \n",
      "\n",
      "epoch: 23 ( batch:  101 / 286 ) recon_loss: 145.63172912597656  perplexity:  725.9257202148438  codebook loss:  816.9005737304688 \n",
      " total_loss:  227.32177734375 \n",
      "\n",
      "epoch: 23 ( batch:  151 / 286 ) recon_loss: 153.07901000976562  perplexity:  745.31201171875  codebook loss:  968.96533203125 \n",
      " total_loss:  249.97555541992188 \n",
      "\n",
      "epoch: 23 ( batch:  201 / 286 ) recon_loss: 129.57659912109375  perplexity:  732.168212890625  codebook loss:  887.61279296875 \n",
      " total_loss:  218.337890625 \n",
      "\n",
      "epoch: 23 ( batch:  251 / 286 ) recon_loss: 148.78558349609375  perplexity:  747.39892578125  codebook loss:  907.577392578125 \n",
      " total_loss:  239.5433349609375 \n",
      "\n",
      "Validation - epoch: 23  average recon_loss: 1.247082382440567\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 24 ( batch:  1 / 286 ) recon_loss: 115.2887954711914  perplexity:  694.9221801757812  codebook loss:  810.0878295898438 \n",
      " total_loss:  196.29757690429688 \n",
      "\n",
      "epoch: 24 ( batch:  51 / 286 ) recon_loss: 117.64515686035156  perplexity:  711.9451293945312  codebook loss:  779.3804931640625 \n",
      " total_loss:  195.5832061767578 \n",
      "\n",
      "epoch: 24 ( batch:  101 / 286 ) recon_loss: 116.05021667480469  perplexity:  713.2451782226562  codebook loss:  780.9075317382812 \n",
      " total_loss:  194.14096069335938 \n",
      "\n",
      "epoch: 24 ( batch:  151 / 286 ) recon_loss: 109.82727813720703  perplexity:  660.3331298828125  codebook loss:  699.1397094726562 \n",
      " total_loss:  179.74124145507812 \n",
      "\n",
      "epoch: 24 ( batch:  201 / 286 ) recon_loss: 125.00756072998047  perplexity:  725.640869140625  codebook loss:  867.4613037109375 \n",
      " total_loss:  211.75369262695312 \n",
      "\n",
      "epoch: 24 ( batch:  251 / 286 ) recon_loss: 113.26268768310547  perplexity:  704.2213134765625  codebook loss:  746.2134399414062 \n",
      " total_loss:  187.884033203125 \n",
      "\n",
      "Validation - epoch: 24  average recon_loss: 1.2732245938645468\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 25 ( batch:  1 / 286 ) recon_loss: 99.59281158447266  perplexity:  675.1014404296875  codebook loss:  721.40185546875 \n",
      " total_loss:  171.73300170898438 \n",
      "\n",
      "epoch: 25 ( batch:  51 / 286 ) recon_loss: 90.36384582519531  perplexity:  682.9072875976562  codebook loss:  689.3576049804688 \n",
      " total_loss:  159.2996063232422 \n",
      "\n",
      "epoch: 25 ( batch:  101 / 286 ) recon_loss: 108.7509765625  perplexity:  737.0100708007812  codebook loss:  825.9990844726562 \n",
      " total_loss:  191.35089111328125 \n",
      "\n",
      "epoch: 25 ( batch:  151 / 286 ) recon_loss: 115.55290222167969  perplexity:  713.4458618164062  codebook loss:  807.0130615234375 \n",
      " total_loss:  196.25421142578125 \n",
      "\n",
      "epoch: 25 ( batch:  201 / 286 ) recon_loss: 99.96369171142578  perplexity:  705.4366455078125  codebook loss:  770.5768432617188 \n",
      " total_loss:  177.02137756347656 \n",
      "\n",
      "epoch: 25 ( batch:  251 / 286 ) recon_loss: 126.27568054199219  perplexity:  737.9240112304688  codebook loss:  868.9459228515625 \n",
      " total_loss:  213.17027282714844 \n",
      "\n",
      "Validation - epoch: 25  average recon_loss: 1.2671759459707472\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 26 ( batch:  1 / 286 ) recon_loss: 113.5475845336914  perplexity:  717.7904052734375  codebook loss:  751.1322021484375 \n",
      " total_loss:  188.66079711914062 \n",
      "\n",
      "epoch: 26 ( batch:  51 / 286 ) recon_loss: 106.17294311523438  perplexity:  688.2840576171875  codebook loss:  737.39013671875 \n",
      " total_loss:  179.91195678710938 \n",
      "\n",
      "epoch: 26 ( batch:  101 / 286 ) recon_loss: 124.67019653320312  perplexity:  737.4252319335938  codebook loss:  870.1171875 \n",
      " total_loss:  211.68191528320312 \n",
      "\n",
      "epoch: 26 ( batch:  151 / 286 ) recon_loss: 136.36849975585938  perplexity:  729.8279418945312  codebook loss:  823.3587036132812 \n",
      " total_loss:  218.70437622070312 \n",
      "\n",
      "epoch: 26 ( batch:  201 / 286 ) recon_loss: 87.32745361328125  perplexity:  688.7430419921875  codebook loss:  708.7874755859375 \n",
      " total_loss:  158.20620727539062 \n",
      "\n",
      "epoch: 26 ( batch:  251 / 286 ) recon_loss: 96.54212951660156  perplexity:  684.07373046875  codebook loss:  705.6436767578125 \n",
      " total_loss:  167.10650634765625 \n",
      "\n",
      "Validation - epoch: 26  average recon_loss: 1.2403827160596848\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 27 ( batch:  1 / 286 ) recon_loss: 145.44195556640625  perplexity:  749.8259887695312  codebook loss:  1020.7938232421875 \n",
      " total_loss:  247.52133178710938 \n",
      "\n",
      "epoch: 27 ( batch:  51 / 286 ) recon_loss: 143.16311645507812  perplexity:  750.1417236328125  codebook loss:  987.2390747070312 \n",
      " total_loss:  241.88702392578125 \n",
      "\n",
      "epoch: 27 ( batch:  101 / 286 ) recon_loss: 141.77818298339844  perplexity:  766.3253784179688  codebook loss:  954.6045532226562 \n",
      " total_loss:  237.2386474609375 \n",
      "\n",
      "epoch: 27 ( batch:  151 / 286 ) recon_loss: 102.28820037841797  perplexity:  718.0786743164062  codebook loss:  788.393798828125 \n",
      " total_loss:  181.12757873535156 \n",
      "\n",
      "epoch: 27 ( batch:  201 / 286 ) recon_loss: 103.44361877441406  perplexity:  698.7996826171875  codebook loss:  759.71337890625 \n",
      " total_loss:  179.41494750976562 \n",
      "\n",
      "epoch: 27 ( batch:  251 / 286 ) recon_loss: 105.12525939941406  perplexity:  683.5025024414062  codebook loss:  695.1587524414062 \n",
      " total_loss:  174.64114379882812 \n",
      "\n",
      "Validation - epoch: 27  average recon_loss: 1.2279393093453512\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 28 ( batch:  1 / 286 ) recon_loss: 104.52372741699219  perplexity:  700.0316772460938  codebook loss:  718.98779296875 \n",
      " total_loss:  176.42251586914062 \n",
      "\n",
      "epoch: 28 ( batch:  51 / 286 ) recon_loss: 108.06420135498047  perplexity:  720.9476318359375  codebook loss:  749.84912109375 \n",
      " total_loss:  183.0491180419922 \n",
      "\n",
      "epoch: 28 ( batch:  101 / 286 ) recon_loss: 124.0331039428711  perplexity:  745.29638671875  codebook loss:  847.156982421875 \n",
      " total_loss:  208.74880981445312 \n",
      "\n",
      "epoch: 28 ( batch:  151 / 286 ) recon_loss: 147.41168212890625  perplexity:  747.7532958984375  codebook loss:  1038.3494873046875 \n",
      " total_loss:  251.24664306640625 \n",
      "\n",
      "epoch: 28 ( batch:  201 / 286 ) recon_loss: 146.00003051757812  perplexity:  746.1085205078125  codebook loss:  1007.4404907226562 \n",
      " total_loss:  246.74407958984375 \n",
      "\n",
      "epoch: 28 ( batch:  251 / 286 ) recon_loss: 125.99247741699219  perplexity:  729.8644409179688  codebook loss:  858.2919921875 \n",
      " total_loss:  211.82168579101562 \n",
      "\n",
      "Validation - epoch: 28  average recon_loss: 1.2121539910634358\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 29 ( batch:  1 / 286 ) recon_loss: 130.00282287597656  perplexity:  740.0643920898438  codebook loss:  873.4957275390625 \n",
      " total_loss:  217.35238647460938 \n",
      "\n",
      "epoch: 29 ( batch:  51 / 286 ) recon_loss: 117.74969482421875  perplexity:  759.517822265625  codebook loss:  943.580810546875 \n",
      " total_loss:  212.1077880859375 \n",
      "\n",
      "epoch: 29 ( batch:  101 / 286 ) recon_loss: 101.26624298095703  perplexity:  706.0208129882812  codebook loss:  721.2947998046875 \n",
      " total_loss:  173.39572143554688 \n",
      "\n",
      "epoch: 29 ( batch:  151 / 286 ) recon_loss: 130.66700744628906  perplexity:  734.501220703125  codebook loss:  862.1844482421875 \n",
      " total_loss:  216.8854522705078 \n",
      "\n",
      "epoch: 29 ( batch:  201 / 286 ) recon_loss: 105.67320251464844  perplexity:  697.324462890625  codebook loss:  736.6473999023438 \n",
      " total_loss:  179.33795166015625 \n",
      "\n",
      "epoch: 29 ( batch:  251 / 286 ) recon_loss: 132.13198852539062  perplexity:  746.7050170898438  codebook loss:  891.8748779296875 \n",
      " total_loss:  221.31948852539062 \n",
      "\n",
      "Validation - epoch: 29  average recon_loss: 1.1836423476537068\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 30 ( batch:  1 / 286 ) recon_loss: 109.61326599121094  perplexity:  727.2744750976562  codebook loss:  788.6695556640625 \n",
      " total_loss:  188.480224609375 \n",
      "\n",
      "epoch: 30 ( batch:  51 / 286 ) recon_loss: 128.19046020507812  perplexity:  754.3154296875  codebook loss:  981.849365234375 \n",
      " total_loss:  226.37539672851562 \n",
      "\n",
      "epoch: 30 ( batch:  101 / 286 ) recon_loss: 94.12910461425781  perplexity:  722.2758178710938  codebook loss:  769.745849609375 \n",
      " total_loss:  171.10369873046875 \n",
      "\n",
      "epoch: 30 ( batch:  151 / 286 ) recon_loss: 101.320556640625  perplexity:  696.0043334960938  codebook loss:  706.2334594726562 \n",
      " total_loss:  171.94390869140625 \n",
      "\n",
      "epoch: 30 ( batch:  201 / 286 ) recon_loss: 113.73238372802734  perplexity:  725.2901000976562  codebook loss:  808.5449829101562 \n",
      " total_loss:  194.58688354492188 \n",
      "\n",
      "epoch: 30 ( batch:  251 / 286 ) recon_loss: 120.77156066894531  perplexity:  748.296875  codebook loss:  849.042724609375 \n",
      " total_loss:  205.67584228515625 \n",
      "\n",
      "Validation - epoch: 30  average recon_loss: 1.1904846760961745\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 31 ( batch:  1 / 286 ) recon_loss: 98.71615600585938  perplexity:  695.14892578125  codebook loss:  723.8068237304688 \n",
      " total_loss:  171.09683227539062 \n",
      "\n",
      "epoch: 31 ( batch:  51 / 286 ) recon_loss: 97.12803649902344  perplexity:  737.3113403320312  codebook loss:  874.826416015625 \n",
      " total_loss:  184.61068725585938 \n",
      "\n",
      "epoch: 31 ( batch:  101 / 286 ) recon_loss: 120.69577026367188  perplexity:  711.36962890625  codebook loss:  858.9296875 \n",
      " total_loss:  206.5887451171875 \n",
      "\n",
      "epoch: 31 ( batch:  151 / 286 ) recon_loss: 110.56372833251953  perplexity:  737.9254150390625  codebook loss:  895.8814697265625 \n",
      " total_loss:  200.15188598632812 \n",
      "\n",
      "epoch: 31 ( batch:  201 / 286 ) recon_loss: 105.91875457763672  perplexity:  707.37548828125  codebook loss:  744.0868530273438 \n",
      " total_loss:  180.3274383544922 \n",
      "\n",
      "epoch: 31 ( batch:  251 / 286 ) recon_loss: 96.74607849121094  perplexity:  695.3660278320312  codebook loss:  690.28955078125 \n",
      " total_loss:  165.7750244140625 \n",
      "\n",
      "Validation - epoch: 31  average recon_loss: 1.203050575322575\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 32 ( batch:  1 / 286 ) recon_loss: 80.49747467041016  perplexity:  683.5213623046875  codebook loss:  665.7048950195312 \n",
      " total_loss:  147.06796264648438 \n",
      "\n",
      "epoch: 32 ( batch:  51 / 286 ) recon_loss: 88.79710388183594  perplexity:  691.443359375  codebook loss:  703.0518188476562 \n",
      " total_loss:  159.102294921875 \n",
      "\n",
      "epoch: 32 ( batch:  101 / 286 ) recon_loss: 130.113525390625  perplexity:  737.0588989257812  codebook loss:  882.9033203125 \n",
      " total_loss:  218.40386962890625 \n",
      "\n",
      "epoch: 32 ( batch:  151 / 286 ) recon_loss: 104.97669219970703  perplexity:  718.3176879882812  codebook loss:  805.9339599609375 \n",
      " total_loss:  185.57009887695312 \n",
      "\n",
      "epoch: 32 ( batch:  201 / 286 ) recon_loss: 94.7391128540039  perplexity:  730.9039916992188  codebook loss:  776.4349975585938 \n",
      " total_loss:  172.3826141357422 \n",
      "\n",
      "epoch: 32 ( batch:  251 / 286 ) recon_loss: 103.00574493408203  perplexity:  730.3650512695312  codebook loss:  878.89599609375 \n",
      " total_loss:  190.89535522460938 \n",
      "\n",
      "Validation - epoch: 32  average recon_loss: 1.1496920420063867\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 33 ( batch:  1 / 286 ) recon_loss: 90.39678955078125  perplexity:  728.6651000976562  codebook loss:  749.1951904296875 \n",
      " total_loss:  165.31631469726562 \n",
      "\n",
      "epoch: 33 ( batch:  51 / 286 ) recon_loss: 85.71058654785156  perplexity:  712.4640502929688  codebook loss:  779.9237060546875 \n",
      " total_loss:  163.7029571533203 \n",
      "\n",
      "epoch: 33 ( batch:  101 / 286 ) recon_loss: 165.42416381835938  perplexity:  767.4864501953125  codebook loss:  1069.4215087890625 \n",
      " total_loss:  272.3663330078125 \n",
      "\n",
      "epoch: 33 ( batch:  151 / 286 ) recon_loss: 118.99092102050781  perplexity:  752.0635986328125  codebook loss:  886.982177734375 \n",
      " total_loss:  207.68914794921875 \n",
      "\n",
      "epoch: 33 ( batch:  201 / 286 ) recon_loss: 135.98477172851562  perplexity:  758.5109252929688  codebook loss:  989.19140625 \n",
      " total_loss:  234.90391540527344 \n",
      "\n",
      "epoch: 33 ( batch:  251 / 286 ) recon_loss: 104.93649291992188  perplexity:  709.2740478515625  codebook loss:  791.2478637695312 \n",
      " total_loss:  184.061279296875 \n",
      "\n",
      "Validation - epoch: 33  average recon_loss: 1.144351025422414\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 34 ( batch:  1 / 286 ) recon_loss: 112.74486541748047  perplexity:  723.6416625976562  codebook loss:  840.0253295898438 \n",
      " total_loss:  196.74740600585938 \n",
      "\n",
      "epoch: 34 ( batch:  51 / 286 ) recon_loss: 117.4643783569336  perplexity:  738.3580322265625  codebook loss:  1074.4541015625 \n",
      " total_loss:  224.9097900390625 \n",
      "\n",
      "epoch: 34 ( batch:  101 / 286 ) recon_loss: 83.74774932861328  perplexity:  680.41650390625  codebook loss:  686.2666015625 \n",
      " total_loss:  152.37442016601562 \n",
      "\n",
      "epoch: 34 ( batch:  151 / 286 ) recon_loss: 98.69243621826172  perplexity:  759.08984375  codebook loss:  906.02734375 \n",
      " total_loss:  189.295166015625 \n",
      "\n",
      "epoch: 34 ( batch:  201 / 286 ) recon_loss: 146.63453674316406  perplexity:  778.0711059570312  codebook loss:  1049.89794921875 \n",
      " total_loss:  251.62432861328125 \n",
      "\n",
      "epoch: 34 ( batch:  251 / 286 ) recon_loss: 114.66996765136719  perplexity:  754.3574829101562  codebook loss:  949.830810546875 \n",
      " total_loss:  209.65304565429688 \n",
      "\n",
      "Validation - epoch: 34  average recon_loss: 1.1416812936464946\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 35 ( batch:  1 / 286 ) recon_loss: 117.84268951416016  perplexity:  741.6286010742188  codebook loss:  881.6644287109375 \n",
      " total_loss:  206.00912475585938 \n",
      "\n",
      "epoch: 35 ( batch:  51 / 286 ) recon_loss: 161.47235107421875  perplexity:  779.5885620117188  codebook loss:  1306.99560546875 \n",
      " total_loss:  292.17193603515625 \n",
      "\n",
      "epoch: 35 ( batch:  101 / 286 ) recon_loss: 120.53398895263672  perplexity:  752.2741088867188  codebook loss:  895.3683471679688 \n",
      " total_loss:  210.07083129882812 \n",
      "\n",
      "epoch: 35 ( batch:  151 / 286 ) recon_loss: 149.85887145996094  perplexity:  796.0338745117188  codebook loss:  1091.5810546875 \n",
      " total_loss:  259.0169677734375 \n",
      "\n",
      "epoch: 35 ( batch:  201 / 286 ) recon_loss: 79.5055160522461  perplexity:  692.3723754882812  codebook loss:  702.0252685546875 \n",
      " total_loss:  149.70803833007812 \n",
      "\n",
      "epoch: 35 ( batch:  251 / 286 ) recon_loss: 152.5238037109375  perplexity:  781.2396850585938  codebook loss:  1127.450439453125 \n",
      " total_loss:  265.26885986328125 \n",
      "\n",
      "Validation - epoch: 35  average recon_loss: 1.128385113345252\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 36 ( batch:  1 / 286 ) recon_loss: 148.25111389160156  perplexity:  774.6375732421875  codebook loss:  1095.4453125 \n",
      " total_loss:  257.795654296875 \n",
      "\n",
      "epoch: 36 ( batch:  51 / 286 ) recon_loss: 85.16448211669922  perplexity:  711.0643920898438  codebook loss:  735.4883422851562 \n",
      " total_loss:  158.71331787109375 \n",
      "\n",
      "epoch: 36 ( batch:  101 / 286 ) recon_loss: 110.3941421508789  perplexity:  766.6860961914062  codebook loss:  947.9608154296875 \n",
      " total_loss:  205.19021606445312 \n",
      "\n",
      "epoch: 36 ( batch:  151 / 286 ) recon_loss: 134.5280303955078  perplexity:  774.4130249023438  codebook loss:  1203.946533203125 \n",
      " total_loss:  254.9226837158203 \n",
      "\n",
      "epoch: 36 ( batch:  201 / 286 ) recon_loss: 113.13394165039062  perplexity:  753.65966796875  codebook loss:  837.5208129882812 \n",
      " total_loss:  196.88601684570312 \n",
      "\n",
      "epoch: 36 ( batch:  251 / 286 ) recon_loss: 95.73050689697266  perplexity:  710.1721801757812  codebook loss:  765.8087768554688 \n",
      " total_loss:  172.31138610839844 \n",
      "\n",
      "Validation - epoch: 36  average recon_loss: 1.1037566810846329\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 37 ( batch:  1 / 286 ) recon_loss: 132.27830505371094  perplexity:  774.3642578125  codebook loss:  1002.6978759765625 \n",
      " total_loss:  232.548095703125 \n",
      "\n",
      "epoch: 37 ( batch:  51 / 286 ) recon_loss: 105.6092529296875  perplexity:  753.279541015625  codebook loss:  865.880615234375 \n",
      " total_loss:  192.19732666015625 \n",
      "\n",
      "epoch: 37 ( batch:  101 / 286 ) recon_loss: 105.75959777832031  perplexity:  707.5340576171875  codebook loss:  813.7337646484375 \n",
      " total_loss:  187.13296508789062 \n",
      "\n",
      "epoch: 37 ( batch:  151 / 286 ) recon_loss: 90.01383972167969  perplexity:  716.601318359375  codebook loss:  773.4479370117188 \n",
      " total_loss:  167.358642578125 \n",
      "\n",
      "epoch: 37 ( batch:  201 / 286 ) recon_loss: 99.22925567626953  perplexity:  709.42822265625  codebook loss:  759.654541015625 \n",
      " total_loss:  175.1947021484375 \n",
      "\n",
      "epoch: 37 ( batch:  251 / 286 ) recon_loss: 103.73623657226562  perplexity:  742.6923828125  codebook loss:  837.7269287109375 \n",
      " total_loss:  187.50894165039062 \n",
      "\n",
      "Validation - epoch: 37  average recon_loss: 1.1196819129917357\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 38 ( batch:  1 / 286 ) recon_loss: 108.86194610595703  perplexity:  711.457763671875  codebook loss:  772.9291381835938 \n",
      " total_loss:  186.1548614501953 \n",
      "\n",
      "epoch: 38 ( batch:  51 / 286 ) recon_loss: 171.8382110595703  perplexity:  762.231689453125  codebook loss:  1564.0357666015625 \n",
      " total_loss:  328.2417907714844 \n",
      "\n",
      "epoch: 38 ( batch:  101 / 286 ) recon_loss: 108.71692657470703  perplexity:  754.8861694335938  codebook loss:  912.298095703125 \n",
      " total_loss:  199.94674682617188 \n",
      "\n",
      "epoch: 38 ( batch:  151 / 286 ) recon_loss: 123.76596069335938  perplexity:  778.3936157226562  codebook loss:  1011.6063232421875 \n",
      " total_loss:  224.92660522460938 \n",
      "\n",
      "epoch: 38 ( batch:  201 / 286 ) recon_loss: 97.19343566894531  perplexity:  743.089111328125  codebook loss:  813.5927734375 \n",
      " total_loss:  178.55270385742188 \n",
      "\n",
      "epoch: 38 ( batch:  251 / 286 ) recon_loss: 89.48294067382812  perplexity:  731.4479370117188  codebook loss:  781.6591796875 \n",
      " total_loss:  167.64886474609375 \n",
      "\n",
      "Validation - epoch: 38  average recon_loss: 1.0874722401301067\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 39 ( batch:  1 / 286 ) recon_loss: 132.3571014404297  perplexity:  741.6353759765625  codebook loss:  1061.0196533203125 \n",
      " total_loss:  238.45907592773438 \n",
      "\n",
      "epoch: 39 ( batch:  51 / 286 ) recon_loss: 96.65533447265625  perplexity:  724.2947387695312  codebook loss:  839.2990112304688 \n",
      " total_loss:  180.58523559570312 \n",
      "\n",
      "epoch: 39 ( batch:  101 / 286 ) recon_loss: 145.44699096679688  perplexity:  775.5419921875  codebook loss:  1310.64404296875 \n",
      " total_loss:  276.51141357421875 \n",
      "\n",
      "epoch: 39 ( batch:  151 / 286 ) recon_loss: 132.07081604003906  perplexity:  773.1813354492188  codebook loss:  1048.78369140625 \n",
      " total_loss:  236.94918823242188 \n",
      "\n",
      "epoch: 39 ( batch:  201 / 286 ) recon_loss: 99.1274642944336  perplexity:  761.8050537109375  codebook loss:  914.9908447265625 \n",
      " total_loss:  190.62655639648438 \n",
      "\n",
      "epoch: 39 ( batch:  251 / 286 ) recon_loss: 131.09710693359375  perplexity:  733.5828247070312  codebook loss:  1015.3268432617188 \n",
      " total_loss:  232.62979125976562 \n",
      "\n",
      "Validation - epoch: 39  average recon_loss: 1.113243939148055\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 40 ( batch:  1 / 286 ) recon_loss: 100.27300262451172  perplexity:  740.1215209960938  codebook loss:  908.8677978515625 \n",
      " total_loss:  191.1597900390625 \n",
      "\n",
      "epoch: 40 ( batch:  51 / 286 ) recon_loss: 83.05443572998047  perplexity:  712.1427612304688  codebook loss:  774.0128173828125 \n",
      " total_loss:  160.45571899414062 \n",
      "\n",
      "epoch: 40 ( batch:  101 / 286 ) recon_loss: 92.95519256591797  perplexity:  718.8714599609375  codebook loss:  859.8177490234375 \n",
      " total_loss:  178.9369659423828 \n",
      "\n",
      "epoch: 40 ( batch:  151 / 286 ) recon_loss: 103.53240966796875  perplexity:  728.3829956054688  codebook loss:  962.7047729492188 \n",
      " total_loss:  199.80288696289062 \n",
      "\n",
      "epoch: 40 ( batch:  201 / 286 ) recon_loss: 106.61214447021484  perplexity:  783.6985473632812  codebook loss:  1046.913818359375 \n",
      " total_loss:  211.30352783203125 \n",
      "\n",
      "epoch: 40 ( batch:  251 / 286 ) recon_loss: 94.2916030883789  perplexity:  763.7225952148438  codebook loss:  874.4500122070312 \n",
      " total_loss:  181.73660278320312 \n",
      "\n",
      "Validation - epoch: 40  average recon_loss: 1.135141834616661\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 41 ( batch:  1 / 286 ) recon_loss: 111.94780731201172  perplexity:  711.3464965820312  codebook loss:  924.837158203125 \n",
      " total_loss:  204.4315185546875 \n",
      "\n",
      "epoch: 41 ( batch:  51 / 286 ) recon_loss: 112.71546936035156  perplexity:  800.5995483398438  codebook loss:  1093.4564208984375 \n",
      " total_loss:  222.0611114501953 \n",
      "\n",
      "epoch: 41 ( batch:  101 / 286 ) recon_loss: 81.4124526977539  perplexity:  722.196533203125  codebook loss:  774.724365234375 \n",
      " total_loss:  158.8848876953125 \n",
      "\n",
      "epoch: 41 ( batch:  151 / 286 ) recon_loss: 97.50955963134766  perplexity:  729.912109375  codebook loss:  863.4840087890625 \n",
      " total_loss:  183.85797119140625 \n",
      "\n",
      "epoch: 41 ( batch:  201 / 286 ) recon_loss: 89.97030639648438  perplexity:  736.7310791015625  codebook loss:  918.3402099609375 \n",
      " total_loss:  181.8043212890625 \n",
      "\n",
      "epoch: 41 ( batch:  251 / 286 ) recon_loss: 84.7723388671875  perplexity:  727.4909057617188  codebook loss:  820.9265747070312 \n",
      " total_loss:  166.864990234375 \n",
      "\n",
      "Validation - epoch: 41  average recon_loss: 1.1081442270014021\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 42 ( batch:  1 / 286 ) recon_loss: 114.9168701171875  perplexity:  762.342529296875  codebook loss:  1133.072021484375 \n",
      " total_loss:  228.2240753173828 \n",
      "\n",
      "epoch: 42 ( batch:  51 / 286 ) recon_loss: 91.29291534423828  perplexity:  745.5480346679688  codebook loss:  910.6375732421875 \n",
      " total_loss:  182.35667419433594 \n",
      "\n",
      "epoch: 42 ( batch:  101 / 286 ) recon_loss: 85.95440673828125  perplexity:  734.8616943359375  codebook loss:  872.3958740234375 \n",
      " total_loss:  173.19400024414062 \n",
      "\n",
      "epoch: 42 ( batch:  151 / 286 ) recon_loss: 103.49417114257812  perplexity:  769.8388061523438  codebook loss:  957.9400024414062 \n",
      " total_loss:  199.28817749023438 \n",
      "\n",
      "epoch: 42 ( batch:  201 / 286 ) recon_loss: 140.2036590576172  perplexity:  793.7748413085938  codebook loss:  1043.4415283203125 \n",
      " total_loss:  244.54782104492188 \n",
      "\n",
      "epoch: 42 ( batch:  251 / 286 ) recon_loss: 122.62498474121094  perplexity:  773.211669921875  codebook loss:  994.70556640625 \n",
      " total_loss:  222.09555053710938 \n",
      "\n",
      "Validation - epoch: 42  average recon_loss: 1.08526973426342\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 43 ( batch:  1 / 286 ) recon_loss: 88.72142791748047  perplexity:  728.6980590820312  codebook loss:  865.8253173828125 \n",
      " total_loss:  175.303955078125 \n",
      "\n",
      "epoch: 43 ( batch:  51 / 286 ) recon_loss: 100.38145446777344  perplexity:  766.3929443359375  codebook loss:  871.3947143554688 \n",
      " total_loss:  187.52093505859375 \n",
      "\n",
      "epoch: 43 ( batch:  101 / 286 ) recon_loss: 92.76030731201172  perplexity:  736.054443359375  codebook loss:  856.0064086914062 \n",
      " total_loss:  178.36094665527344 \n",
      "\n",
      "epoch: 43 ( batch:  151 / 286 ) recon_loss: 93.47206115722656  perplexity:  705.9541625976562  codebook loss:  815.6915283203125 \n",
      " total_loss:  175.0412139892578 \n",
      "\n",
      "epoch: 43 ( batch:  201 / 286 ) recon_loss: 82.33789825439453  perplexity:  726.8806762695312  codebook loss:  761.9183959960938 \n",
      " total_loss:  158.5297393798828 \n",
      "\n",
      "epoch: 43 ( batch:  251 / 286 ) recon_loss: 86.8879165649414  perplexity:  750.9097900390625  codebook loss:  875.0049438476562 \n",
      " total_loss:  174.38841247558594 \n",
      "\n",
      "Validation - epoch: 43  average recon_loss: 1.0832232567999098\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 44 ( batch:  1 / 286 ) recon_loss: 83.85357666015625  perplexity:  735.6698608398438  codebook loss:  901.1199951171875 \n",
      " total_loss:  173.965576171875 \n",
      "\n",
      "epoch: 44 ( batch:  51 / 286 ) recon_loss: 103.43568420410156  perplexity:  736.0446166992188  codebook loss:  805.061767578125 \n",
      " total_loss:  183.94186401367188 \n",
      "\n",
      "epoch: 44 ( batch:  101 / 286 ) recon_loss: 87.95674133300781  perplexity:  736.2113037109375  codebook loss:  774.85693359375 \n",
      " total_loss:  165.44244384765625 \n",
      "\n",
      "epoch: 44 ( batch:  151 / 286 ) recon_loss: 89.50363159179688  perplexity:  710.0313110351562  codebook loss:  756.2464599609375 \n",
      " total_loss:  165.12828063964844 \n",
      "\n",
      "epoch: 44 ( batch:  201 / 286 ) recon_loss: 89.3044204711914  perplexity:  713.8477172851562  codebook loss:  775.5206909179688 \n",
      " total_loss:  166.8564910888672 \n",
      "\n",
      "epoch: 44 ( batch:  251 / 286 ) recon_loss: 116.02217864990234  perplexity:  776.3389282226562  codebook loss:  1118.147705078125 \n",
      " total_loss:  227.83694458007812 \n",
      "\n",
      "Validation - epoch: 44  average recon_loss: 1.0709356317917507\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 45 ( batch:  1 / 286 ) recon_loss: 102.82237243652344  perplexity:  767.0335083007812  codebook loss:  1022.6434936523438 \n",
      " total_loss:  205.08673095703125 \n",
      "\n",
      "epoch: 45 ( batch:  51 / 286 ) recon_loss: 166.65069580078125  perplexity:  791.4873657226562  codebook loss:  1224.61279296875 \n",
      " total_loss:  289.1119689941406 \n",
      "\n",
      "epoch: 45 ( batch:  101 / 286 ) recon_loss: 118.98163604736328  perplexity:  741.6455688476562  codebook loss:  908.34375 \n",
      " total_loss:  209.81600952148438 \n",
      "\n",
      "epoch: 45 ( batch:  151 / 286 ) recon_loss: 136.5504608154297  perplexity:  773.8585815429688  codebook loss:  1090.4510498046875 \n",
      " total_loss:  245.59556579589844 \n",
      "\n",
      "epoch: 45 ( batch:  201 / 286 ) recon_loss: 119.20024108886719  perplexity:  777.8900756835938  codebook loss:  1005.1889038085938 \n",
      " total_loss:  219.71913146972656 \n",
      "\n",
      "epoch: 45 ( batch:  251 / 286 ) recon_loss: 104.84485626220703  perplexity:  734.110107421875  codebook loss:  893.028564453125 \n",
      " total_loss:  194.147705078125 \n",
      "\n",
      "Validation - epoch: 45  average recon_loss: 1.0766063100761838\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 46 ( batch:  1 / 286 ) recon_loss: 98.9426040649414  perplexity:  738.7242431640625  codebook loss:  875.9422607421875 \n",
      " total_loss:  186.53683471679688 \n",
      "\n",
      "epoch: 46 ( batch:  51 / 286 ) recon_loss: 83.947509765625  perplexity:  741.4688110351562  codebook loss:  854.3687744140625 \n",
      " total_loss:  169.3843994140625 \n",
      "\n",
      "epoch: 46 ( batch:  101 / 286 ) recon_loss: 82.86800384521484  perplexity:  713.7725219726562  codebook loss:  755.3294677734375 \n",
      " total_loss:  158.4009552001953 \n",
      "\n",
      "epoch: 46 ( batch:  151 / 286 ) recon_loss: 118.88802337646484  perplexity:  778.71435546875  codebook loss:  1019.5809326171875 \n",
      " total_loss:  220.8461151123047 \n",
      "\n",
      "epoch: 46 ( batch:  201 / 286 ) recon_loss: 117.16584014892578  perplexity:  764.635009765625  codebook loss:  1026.02294921875 \n",
      " total_loss:  219.76812744140625 \n",
      "\n",
      "epoch: 46 ( batch:  251 / 286 ) recon_loss: 91.0030746459961  perplexity:  769.4083251953125  codebook loss:  943.72802734375 \n",
      " total_loss:  185.37588500976562 \n",
      "\n",
      "Validation - epoch: 46  average recon_loss: 1.0692523850335016\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 47 ( batch:  1 / 286 ) recon_loss: 76.42405700683594  perplexity:  722.0288696289062  codebook loss:  761.797607421875 \n",
      " total_loss:  152.60382080078125 \n",
      "\n",
      "epoch: 47 ( batch:  51 / 286 ) recon_loss: 98.51097869873047  perplexity:  780.501708984375  codebook loss:  910.2579345703125 \n",
      " total_loss:  189.53677368164062 \n",
      "\n",
      "epoch: 47 ( batch:  101 / 286 ) recon_loss: 110.62382507324219  perplexity:  767.9835815429688  codebook loss:  916.9872436523438 \n",
      " total_loss:  202.32254028320312 \n",
      "\n",
      "epoch: 47 ( batch:  151 / 286 ) recon_loss: 116.68319702148438  perplexity:  794.9497680664062  codebook loss:  1094.404052734375 \n",
      " total_loss:  226.12359619140625 \n",
      "\n",
      "epoch: 47 ( batch:  201 / 286 ) recon_loss: 81.39977264404297  perplexity:  732.7412719726562  codebook loss:  853.16650390625 \n",
      " total_loss:  166.7164306640625 \n",
      "\n",
      "epoch: 47 ( batch:  251 / 286 ) recon_loss: 98.2293701171875  perplexity:  726.4343872070312  codebook loss:  808.6597290039062 \n",
      " total_loss:  179.0953369140625 \n",
      "\n",
      "Validation - epoch: 47  average recon_loss: 1.0845077517959807\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 48 ( batch:  1 / 286 ) recon_loss: 102.15924072265625  perplexity:  768.4597778320312  codebook loss:  965.0591430664062 \n",
      " total_loss:  198.6651611328125 \n",
      "\n",
      "epoch: 48 ( batch:  51 / 286 ) recon_loss: 141.6510772705078  perplexity:  787.0855102539062  codebook loss:  1215.173583984375 \n",
      " total_loss:  263.1684265136719 \n",
      "\n",
      "epoch: 48 ( batch:  101 / 286 ) recon_loss: 105.25116729736328  perplexity:  751.0250854492188  codebook loss:  1008.2884521484375 \n",
      " total_loss:  206.08001708984375 \n",
      "\n",
      "epoch: 48 ( batch:  151 / 286 ) recon_loss: 92.211669921875  perplexity:  745.4755249023438  codebook loss:  861.2542114257812 \n",
      " total_loss:  178.33709716796875 \n",
      "\n",
      "epoch: 48 ( batch:  201 / 286 ) recon_loss: 106.89305114746094  perplexity:  765.6229858398438  codebook loss:  953.4777221679688 \n",
      " total_loss:  202.24081420898438 \n",
      "\n",
      "epoch: 48 ( batch:  251 / 286 ) recon_loss: 124.60836029052734  perplexity:  792.8328857421875  codebook loss:  1065.994873046875 \n",
      " total_loss:  231.20785522460938 \n",
      "\n",
      "Validation - epoch: 48  average recon_loss: 1.0865323560105429\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 49 ( batch:  1 / 286 ) recon_loss: 110.1981201171875  perplexity:  749.1726684570312  codebook loss:  1046.7513427734375 \n",
      " total_loss:  214.87326049804688 \n",
      "\n",
      "epoch: 49 ( batch:  51 / 286 ) recon_loss: 102.83428192138672  perplexity:  768.294921875  codebook loss:  915.8704833984375 \n",
      " total_loss:  194.42132568359375 \n",
      "\n",
      "epoch: 49 ( batch:  101 / 286 ) recon_loss: 97.96064758300781  perplexity:  741.3514404296875  codebook loss:  875.5347290039062 \n",
      " total_loss:  185.51412963867188 \n",
      "\n",
      "epoch: 49 ( batch:  151 / 286 ) recon_loss: 106.68872833251953  perplexity:  773.0089111328125  codebook loss:  952.7410278320312 \n",
      " total_loss:  201.96282958984375 \n",
      "\n",
      "epoch: 49 ( batch:  201 / 286 ) recon_loss: 96.03507232666016  perplexity:  772.460205078125  codebook loss:  988.0438232421875 \n",
      " total_loss:  194.83944702148438 \n",
      "\n",
      "epoch: 49 ( batch:  251 / 286 ) recon_loss: 95.27301788330078  perplexity:  731.3363037109375  codebook loss:  920.148193359375 \n",
      " total_loss:  187.287841796875 \n",
      "\n",
      "Validation - epoch: 49  average recon_loss: 1.0605833729108174\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 50 ( batch:  1 / 286 ) recon_loss: 102.9831314086914  perplexity:  763.582763671875  codebook loss:  953.313720703125 \n",
      " total_loss:  198.31451416015625 \n",
      "\n",
      "epoch: 50 ( batch:  51 / 286 ) recon_loss: 114.8960952758789  perplexity:  801.294677734375  codebook loss:  984.1814575195312 \n",
      " total_loss:  213.31423950195312 \n",
      "\n",
      "epoch: 50 ( batch:  101 / 286 ) recon_loss: 130.83274841308594  perplexity:  791.5636596679688  codebook loss:  1203.8922119140625 \n",
      " total_loss:  251.2219696044922 \n",
      "\n",
      "epoch: 50 ( batch:  151 / 286 ) recon_loss: 85.27236938476562  perplexity:  721.90771484375  codebook loss:  765.698974609375 \n",
      " total_loss:  161.84226989746094 \n",
      "\n",
      "epoch: 50 ( batch:  201 / 286 ) recon_loss: 140.32736206054688  perplexity:  825.6861572265625  codebook loss:  1218.7012939453125 \n",
      " total_loss:  262.197509765625 \n",
      "\n",
      "epoch: 50 ( batch:  251 / 286 ) recon_loss: 75.91950225830078  perplexity:  734.551025390625  codebook loss:  836.6890869140625 \n",
      " total_loss:  159.58840942382812 \n",
      "\n",
      "Validation - epoch: 50  average recon_loss: 1.0431596553987927\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 51 ( batch:  1 / 286 ) recon_loss: 97.76799011230469  perplexity:  762.3778076171875  codebook loss:  916.751220703125 \n",
      " total_loss:  189.443115234375 \n",
      "\n",
      "epoch: 51 ( batch:  51 / 286 ) recon_loss: 104.80191802978516  perplexity:  767.4219970703125  codebook loss:  906.0169067382812 \n",
      " total_loss:  195.4036102294922 \n",
      "\n",
      "epoch: 51 ( batch:  101 / 286 ) recon_loss: 85.96337890625  perplexity:  745.0447998046875  codebook loss:  805.3128051757812 \n",
      " total_loss:  166.49465942382812 \n",
      "\n",
      "epoch: 51 ( batch:  151 / 286 ) recon_loss: 116.6579818725586  perplexity:  739.0755615234375  codebook loss:  978.964599609375 \n",
      " total_loss:  214.554443359375 \n",
      "\n",
      "epoch: 51 ( batch:  201 / 286 ) recon_loss: 112.068115234375  perplexity:  762.50537109375  codebook loss:  956.89453125 \n",
      " total_loss:  207.757568359375 \n",
      "\n",
      "epoch: 51 ( batch:  251 / 286 ) recon_loss: 94.55364990234375  perplexity:  721.78759765625  codebook loss:  807.076416015625 \n",
      " total_loss:  175.26129150390625 \n",
      "\n",
      "Validation - epoch: 51  average recon_loss: 1.0537462217940226\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 52 ( batch:  1 / 286 ) recon_loss: 114.44261169433594  perplexity:  778.1961669921875  codebook loss:  961.1388549804688 \n",
      " total_loss:  210.55648803710938 \n",
      "\n",
      "epoch: 52 ( batch:  51 / 286 ) recon_loss: 101.74125671386719  perplexity:  764.39111328125  codebook loss:  921.824951171875 \n",
      " total_loss:  193.9237518310547 \n",
      "\n",
      "epoch: 52 ( batch:  101 / 286 ) recon_loss: 125.99589538574219  perplexity:  802.5427856445312  codebook loss:  1160.619873046875 \n",
      " total_loss:  242.05789184570312 \n",
      "\n",
      "epoch: 52 ( batch:  151 / 286 ) recon_loss: 97.24967193603516  perplexity:  748.8683471679688  codebook loss:  881.973876953125 \n",
      " total_loss:  185.44705200195312 \n",
      "\n",
      "epoch: 52 ( batch:  201 / 286 ) recon_loss: 118.48310089111328  perplexity:  784.4649658203125  codebook loss:  1107.3775634765625 \n",
      " total_loss:  229.22085571289062 \n",
      "\n",
      "epoch: 52 ( batch:  251 / 286 ) recon_loss: 88.72311401367188  perplexity:  747.6331176757812  codebook loss:  869.3878784179688 \n",
      " total_loss:  175.66189575195312 \n",
      "\n",
      "Validation - epoch: 52  average recon_loss: 1.074085396197107\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 53 ( batch:  1 / 286 ) recon_loss: 89.39027404785156  perplexity:  718.1115112304688  codebook loss:  737.6416625976562 \n",
      " total_loss:  163.15444946289062 \n",
      "\n",
      "epoch: 53 ( batch:  51 / 286 ) recon_loss: 91.51499938964844  perplexity:  771.9325561523438  codebook loss:  911.2979125976562 \n",
      " total_loss:  182.64479064941406 \n",
      "\n",
      "epoch: 53 ( batch:  101 / 286 ) recon_loss: 84.00730895996094  perplexity:  717.6538696289062  codebook loss:  726.6506958007812 \n",
      " total_loss:  156.67237854003906 \n",
      "\n",
      "epoch: 53 ( batch:  151 / 286 ) recon_loss: 106.3127670288086  perplexity:  755.9465942382812  codebook loss:  897.5531616210938 \n",
      " total_loss:  196.06808471679688 \n",
      "\n",
      "epoch: 53 ( batch:  201 / 286 ) recon_loss: 114.23571014404297  perplexity:  753.5802612304688  codebook loss:  1082.9954833984375 \n",
      " total_loss:  222.53526306152344 \n",
      "\n",
      "epoch: 53 ( batch:  251 / 286 ) recon_loss: 128.6507568359375  perplexity:  787.9348754882812  codebook loss:  944.803466796875 \n",
      " total_loss:  223.131103515625 \n",
      "\n",
      "Validation - epoch: 53  average recon_loss: 1.0573443637953863\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 54 ( batch:  1 / 286 ) recon_loss: 97.12858581542969  perplexity:  773.3864135742188  codebook loss:  933.5269165039062 \n",
      " total_loss:  190.4812774658203 \n",
      "\n",
      "epoch: 54 ( batch:  51 / 286 ) recon_loss: 120.56704711914062  perplexity:  752.63037109375  codebook loss:  951.9902954101562 \n",
      " total_loss:  215.76608276367188 \n",
      "\n",
      "epoch: 54 ( batch:  101 / 286 ) recon_loss: 96.31930541992188  perplexity:  731.9486083984375  codebook loss:  834.9939575195312 \n",
      " total_loss:  179.81869506835938 \n",
      "\n",
      "epoch: 54 ( batch:  151 / 286 ) recon_loss: 103.50447845458984  perplexity:  735.1964111328125  codebook loss:  792.4002075195312 \n",
      " total_loss:  182.7445068359375 \n",
      "\n",
      "epoch: 54 ( batch:  201 / 286 ) recon_loss: 88.11127471923828  perplexity:  731.9492797851562  codebook loss:  853.3804321289062 \n",
      " total_loss:  173.44931030273438 \n",
      "\n",
      "epoch: 54 ( batch:  251 / 286 ) recon_loss: 118.93450164794922  perplexity:  750.6674194335938  codebook loss:  894.3836059570312 \n",
      " total_loss:  208.37286376953125 \n",
      "\n",
      "Validation - epoch: 54  average recon_loss: 1.0764106528626547\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 55 ( batch:  1 / 286 ) recon_loss: 81.31131744384766  perplexity:  728.9642944335938  codebook loss:  790.5927124023438 \n",
      " total_loss:  160.37059020996094 \n",
      "\n",
      "epoch: 55 ( batch:  51 / 286 ) recon_loss: 123.66222381591797  perplexity:  792.4315185546875  codebook loss:  1223.0484619140625 \n",
      " total_loss:  245.96707153320312 \n",
      "\n",
      "epoch: 55 ( batch:  101 / 286 ) recon_loss: 101.77364349365234  perplexity:  736.3303833007812  codebook loss:  987.896728515625 \n",
      " total_loss:  200.56332397460938 \n",
      "\n",
      "epoch: 55 ( batch:  151 / 286 ) recon_loss: 75.27623748779297  perplexity:  728.5386352539062  codebook loss:  763.6346435546875 \n",
      " total_loss:  151.63970947265625 \n",
      "\n",
      "epoch: 55 ( batch:  201 / 286 ) recon_loss: 110.96505737304688  perplexity:  775.8722534179688  codebook loss:  1020.5225830078125 \n",
      " total_loss:  213.01731872558594 \n",
      "\n",
      "epoch: 55 ( batch:  251 / 286 ) recon_loss: 114.27084350585938  perplexity:  749.9078369140625  codebook loss:  893.3649291992188 \n",
      " total_loss:  203.60733032226562 \n",
      "\n",
      "Validation - epoch: 55  average recon_loss: 1.07917453845342\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 56 ( batch:  1 / 286 ) recon_loss: 99.9605484008789  perplexity:  731.8016967773438  codebook loss:  799.0252075195312 \n",
      " total_loss:  179.86306762695312 \n",
      "\n",
      "epoch: 56 ( batch:  51 / 286 ) recon_loss: 101.01248931884766  perplexity:  722.355712890625  codebook loss:  814.1409912109375 \n",
      " total_loss:  182.4265899658203 \n",
      "\n",
      "epoch: 56 ( batch:  101 / 286 ) recon_loss: 109.94422149658203  perplexity:  772.2576293945312  codebook loss:  1079.4755859375 \n",
      " total_loss:  217.89178466796875 \n",
      "\n",
      "epoch: 56 ( batch:  151 / 286 ) recon_loss: 85.41639709472656  perplexity:  747.4830322265625  codebook loss:  812.22412109375 \n",
      " total_loss:  166.63880920410156 \n",
      "\n",
      "epoch: 56 ( batch:  201 / 286 ) recon_loss: 87.10127258300781  perplexity:  754.02880859375  codebook loss:  881.2377319335938 \n",
      " total_loss:  175.22503662109375 \n",
      "\n",
      "epoch: 56 ( batch:  251 / 286 ) recon_loss: 89.04008483886719  perplexity:  760.2388305664062  codebook loss:  885.9848022460938 \n",
      " total_loss:  177.63856506347656 \n",
      "\n",
      "Validation - epoch: 56  average recon_loss: 1.081236771411366\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 57 ( batch:  1 / 286 ) recon_loss: 103.14334869384766  perplexity:  766.2760620117188  codebook loss:  971.9329223632812 \n",
      " total_loss:  200.33663940429688 \n",
      "\n",
      "epoch: 57 ( batch:  51 / 286 ) recon_loss: 83.52749633789062  perplexity:  737.492431640625  codebook loss:  768.0945434570312 \n",
      " total_loss:  160.33694458007812 \n",
      "\n",
      "epoch: 57 ( batch:  101 / 286 ) recon_loss: 150.16363525390625  perplexity:  812.291015625  codebook loss:  1122.30712890625 \n",
      " total_loss:  262.39434814453125 \n",
      "\n",
      "epoch: 57 ( batch:  151 / 286 ) recon_loss: 110.46066284179688  perplexity:  772.8931274414062  codebook loss:  1029.381591796875 \n",
      " total_loss:  213.39883422851562 \n",
      "\n",
      "epoch: 57 ( batch:  201 / 286 ) recon_loss: 111.17180633544922  perplexity:  784.9371948242188  codebook loss:  1024.672607421875 \n",
      " total_loss:  213.63906860351562 \n",
      "\n",
      "epoch: 57 ( batch:  251 / 286 ) recon_loss: 112.88072967529297  perplexity:  762.5890502929688  codebook loss:  1046.928955078125 \n",
      " total_loss:  217.57362365722656 \n",
      "\n",
      "Validation - epoch: 57  average recon_loss: 1.067888884080781\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 58 ( batch:  1 / 286 ) recon_loss: 76.7049789428711  perplexity:  722.801513671875  codebook loss:  717.6862182617188 \n",
      " total_loss:  148.47360229492188 \n",
      "\n",
      "epoch: 58 ( batch:  51 / 286 ) recon_loss: 111.66278076171875  perplexity:  759.2379150390625  codebook loss:  887.60205078125 \n",
      " total_loss:  200.42298889160156 \n",
      "\n",
      "epoch: 58 ( batch:  101 / 286 ) recon_loss: 85.73106384277344  perplexity:  681.2410888671875  codebook loss:  678.6483764648438 \n",
      " total_loss:  153.5959014892578 \n",
      "\n",
      "epoch: 58 ( batch:  151 / 286 ) recon_loss: 90.82996368408203  perplexity:  722.5482177734375  codebook loss:  735.2456665039062 \n",
      " total_loss:  164.35452270507812 \n",
      "\n",
      "epoch: 58 ( batch:  201 / 286 ) recon_loss: 93.8750991821289  perplexity:  761.82177734375  codebook loss:  955.5498657226562 \n",
      " total_loss:  189.43008422851562 \n",
      "\n",
      "epoch: 58 ( batch:  251 / 286 ) recon_loss: 79.2047348022461  perplexity:  711.4096069335938  codebook loss:  724.57666015625 \n",
      " total_loss:  151.6623992919922 \n",
      "\n",
      "Validation - epoch: 58  average recon_loss: 1.0670903871456783\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 59 ( batch:  1 / 286 ) recon_loss: 100.62471771240234  perplexity:  752.2310791015625  codebook loss:  991.0814208984375 \n",
      " total_loss:  199.7328643798828 \n",
      "\n",
      "epoch: 59 ( batch:  51 / 286 ) recon_loss: 84.42869567871094  perplexity:  733.9207763671875  codebook loss:  774.1190185546875 \n",
      " total_loss:  161.84060668945312 \n",
      "\n",
      "epoch: 59 ( batch:  101 / 286 ) recon_loss: 95.79476928710938  perplexity:  750.8914794921875  codebook loss:  900.84423828125 \n",
      " total_loss:  185.8791961669922 \n",
      "\n",
      "epoch: 59 ( batch:  151 / 286 ) recon_loss: 105.5984115600586  perplexity:  769.9658203125  codebook loss:  988.3224487304688 \n",
      " total_loss:  204.4306640625 \n",
      "\n",
      "epoch: 59 ( batch:  201 / 286 ) recon_loss: 90.15725708007812  perplexity:  736.1510009765625  codebook loss:  804.27783203125 \n",
      " total_loss:  170.58505249023438 \n",
      "\n",
      "epoch: 59 ( batch:  251 / 286 ) recon_loss: 123.12518310546875  perplexity:  771.3434448242188  codebook loss:  971.041748046875 \n",
      " total_loss:  220.2293701171875 \n",
      "\n",
      "Validation - epoch: 59  average recon_loss: 1.095732366045316\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 60 ( batch:  1 / 286 ) recon_loss: 103.1441650390625  perplexity:  765.3543090820312  codebook loss:  899.367431640625 \n",
      " total_loss:  193.08090209960938 \n",
      "\n",
      "epoch: 60 ( batch:  51 / 286 ) recon_loss: 84.8982925415039  perplexity:  731.238037109375  codebook loss:  876.55029296875 \n",
      " total_loss:  172.55331420898438 \n",
      "\n",
      "epoch: 60 ( batch:  101 / 286 ) recon_loss: 129.9358673095703  perplexity:  791.643310546875  codebook loss:  1131.0684814453125 \n",
      " total_loss:  243.042724609375 \n",
      "\n",
      "epoch: 60 ( batch:  151 / 286 ) recon_loss: 85.41413116455078  perplexity:  738.35693359375  codebook loss:  820.7925415039062 \n",
      " total_loss:  167.49337768554688 \n",
      "\n",
      "epoch: 60 ( batch:  201 / 286 ) recon_loss: 87.54830932617188  perplexity:  736.6808471679688  codebook loss:  851.8018188476562 \n",
      " total_loss:  172.72848510742188 \n",
      "\n",
      "epoch: 60 ( batch:  251 / 286 ) recon_loss: 85.99176025390625  perplexity:  734.5569458007812  codebook loss:  777.0624389648438 \n",
      " total_loss:  163.697998046875 \n",
      "\n",
      "Validation - epoch: 60  average recon_loss: 1.0739282038476732\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 61 ( batch:  1 / 286 ) recon_loss: 100.45989990234375  perplexity:  763.390869140625  codebook loss:  860.10546875 \n",
      " total_loss:  186.470458984375 \n",
      "\n",
      "epoch: 61 ( batch:  51 / 286 ) recon_loss: 118.76634216308594  perplexity:  740.2853393554688  codebook loss:  1003.3270874023438 \n",
      " total_loss:  219.09906005859375 \n",
      "\n",
      "epoch: 61 ( batch:  101 / 286 ) recon_loss: 94.5956802368164  perplexity:  762.4374389648438  codebook loss:  896.7443237304688 \n",
      " total_loss:  184.27011108398438 \n",
      "\n",
      "epoch: 61 ( batch:  151 / 286 ) recon_loss: 126.9476547241211  perplexity:  782.4520874023438  codebook loss:  1076.057861328125 \n",
      " total_loss:  234.55343627929688 \n",
      "\n",
      "epoch: 61 ( batch:  201 / 286 ) recon_loss: 81.97437286376953  perplexity:  731.4141235351562  codebook loss:  816.050048828125 \n",
      " total_loss:  163.57937622070312 \n",
      "\n",
      "epoch: 61 ( batch:  251 / 286 ) recon_loss: 107.00787353515625  perplexity:  762.36865234375  codebook loss:  930.4684448242188 \n",
      " total_loss:  200.05471801757812 \n",
      "\n",
      "Validation - epoch: 61  average recon_loss: 1.065512581004037\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 62 ( batch:  1 / 286 ) recon_loss: 117.470703125  perplexity:  779.1448364257812  codebook loss:  1050.29443359375 \n",
      " total_loss:  222.50015258789062 \n",
      "\n",
      "epoch: 62 ( batch:  51 / 286 ) recon_loss: 85.92473602294922  perplexity:  737.800537109375  codebook loss:  882.57177734375 \n",
      " total_loss:  174.18191528320312 \n",
      "\n",
      "epoch: 62 ( batch:  101 / 286 ) recon_loss: 120.67750549316406  perplexity:  773.145263671875  codebook loss:  1022.09521484375 \n",
      " total_loss:  222.88702392578125 \n",
      "\n",
      "epoch: 62 ( batch:  151 / 286 ) recon_loss: 97.7669448852539  perplexity:  749.6807861328125  codebook loss:  890.5374755859375 \n",
      " total_loss:  186.82069396972656 \n",
      "\n",
      "epoch: 62 ( batch:  201 / 286 ) recon_loss: 121.4635009765625  perplexity:  781.5657348632812  codebook loss:  996.0425415039062 \n",
      " total_loss:  221.0677490234375 \n",
      "\n",
      "epoch: 62 ( batch:  251 / 286 ) recon_loss: 78.75485229492188  perplexity:  718.9318237304688  codebook loss:  725.297607421875 \n",
      " total_loss:  151.28460693359375 \n",
      "\n",
      "Validation - epoch: 62  average recon_loss: 1.0816433214479022\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 63 ( batch:  1 / 286 ) recon_loss: 85.27735137939453  perplexity:  726.7035522460938  codebook loss:  797.1201171875 \n",
      " total_loss:  164.98936462402344 \n",
      "\n",
      "epoch: 63 ( batch:  51 / 286 ) recon_loss: 82.95980834960938  perplexity:  736.3447265625  codebook loss:  803.9888916015625 \n",
      " total_loss:  163.35870361328125 \n",
      "\n",
      "epoch: 63 ( batch:  101 / 286 ) recon_loss: 123.19196319580078  perplexity:  746.677978515625  codebook loss:  931.1171875 \n",
      " total_loss:  216.30368041992188 \n",
      "\n",
      "epoch: 63 ( batch:  151 / 286 ) recon_loss: 132.51797485351562  perplexity:  767.9989013671875  codebook loss:  1064.16064453125 \n",
      " total_loss:  238.93405151367188 \n",
      "\n",
      "epoch: 63 ( batch:  201 / 286 ) recon_loss: 95.29129791259766  perplexity:  711.6376342773438  codebook loss:  947.024169921875 \n",
      " total_loss:  189.99371337890625 \n",
      "\n",
      "epoch: 63 ( batch:  251 / 286 ) recon_loss: 84.62655639648438  perplexity:  737.5258178710938  codebook loss:  794.796630859375 \n",
      " total_loss:  164.10623168945312 \n",
      "\n",
      "Validation - epoch: 63  average recon_loss: 1.0748375886016421\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 64 ( batch:  1 / 286 ) recon_loss: 113.1393051147461  perplexity:  732.888427734375  codebook loss:  842.0092163085938 \n",
      " total_loss:  197.34022521972656 \n",
      "\n",
      "epoch: 64 ( batch:  51 / 286 ) recon_loss: 82.69152069091797  perplexity:  694.001953125  codebook loss:  678.8961181640625 \n",
      " total_loss:  150.5811309814453 \n",
      "\n",
      "epoch: 64 ( batch:  101 / 286 ) recon_loss: 86.79539489746094  perplexity:  730.8287963867188  codebook loss:  821.2769775390625 \n",
      " total_loss:  168.923095703125 \n",
      "\n",
      "epoch: 64 ( batch:  151 / 286 ) recon_loss: 96.48641204833984  perplexity:  740.942138671875  codebook loss:  779.8580322265625 \n",
      " total_loss:  174.4722137451172 \n",
      "\n",
      "epoch: 64 ( batch:  201 / 286 ) recon_loss: 154.87078857421875  perplexity:  794.0140991210938  codebook loss:  1134.3519287109375 \n",
      " total_loss:  268.30596923828125 \n",
      "\n",
      "epoch: 64 ( batch:  251 / 286 ) recon_loss: 89.6636734008789  perplexity:  729.77783203125  codebook loss:  835.087646484375 \n",
      " total_loss:  173.1724395751953 \n",
      "\n",
      "Validation - epoch: 64  average recon_loss: 1.0724173221323225\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 65 ( batch:  1 / 286 ) recon_loss: 92.42027282714844  perplexity:  729.19970703125  codebook loss:  798.881103515625 \n",
      " total_loss:  172.30838012695312 \n",
      "\n",
      "epoch: 65 ( batch:  51 / 286 ) recon_loss: 96.03463745117188  perplexity:  759.6934814453125  codebook loss:  944.0703735351562 \n",
      " total_loss:  190.44168090820312 \n",
      "\n",
      "epoch: 65 ( batch:  101 / 286 ) recon_loss: 98.92437744140625  perplexity:  773.1740112304688  codebook loss:  926.8912353515625 \n",
      " total_loss:  191.61349487304688 \n",
      "\n",
      "epoch: 65 ( batch:  151 / 286 ) recon_loss: 99.14825439453125  perplexity:  772.2867431640625  codebook loss:  933.1539916992188 \n",
      " total_loss:  192.46365356445312 \n",
      "\n",
      "epoch: 65 ( batch:  201 / 286 ) recon_loss: 94.91273498535156  perplexity:  745.9651489257812  codebook loss:  860.61767578125 \n",
      " total_loss:  180.97450256347656 \n",
      "\n",
      "epoch: 65 ( batch:  251 / 286 ) recon_loss: 112.14549255371094  perplexity:  774.5578002929688  codebook loss:  1052.7393798828125 \n",
      " total_loss:  217.41943359375 \n",
      "\n",
      "Validation - epoch: 65  average recon_loss: 1.0666094952159457\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 66 ( batch:  1 / 286 ) recon_loss: 112.71419525146484  perplexity:  767.8327026367188  codebook loss:  987.6365966796875 \n",
      " total_loss:  211.4778594970703 \n",
      "\n",
      "epoch: 66 ( batch:  51 / 286 ) recon_loss: 89.94633483886719  perplexity:  723.9156494140625  codebook loss:  783.5648193359375 \n",
      " total_loss:  168.30282592773438 \n",
      "\n",
      "epoch: 66 ( batch:  101 / 286 ) recon_loss: 91.13356018066406  perplexity:  758.7803955078125  codebook loss:  846.3180541992188 \n",
      " total_loss:  175.76536560058594 \n",
      "\n",
      "epoch: 66 ( batch:  151 / 286 ) recon_loss: 95.22119903564453  perplexity:  708.6847534179688  codebook loss:  797.2901000976562 \n",
      " total_loss:  174.95021057128906 \n",
      "\n",
      "epoch: 66 ( batch:  201 / 286 ) recon_loss: 85.84883880615234  perplexity:  750.768310546875  codebook loss:  862.2906494140625 \n",
      " total_loss:  172.07791137695312 \n",
      "\n",
      "epoch: 66 ( batch:  251 / 286 ) recon_loss: 104.25856018066406  perplexity:  743.384033203125  codebook loss:  856.53857421875 \n",
      " total_loss:  189.91241455078125 \n",
      "\n",
      "Validation - epoch: 66  average recon_loss: 1.0871911694606144\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 67 ( batch:  1 / 286 ) recon_loss: 76.67373657226562  perplexity:  702.7457275390625  codebook loss:  669.348876953125 \n",
      " total_loss:  143.60862731933594 \n",
      "\n",
      "epoch: 67 ( batch:  51 / 286 ) recon_loss: 109.77643585205078  perplexity:  746.1931762695312  codebook loss:  879.2485961914062 \n",
      " total_loss:  197.7012939453125 \n",
      "\n",
      "epoch: 67 ( batch:  101 / 286 ) recon_loss: 105.74285125732422  perplexity:  772.0765380859375  codebook loss:  1000.8120727539062 \n",
      " total_loss:  205.82406616210938 \n",
      "\n",
      "epoch: 67 ( batch:  151 / 286 ) recon_loss: 83.91118621826172  perplexity:  674.1949462890625  codebook loss:  629.853759765625 \n",
      " total_loss:  146.8965606689453 \n",
      "\n",
      "epoch: 67 ( batch:  201 / 286 ) recon_loss: 97.35914611816406  perplexity:  763.1160888671875  codebook loss:  919.3047485351562 \n",
      " total_loss:  189.28961181640625 \n",
      "\n",
      "epoch: 67 ( batch:  251 / 286 ) recon_loss: 84.53441619873047  perplexity:  692.7182006835938  codebook loss:  748.66455078125 \n",
      " total_loss:  159.40087890625 \n",
      "\n",
      "Validation - epoch: 67  average recon_loss: 1.0859299914704428\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 68 ( batch:  1 / 286 ) recon_loss: 88.02826690673828  perplexity:  719.6609497070312  codebook loss:  726.9888916015625 \n",
      " total_loss:  160.72715759277344 \n",
      "\n",
      "epoch: 68 ( batch:  51 / 286 ) recon_loss: 119.63666534423828  perplexity:  772.3040771484375  codebook loss:  1068.90380859375 \n",
      " total_loss:  226.52703857421875 \n",
      "\n",
      "epoch: 68 ( batch:  101 / 286 ) recon_loss: 100.72108459472656  perplexity:  743.622314453125  codebook loss:  874.0546264648438 \n",
      " total_loss:  188.12655639648438 \n",
      "\n",
      "epoch: 68 ( batch:  151 / 286 ) recon_loss: 100.38064575195312  perplexity:  719.820556640625  codebook loss:  759.51416015625 \n",
      " total_loss:  176.33206176757812 \n",
      "\n",
      "epoch: 68 ( batch:  201 / 286 ) recon_loss: 84.35160827636719  perplexity:  728.81103515625  codebook loss:  759.4619140625 \n",
      " total_loss:  160.29779052734375 \n",
      "\n",
      "epoch: 68 ( batch:  251 / 286 ) recon_loss: 138.9790802001953  perplexity:  776.1412963867188  codebook loss:  1119.005615234375 \n",
      " total_loss:  250.879638671875 \n",
      "\n",
      "Validation - epoch: 68  average recon_loss: 1.0894216150045395\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 69 ( batch:  1 / 286 ) recon_loss: 100.01034545898438  perplexity:  732.517333984375  codebook loss:  788.6620483398438 \n",
      " total_loss:  178.87655639648438 \n",
      "\n",
      "epoch: 69 ( batch:  51 / 286 ) recon_loss: 118.80796813964844  perplexity:  739.7884521484375  codebook loss:  1011.6541137695312 \n",
      " total_loss:  219.973388671875 \n",
      "\n",
      "epoch: 69 ( batch:  101 / 286 ) recon_loss: 76.8262710571289  perplexity:  688.0609130859375  codebook loss:  651.851318359375 \n",
      " total_loss:  142.01141357421875 \n",
      "\n",
      "epoch: 69 ( batch:  151 / 286 ) recon_loss: 100.8682632446289  perplexity:  731.97509765625  codebook loss:  817.482177734375 \n",
      " total_loss:  182.61648559570312 \n",
      "\n",
      "epoch: 69 ( batch:  201 / 286 ) recon_loss: 103.79948425292969  perplexity:  755.7613525390625  codebook loss:  857.7645874023438 \n",
      " total_loss:  189.57594299316406 \n",
      "\n",
      "epoch: 69 ( batch:  251 / 286 ) recon_loss: 78.07605743408203  perplexity:  702.9193115234375  codebook loss:  713.561279296875 \n",
      " total_loss:  149.43218994140625 \n",
      "\n",
      "Validation - epoch: 69  average recon_loss: 1.101060567630662\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 70 ( batch:  1 / 286 ) recon_loss: 126.18534851074219  perplexity:  769.082275390625  codebook loss:  1028.207275390625 \n",
      " total_loss:  229.00607299804688 \n",
      "\n",
      "epoch: 70 ( batch:  51 / 286 ) recon_loss: 99.1244888305664  perplexity:  717.13427734375  codebook loss:  837.699951171875 \n",
      " total_loss:  182.8944854736328 \n",
      "\n",
      "epoch: 70 ( batch:  101 / 286 ) recon_loss: 144.00167846679688  perplexity:  782.2603759765625  codebook loss:  1071.2828369140625 \n",
      " total_loss:  251.12997436523438 \n",
      "\n",
      "epoch: 70 ( batch:  151 / 286 ) recon_loss: 93.936767578125  perplexity:  707.1353149414062  codebook loss:  714.95654296875 \n",
      " total_loss:  165.43243408203125 \n",
      "\n",
      "epoch: 70 ( batch:  201 / 286 ) recon_loss: 93.20994567871094  perplexity:  744.4073486328125  codebook loss:  796.2704467773438 \n",
      " total_loss:  172.8369903564453 \n",
      "\n",
      "epoch: 70 ( batch:  251 / 286 ) recon_loss: 85.59949493408203  perplexity:  700.6895141601562  codebook loss:  728.06884765625 \n",
      " total_loss:  158.4063720703125 \n",
      "\n",
      "Validation - epoch: 70  average recon_loss: 1.1074000630113814\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 71 ( batch:  1 / 286 ) recon_loss: 89.4380874633789  perplexity:  723.619140625  codebook loss:  737.63427734375 \n",
      " total_loss:  163.20150756835938 \n",
      "\n",
      "epoch: 71 ( batch:  51 / 286 ) recon_loss: 92.78346252441406  perplexity:  761.5105590820312  codebook loss:  881.4927978515625 \n",
      " total_loss:  180.9327392578125 \n",
      "\n",
      "epoch: 71 ( batch:  101 / 286 ) recon_loss: 118.92095947265625  perplexity:  742.6587524414062  codebook loss:  925.2822875976562 \n",
      " total_loss:  211.44918823242188 \n",
      "\n",
      "epoch: 71 ( batch:  151 / 286 ) recon_loss: 100.40058135986328  perplexity:  745.8285522460938  codebook loss:  851.917724609375 \n",
      " total_loss:  185.59234619140625 \n",
      "\n",
      "epoch: 71 ( batch:  201 / 286 ) recon_loss: 96.58220672607422  perplexity:  757.5906372070312  codebook loss:  852.784912109375 \n",
      " total_loss:  181.86070251464844 \n",
      "\n",
      "epoch: 71 ( batch:  251 / 286 ) recon_loss: 88.57829284667969  perplexity:  690.2429809570312  codebook loss:  693.13818359375 \n",
      " total_loss:  157.89212036132812 \n",
      "\n",
      "Validation - epoch: 71  average recon_loss: 1.0840213977628284\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 72 ( batch:  1 / 286 ) recon_loss: 94.20048522949219  perplexity:  746.7659301757812  codebook loss:  796.0587158203125 \n",
      " total_loss:  173.80636596679688 \n",
      "\n",
      "epoch: 72 ( batch:  51 / 286 ) recon_loss: 108.21444702148438  perplexity:  750.7049560546875  codebook loss:  844.4642333984375 \n",
      " total_loss:  192.66087341308594 \n",
      "\n",
      "epoch: 72 ( batch:  101 / 286 ) recon_loss: 100.68670654296875  perplexity:  715.638671875  codebook loss:  844.0051879882812 \n",
      " total_loss:  185.08721923828125 \n",
      "\n",
      "epoch: 72 ( batch:  151 / 286 ) recon_loss: 90.81502532958984  perplexity:  710.7999267578125  codebook loss:  773.2428588867188 \n",
      " total_loss:  168.13931274414062 \n",
      "\n",
      "epoch: 72 ( batch:  201 / 286 ) recon_loss: 97.88838958740234  perplexity:  735.5590209960938  codebook loss:  845.180908203125 \n",
      " total_loss:  182.40647888183594 \n",
      "\n",
      "epoch: 72 ( batch:  251 / 286 ) recon_loss: 104.10891723632812  perplexity:  702.806396484375  codebook loss:  752.5609741210938 \n",
      " total_loss:  179.36502075195312 \n",
      "\n",
      "Validation - epoch: 72  average recon_loss: 1.0762518694003422\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 73 ( batch:  1 / 286 ) recon_loss: 94.54131317138672  perplexity:  711.4319458007812  codebook loss:  807.537353515625 \n",
      " total_loss:  175.2950439453125 \n",
      "\n",
      "epoch: 73 ( batch:  51 / 286 ) recon_loss: 91.68108367919922  perplexity:  727.838623046875  codebook loss:  791.3842163085938 \n",
      " total_loss:  170.8195037841797 \n",
      "\n",
      "epoch: 73 ( batch:  101 / 286 ) recon_loss: 100.05298614501953  perplexity:  748.7615966796875  codebook loss:  812.59130859375 \n",
      " total_loss:  181.31211853027344 \n",
      "\n",
      "epoch: 73 ( batch:  151 / 286 ) recon_loss: 80.3786849975586  perplexity:  679.5961303710938  codebook loss:  647.6754150390625 \n",
      " total_loss:  145.14622497558594 \n",
      "\n",
      "epoch: 73 ( batch:  201 / 286 ) recon_loss: 92.93782806396484  perplexity:  720.92626953125  codebook loss:  719.9303588867188 \n",
      " total_loss:  164.9308624267578 \n",
      "\n",
      "epoch: 73 ( batch:  251 / 286 ) recon_loss: 109.53539276123047  perplexity:  755.6719970703125  codebook loss:  873.7531127929688 \n",
      " total_loss:  196.91070556640625 \n",
      "\n",
      "Validation - epoch: 73  average recon_loss: 1.110081700815095\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 74 ( batch:  1 / 286 ) recon_loss: 116.26049041748047  perplexity:  740.28955078125  codebook loss:  1023.9733276367188 \n",
      " total_loss:  218.65782165527344 \n",
      "\n",
      "epoch: 74 ( batch:  51 / 286 ) recon_loss: 127.56336975097656  perplexity:  739.17138671875  codebook loss:  955.6904296875 \n",
      " total_loss:  223.13241577148438 \n",
      "\n",
      "epoch: 74 ( batch:  101 / 286 ) recon_loss: 114.20668029785156  perplexity:  747.215087890625  codebook loss:  866.3924560546875 \n",
      " total_loss:  200.84591674804688 \n",
      "\n",
      "epoch: 74 ( batch:  151 / 286 ) recon_loss: 86.95762634277344  perplexity:  700.7677612304688  codebook loss:  732.5265502929688 \n",
      " total_loss:  160.2102813720703 \n",
      "\n",
      "epoch: 74 ( batch:  201 / 286 ) recon_loss: 106.44173431396484  perplexity:  722.7491455078125  codebook loss:  747.142578125 \n",
      " total_loss:  181.15599060058594 \n",
      "\n",
      "epoch: 74 ( batch:  251 / 286 ) recon_loss: 122.87550354003906  perplexity:  723.223876953125  codebook loss:  837.3790283203125 \n",
      " total_loss:  206.6134033203125 \n",
      "\n",
      "Validation - epoch: 74  average recon_loss: 1.1159194161494572\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 75 ( batch:  1 / 286 ) recon_loss: 128.2779998779297  perplexity:  731.9356689453125  codebook loss:  918.53955078125 \n",
      " total_loss:  220.1319580078125 \n",
      "\n",
      "epoch: 75 ( batch:  51 / 286 ) recon_loss: 127.68614196777344  perplexity:  738.3062133789062  codebook loss:  882.4199829101562 \n",
      " total_loss:  215.92813110351562 \n",
      "\n",
      "epoch: 75 ( batch:  101 / 286 ) recon_loss: 76.15869903564453  perplexity:  657.0208740234375  codebook loss:  566.8792114257812 \n",
      " total_loss:  132.84661865234375 \n",
      "\n",
      "epoch: 75 ( batch:  151 / 286 ) recon_loss: 88.39737701416016  perplexity:  722.25927734375  codebook loss:  758.2805786132812 \n",
      " total_loss:  164.22543334960938 \n",
      "\n",
      "epoch: 75 ( batch:  201 / 286 ) recon_loss: 117.91606140136719  perplexity:  751.019287109375  codebook loss:  887.093505859375 \n",
      " total_loss:  206.6254119873047 \n",
      "\n",
      "epoch: 75 ( batch:  251 / 286 ) recon_loss: 111.72132873535156  perplexity:  742.9814453125  codebook loss:  902.084716796875 \n",
      " total_loss:  201.9298095703125 \n",
      "\n",
      "Validation - epoch: 75  average recon_loss: 1.1152168528901205\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 76 ( batch:  1 / 286 ) recon_loss: 102.36699676513672  perplexity:  750.5650024414062  codebook loss:  825.2947387695312 \n",
      " total_loss:  184.89646911621094 \n",
      "\n",
      "epoch: 76 ( batch:  51 / 286 ) recon_loss: 99.7383804321289  perplexity:  727.1170654296875  codebook loss:  747.8229370117188 \n",
      " total_loss:  174.5206756591797 \n",
      "\n",
      "epoch: 76 ( batch:  101 / 286 ) recon_loss: 115.1285400390625  perplexity:  747.4912719726562  codebook loss:  780.4705810546875 \n",
      " total_loss:  193.17559814453125 \n",
      "\n",
      "epoch: 76 ( batch:  151 / 286 ) recon_loss: 93.75189208984375  perplexity:  727.3167724609375  codebook loss:  761.7474975585938 \n",
      " total_loss:  169.9266357421875 \n",
      "\n",
      "epoch: 76 ( batch:  201 / 286 ) recon_loss: 97.20451354980469  perplexity:  734.149658203125  codebook loss:  796.2676391601562 \n",
      " total_loss:  176.83126831054688 \n",
      "\n",
      "epoch: 76 ( batch:  251 / 286 ) recon_loss: 100.41082000732422  perplexity:  696.6451416015625  codebook loss:  697.1129760742188 \n",
      " total_loss:  170.1221160888672 \n",
      "\n",
      "Validation - epoch: 76  average recon_loss: 1.1091696868340175\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 77 ( batch:  1 / 286 ) recon_loss: 139.1207733154297  perplexity:  714.730224609375  codebook loss:  888.9368286132812 \n",
      " total_loss:  228.01446533203125 \n",
      "\n",
      "epoch: 77 ( batch:  51 / 286 ) recon_loss: 114.291015625  perplexity:  738.0169067382812  codebook loss:  866.3011474609375 \n",
      " total_loss:  200.921142578125 \n",
      "\n",
      "epoch: 77 ( batch:  101 / 286 ) recon_loss: 88.62004852294922  perplexity:  710.3770751953125  codebook loss:  665.834716796875 \n",
      " total_loss:  155.20352172851562 \n",
      "\n",
      "epoch: 77 ( batch:  151 / 286 ) recon_loss: 101.31157684326172  perplexity:  719.6568603515625  codebook loss:  748.3206787109375 \n",
      " total_loss:  176.14364624023438 \n",
      "\n",
      "epoch: 77 ( batch:  201 / 286 ) recon_loss: 94.9898910522461  perplexity:  671.2479858398438  codebook loss:  611.0068969726562 \n",
      " total_loss:  156.090576171875 \n",
      "\n",
      "epoch: 77 ( batch:  251 / 286 ) recon_loss: 106.86869812011719  perplexity:  715.1004028320312  codebook loss:  740.1455688476562 \n",
      " total_loss:  180.8832550048828 \n",
      "\n",
      "Validation - epoch: 77  average recon_loss: 1.138184107012219\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 78 ( batch:  1 / 286 ) recon_loss: 117.02677917480469  perplexity:  711.08984375  codebook loss:  828.4920654296875 \n",
      " total_loss:  199.8759765625 \n",
      "\n",
      "epoch: 78 ( batch:  51 / 286 ) recon_loss: 94.73548889160156  perplexity:  694.6104125976562  codebook loss:  704.9414672851562 \n",
      " total_loss:  165.22964477539062 \n",
      "\n",
      "epoch: 78 ( batch:  101 / 286 ) recon_loss: 83.85838317871094  perplexity:  681.382080078125  codebook loss:  582.4100341796875 \n",
      " total_loss:  142.09939575195312 \n",
      "\n",
      "epoch: 78 ( batch:  151 / 286 ) recon_loss: 121.78071594238281  perplexity:  739.2178955078125  codebook loss:  853.4082641601562 \n",
      " total_loss:  207.12155151367188 \n",
      "\n",
      "epoch: 78 ( batch:  201 / 286 ) recon_loss: 130.21023559570312  perplexity:  723.1790161132812  codebook loss:  978.581787109375 \n",
      " total_loss:  228.06842041015625 \n",
      "\n",
      "epoch: 78 ( batch:  251 / 286 ) recon_loss: 94.02977752685547  perplexity:  693.7068481445312  codebook loss:  654.6790161132812 \n",
      " total_loss:  159.4976806640625 \n",
      "\n",
      "Validation - epoch: 78  average recon_loss: 1.13726640244325\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 79 ( batch:  1 / 286 ) recon_loss: 102.14385986328125  perplexity:  693.7326049804688  codebook loss:  688.43359375 \n",
      " total_loss:  170.98721313476562 \n",
      "\n",
      "epoch: 79 ( batch:  51 / 286 ) recon_loss: 94.51627349853516  perplexity:  728.8377685546875  codebook loss:  758.9651489257812 \n",
      " total_loss:  170.41278076171875 \n",
      "\n",
      "epoch: 79 ( batch:  101 / 286 ) recon_loss: 133.2603302001953  perplexity:  704.7184448242188  codebook loss:  837.6203002929688 \n",
      " total_loss:  217.02236938476562 \n",
      "\n",
      "epoch: 79 ( batch:  151 / 286 ) recon_loss: 89.58224487304688  perplexity:  676.9847412109375  codebook loss:  593.4036865234375 \n",
      " total_loss:  148.922607421875 \n",
      "\n",
      "epoch: 79 ( batch:  201 / 286 ) recon_loss: 105.42961120605469  perplexity:  701.7471313476562  codebook loss:  720.7825927734375 \n",
      " total_loss:  177.50787353515625 \n",
      "\n",
      "epoch: 79 ( batch:  251 / 286 ) recon_loss: 103.62353515625  perplexity:  700.934814453125  codebook loss:  743.438720703125 \n",
      " total_loss:  177.9674072265625 \n",
      "\n",
      "Validation - epoch: 79  average recon_loss: 1.0872461514340506\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 80 ( batch:  1 / 286 ) recon_loss: 136.5526580810547  perplexity:  730.7398681640625  codebook loss:  843.045166015625 \n",
      " total_loss:  220.857177734375 \n",
      "\n",
      "epoch: 80 ( batch:  51 / 286 ) recon_loss: 115.56501770019531  perplexity:  715.43603515625  codebook loss:  783.1771850585938 \n",
      " total_loss:  193.8827362060547 \n",
      "\n",
      "epoch: 80 ( batch:  101 / 286 ) recon_loss: 96.31438446044922  perplexity:  689.5941772460938  codebook loss:  707.9125366210938 \n",
      " total_loss:  167.1056365966797 \n",
      "\n",
      "epoch: 80 ( batch:  151 / 286 ) recon_loss: 111.84239196777344  perplexity:  675.104736328125  codebook loss:  729.89892578125 \n",
      " total_loss:  184.832275390625 \n",
      "\n",
      "epoch: 80 ( batch:  201 / 286 ) recon_loss: 90.93134307861328  perplexity:  708.3104248046875  codebook loss:  740.3602294921875 \n",
      " total_loss:  164.96737670898438 \n",
      "\n",
      "epoch: 80 ( batch:  251 / 286 ) recon_loss: 75.9375  perplexity:  649.4644775390625  codebook loss:  582.7469482421875 \n",
      " total_loss:  134.21218872070312 \n",
      "\n",
      "Validation - epoch: 80  average recon_loss: 1.1124996427032683\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 81 ( batch:  1 / 286 ) recon_loss: 86.60935974121094  perplexity:  701.63037109375  codebook loss:  636.1910400390625 \n",
      " total_loss:  150.2284698486328 \n",
      "\n",
      "epoch: 81 ( batch:  51 / 286 ) recon_loss: 111.21713256835938  perplexity:  708.4401245117188  codebook loss:  683.3892822265625 \n",
      " total_loss:  179.55606079101562 \n",
      "\n",
      "epoch: 81 ( batch:  101 / 286 ) recon_loss: 80.66584777832031  perplexity:  664.5044555664062  codebook loss:  548.4410400390625 \n",
      " total_loss:  135.50994873046875 \n",
      "\n",
      "epoch: 81 ( batch:  151 / 286 ) recon_loss: 96.64733123779297  perplexity:  664.4905395507812  codebook loss:  600.4877319335938 \n",
      " total_loss:  156.69610595703125 \n",
      "\n",
      "epoch: 81 ( batch:  201 / 286 ) recon_loss: 101.443603515625  perplexity:  682.4209594726562  codebook loss:  635.7705078125 \n",
      " total_loss:  165.02066040039062 \n",
      "\n",
      "epoch: 81 ( batch:  251 / 286 ) recon_loss: 87.72444152832031  perplexity:  684.0841674804688  codebook loss:  633.2899169921875 \n",
      " total_loss:  151.05343627929688 \n",
      "\n",
      "Validation - epoch: 81  average recon_loss: 1.121902318464385\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 82 ( batch:  1 / 286 ) recon_loss: 129.0050811767578  perplexity:  701.9158325195312  codebook loss:  779.4974365234375 \n",
      " total_loss:  206.954833984375 \n",
      "\n",
      "epoch: 82 ( batch:  51 / 286 ) recon_loss: 100.82079315185547  perplexity:  725.3157348632812  codebook loss:  784.4534912109375 \n",
      " total_loss:  179.26614379882812 \n",
      "\n",
      "epoch: 82 ( batch:  101 / 286 ) recon_loss: 141.1240692138672  perplexity:  704.9984130859375  codebook loss:  832.3812866210938 \n",
      " total_loss:  224.36219787597656 \n",
      "\n",
      "epoch: 82 ( batch:  151 / 286 ) recon_loss: 107.30361938476562  perplexity:  716.5586547851562  codebook loss:  799.1293334960938 \n",
      " total_loss:  187.216552734375 \n",
      "\n",
      "epoch: 82 ( batch:  201 / 286 ) recon_loss: 108.4955062866211  perplexity:  705.6973876953125  codebook loss:  739.0261840820312 \n",
      " total_loss:  182.39813232421875 \n",
      "\n",
      "epoch: 82 ( batch:  251 / 286 ) recon_loss: 84.3697738647461  perplexity:  679.9076538085938  codebook loss:  561.0621337890625 \n",
      " total_loss:  140.47598266601562 \n",
      "\n",
      "Validation - epoch: 82  average recon_loss: 1.1105862591001723\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 83 ( batch:  1 / 286 ) recon_loss: 98.13658905029297  perplexity:  710.41162109375  codebook loss:  685.6212768554688 \n",
      " total_loss:  166.69871520996094 \n",
      "\n",
      "epoch: 83 ( batch:  51 / 286 ) recon_loss: 109.71350860595703  perplexity:  717.0781860351562  codebook loss:  678.0221557617188 \n",
      " total_loss:  177.51571655273438 \n",
      "\n",
      "epoch: 83 ( batch:  101 / 286 ) recon_loss: 90.77409362792969  perplexity:  705.087890625  codebook loss:  644.9834594726562 \n",
      " total_loss:  155.27243041992188 \n",
      "\n",
      "epoch: 83 ( batch:  151 / 286 ) recon_loss: 75.25474548339844  perplexity:  625.8607788085938  codebook loss:  484.6246643066406 \n",
      " total_loss:  123.71720886230469 \n",
      "\n",
      "epoch: 83 ( batch:  201 / 286 ) recon_loss: 116.31524658203125  perplexity:  735.0457153320312  codebook loss:  917.9674072265625 \n",
      " total_loss:  208.11199951171875 \n",
      "\n",
      "epoch: 83 ( batch:  251 / 286 ) recon_loss: 103.94588470458984  perplexity:  706.1312866210938  codebook loss:  667.226318359375 \n",
      " total_loss:  170.66851806640625 \n",
      "\n",
      "Validation - epoch: 83  average recon_loss: 1.1183942142460082\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 84 ( batch:  1 / 286 ) recon_loss: 106.67355346679688  perplexity:  721.2418823242188  codebook loss:  767.6573486328125 \n",
      " total_loss:  183.43930053710938 \n",
      "\n",
      "epoch: 84 ( batch:  51 / 286 ) recon_loss: 103.01744079589844  perplexity:  679.672607421875  codebook loss:  622.225830078125 \n",
      " total_loss:  165.24002075195312 \n",
      "\n",
      "epoch: 84 ( batch:  101 / 286 ) recon_loss: 86.04285430908203  perplexity:  698.3959350585938  codebook loss:  629.4576416015625 \n",
      " total_loss:  148.98861694335938 \n",
      "\n",
      "epoch: 84 ( batch:  151 / 286 ) recon_loss: 114.44183349609375  perplexity:  713.6380615234375  codebook loss:  736.4923706054688 \n",
      " total_loss:  188.091064453125 \n",
      "\n",
      "epoch: 84 ( batch:  201 / 286 ) recon_loss: 145.5897979736328  perplexity:  777.0029296875  codebook loss:  936.6140747070312 \n",
      " total_loss:  239.25120544433594 \n",
      "\n",
      "epoch: 84 ( batch:  251 / 286 ) recon_loss: 103.5150146484375  perplexity:  690.8705444335938  codebook loss:  651.055908203125 \n",
      " total_loss:  168.62060546875 \n",
      "\n",
      "Validation - epoch: 84  average recon_loss: 1.1128820528586705\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 85 ( batch:  1 / 286 ) recon_loss: 117.17991638183594  perplexity:  726.3942260742188  codebook loss:  815.18701171875 \n",
      " total_loss:  198.6986083984375 \n",
      "\n",
      "epoch: 85 ( batch:  51 / 286 ) recon_loss: 142.15737915039062  perplexity:  711.3787841796875  codebook loss:  735.0536499023438 \n",
      " total_loss:  215.66275024414062 \n",
      "\n",
      "epoch: 85 ( batch:  101 / 286 ) recon_loss: 81.56654357910156  perplexity:  680.117431640625  codebook loss:  568.6364135742188 \n",
      " total_loss:  138.43019104003906 \n",
      "\n",
      "epoch: 85 ( batch:  151 / 286 ) recon_loss: 103.1667251586914  perplexity:  696.921875  codebook loss:  674.0386962890625 \n",
      " total_loss:  170.57058715820312 \n",
      "\n",
      "epoch: 85 ( batch:  201 / 286 ) recon_loss: 104.19156646728516  perplexity:  730.4873046875  codebook loss:  702.98779296875 \n",
      " total_loss:  174.4903564453125 \n",
      "\n",
      "epoch: 85 ( batch:  251 / 286 ) recon_loss: 104.66357421875  perplexity:  700.2760620117188  codebook loss:  717.6196899414062 \n",
      " total_loss:  176.425537109375 \n",
      "\n",
      "Validation - epoch: 85  average recon_loss: 1.1300149857997894\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 86 ( batch:  1 / 286 ) recon_loss: 106.19933319091797  perplexity:  692.024169921875  codebook loss:  626.3724975585938 \n",
      " total_loss:  168.83657836914062 \n",
      "\n",
      "epoch: 86 ( batch:  51 / 286 ) recon_loss: 126.0085220336914  perplexity:  702.8984985351562  codebook loss:  781.1648559570312 \n",
      " total_loss:  204.125 \n",
      "\n",
      "epoch: 86 ( batch:  101 / 286 ) recon_loss: 95.68221282958984  perplexity:  711.6763305664062  codebook loss:  644.0548095703125 \n",
      " total_loss:  160.0876922607422 \n",
      "\n",
      "epoch: 86 ( batch:  151 / 286 ) recon_loss: 98.98239135742188  perplexity:  684.0443725585938  codebook loss:  606.9791870117188 \n",
      " total_loss:  159.68031311035156 \n",
      "\n",
      "epoch: 86 ( batch:  201 / 286 ) recon_loss: 111.21137237548828  perplexity:  704.4446411132812  codebook loss:  658.701171875 \n",
      " total_loss:  177.08148193359375 \n",
      "\n",
      "epoch: 86 ( batch:  251 / 286 ) recon_loss: 92.67009735107422  perplexity:  711.2451171875  codebook loss:  651.2159423828125 \n",
      " total_loss:  157.79168701171875 \n",
      "\n",
      "Validation - epoch: 86  average recon_loss: 1.1405896527899637\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 87 ( batch:  1 / 286 ) recon_loss: 139.05026245117188  perplexity:  743.1599731445312  codebook loss:  953.586669921875 \n",
      " total_loss:  234.408935546875 \n",
      "\n",
      "epoch: 87 ( batch:  51 / 286 ) recon_loss: 114.97596740722656  perplexity:  725.3399047851562  codebook loss:  687.0977783203125 \n",
      " total_loss:  183.6857452392578 \n",
      "\n",
      "epoch: 87 ( batch:  101 / 286 ) recon_loss: 102.10794067382812  perplexity:  712.3383178710938  codebook loss:  690.4535522460938 \n",
      " total_loss:  171.15328979492188 \n",
      "\n",
      "epoch: 87 ( batch:  151 / 286 ) recon_loss: 106.91144561767578  perplexity:  676.0537109375  codebook loss:  708.6055908203125 \n",
      " total_loss:  177.77200317382812 \n",
      "\n",
      "epoch: 87 ( batch:  201 / 286 ) recon_loss: 111.29885864257812  perplexity:  713.9318237304688  codebook loss:  698.2678833007812 \n",
      " total_loss:  181.12564086914062 \n",
      "\n",
      "epoch: 87 ( batch:  251 / 286 ) recon_loss: 147.72946166992188  perplexity:  757.3052978515625  codebook loss:  944.1610107421875 \n",
      " total_loss:  242.14556884765625 \n",
      "\n",
      "Validation - epoch: 87  average recon_loss: 1.0894772476620145\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 88 ( batch:  1 / 286 ) recon_loss: 103.47021484375  perplexity:  703.664794921875  codebook loss:  624.3626098632812 \n",
      " total_loss:  165.90647888183594 \n",
      "\n",
      "epoch: 88 ( batch:  51 / 286 ) recon_loss: 102.5551528930664  perplexity:  730.0394897460938  codebook loss:  696.12451171875 \n",
      " total_loss:  172.1676025390625 \n",
      "\n",
      "epoch: 88 ( batch:  101 / 286 ) recon_loss: 92.72645568847656  perplexity:  693.4442749023438  codebook loss:  590.8502197265625 \n",
      " total_loss:  151.8114776611328 \n",
      "\n",
      "epoch: 88 ( batch:  151 / 286 ) recon_loss: 84.64787292480469  perplexity:  683.168212890625  codebook loss:  557.8612670898438 \n",
      " total_loss:  140.4340057373047 \n",
      "\n",
      "epoch: 88 ( batch:  201 / 286 ) recon_loss: 101.35103607177734  perplexity:  664.6930541992188  codebook loss:  613.4148559570312 \n",
      " total_loss:  162.69252014160156 \n",
      "\n",
      "epoch: 88 ( batch:  251 / 286 ) recon_loss: 100.89793395996094  perplexity:  658.36376953125  codebook loss:  530.708740234375 \n",
      " total_loss:  153.96881103515625 \n",
      "\n",
      "Validation - epoch: 88  average recon_loss: 1.1034902681907017\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 89 ( batch:  1 / 286 ) recon_loss: 84.55533599853516  perplexity:  679.0635986328125  codebook loss:  541.599853515625 \n",
      " total_loss:  138.71531677246094 \n",
      "\n",
      "epoch: 89 ( batch:  51 / 286 ) recon_loss: 101.51116943359375  perplexity:  710.0120239257812  codebook loss:  624.4722290039062 \n",
      " total_loss:  163.95838928222656 \n",
      "\n",
      "epoch: 89 ( batch:  101 / 286 ) recon_loss: 94.03233337402344  perplexity:  721.5322265625  codebook loss:  732.8839721679688 \n",
      " total_loss:  167.32073974609375 \n",
      "\n",
      "epoch: 89 ( batch:  151 / 286 ) recon_loss: 90.99925231933594  perplexity:  693.8431396484375  codebook loss:  579.0160522460938 \n",
      " total_loss:  148.90086364746094 \n",
      "\n",
      "epoch: 89 ( batch:  201 / 286 ) recon_loss: 114.24037170410156  perplexity:  697.2466430664062  codebook loss:  665.019775390625 \n",
      " total_loss:  180.74234008789062 \n",
      "\n",
      "epoch: 89 ( batch:  251 / 286 ) recon_loss: 101.86929321289062  perplexity:  703.3776245117188  codebook loss:  650.2120971679688 \n",
      " total_loss:  166.8905029296875 \n",
      "\n",
      "Validation - epoch: 89  average recon_loss: 1.1001265231106017\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 90 ( batch:  1 / 286 ) recon_loss: 137.2520294189453  perplexity:  714.501953125  codebook loss:  719.307373046875 \n",
      " total_loss:  209.18276977539062 \n",
      "\n",
      "epoch: 90 ( batch:  51 / 286 ) recon_loss: 137.13763427734375  perplexity:  729.447265625  codebook loss:  981.1519775390625 \n",
      " total_loss:  235.25283813476562 \n",
      "\n",
      "epoch: 90 ( batch:  101 / 286 ) recon_loss: 110.86900329589844  perplexity:  682.8900756835938  codebook loss:  721.9876708984375 \n",
      " total_loss:  183.06777954101562 \n",
      "\n",
      "epoch: 90 ( batch:  151 / 286 ) recon_loss: 95.59993743896484  perplexity:  690.1744995117188  codebook loss:  558.0858154296875 \n",
      " total_loss:  151.4085235595703 \n",
      "\n",
      "epoch: 90 ( batch:  201 / 286 ) recon_loss: 160.76242065429688  perplexity:  724.9495849609375  codebook loss:  743.9434814453125 \n",
      " total_loss:  235.15676879882812 \n",
      "\n",
      "epoch: 90 ( batch:  251 / 286 ) recon_loss: 104.87757873535156  perplexity:  718.6524047851562  codebook loss:  617.297607421875 \n",
      " total_loss:  166.6073455810547 \n",
      "\n",
      "Validation - epoch: 90  average recon_loss: 1.1276866892973583\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 91 ( batch:  1 / 286 ) recon_loss: 79.00013732910156  perplexity:  680.754638671875  codebook loss:  538.0301513671875 \n",
      " total_loss:  132.80316162109375 \n",
      "\n",
      "epoch: 91 ( batch:  51 / 286 ) recon_loss: 79.610595703125  perplexity:  649.2294311523438  codebook loss:  516.890869140625 \n",
      " total_loss:  131.2996826171875 \n",
      "\n",
      "epoch: 91 ( batch:  101 / 286 ) recon_loss: 139.60720825195312  perplexity:  716.7134399414062  codebook loss:  934.6641845703125 \n",
      " total_loss:  233.07363891601562 \n",
      "\n",
      "epoch: 91 ( batch:  151 / 286 ) recon_loss: 153.98907470703125  perplexity:  740.1472778320312  codebook loss:  802.7568359375 \n",
      " total_loss:  234.2647705078125 \n",
      "\n",
      "epoch: 91 ( batch:  201 / 286 ) recon_loss: 119.10868835449219  perplexity:  692.1165771484375  codebook loss:  715.4159545898438 \n",
      " total_loss:  190.65028381347656 \n",
      "\n",
      "epoch: 91 ( batch:  251 / 286 ) recon_loss: 117.85794830322266  perplexity:  729.8787231445312  codebook loss:  727.9149780273438 \n",
      " total_loss:  190.64944458007812 \n",
      "\n",
      "Validation - epoch: 91  average recon_loss: 1.1182404028044806\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 92 ( batch:  1 / 286 ) recon_loss: 87.20990753173828  perplexity:  654.318359375  codebook loss:  526.7490844726562 \n",
      " total_loss:  139.8848114013672 \n",
      "\n",
      "epoch: 92 ( batch:  51 / 286 ) recon_loss: 115.09696197509766  perplexity:  731.5546875  codebook loss:  769.0062866210938 \n",
      " total_loss:  191.99758911132812 \n",
      "\n",
      "epoch: 92 ( batch:  101 / 286 ) recon_loss: 93.28787994384766  perplexity:  693.8077392578125  codebook loss:  594.8278198242188 \n",
      " total_loss:  152.77066040039062 \n",
      "\n",
      "epoch: 92 ( batch:  151 / 286 ) recon_loss: 113.55226135253906  perplexity:  726.1212768554688  codebook loss:  807.1539916992188 \n",
      " total_loss:  194.26766967773438 \n",
      "\n",
      "epoch: 92 ( batch:  201 / 286 ) recon_loss: 102.9672622680664  perplexity:  687.2369384765625  codebook loss:  610.2764282226562 \n",
      " total_loss:  163.99490356445312 \n",
      "\n",
      "epoch: 92 ( batch:  251 / 286 ) recon_loss: 121.2682876586914  perplexity:  749.9525756835938  codebook loss:  832.5694580078125 \n",
      " total_loss:  204.52523803710938 \n",
      "\n",
      "Validation - epoch: 92  average recon_loss: 1.1300293786658182\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 93 ( batch:  1 / 286 ) recon_loss: 154.93177795410156  perplexity:  748.121337890625  codebook loss:  822.9251708984375 \n",
      " total_loss:  237.22430419921875 \n",
      "\n",
      "epoch: 93 ( batch:  51 / 286 ) recon_loss: 98.958984375  perplexity:  654.7930908203125  codebook loss:  529.9151000976562 \n",
      " total_loss:  151.95050048828125 \n",
      "\n",
      "epoch: 93 ( batch:  101 / 286 ) recon_loss: 83.95742797851562  perplexity:  686.3582763671875  codebook loss:  556.3291015625 \n",
      " total_loss:  139.59033203125 \n",
      "\n",
      "epoch: 93 ( batch:  151 / 286 ) recon_loss: 95.2571792602539  perplexity:  709.8400268554688  codebook loss:  642.7479248046875 \n",
      " total_loss:  159.531982421875 \n",
      "\n",
      "epoch: 93 ( batch:  201 / 286 ) recon_loss: 103.05670166015625  perplexity:  714.2171020507812  codebook loss:  634.4768676757812 \n",
      " total_loss:  166.50439453125 \n",
      "\n",
      "epoch: 93 ( batch:  251 / 286 ) recon_loss: 98.38719177246094  perplexity:  712.1488037109375  codebook loss:  612.1341552734375 \n",
      " total_loss:  159.60061645507812 \n",
      "\n",
      "Validation - epoch: 93  average recon_loss: 1.0888888405428991\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 94 ( batch:  1 / 286 ) recon_loss: 104.93937683105469  perplexity:  743.1827392578125  codebook loss:  729.9268798828125 \n",
      " total_loss:  177.93206787109375 \n",
      "\n",
      "epoch: 94 ( batch:  51 / 286 ) recon_loss: 111.8343734741211  perplexity:  673.3673095703125  codebook loss:  585.0172119140625 \n",
      " total_loss:  170.33609008789062 \n",
      "\n",
      "epoch: 94 ( batch:  101 / 286 ) recon_loss: 122.00308990478516  perplexity:  729.894775390625  codebook loss:  721.1961669921875 \n",
      " total_loss:  194.12271118164062 \n",
      "\n",
      "epoch: 94 ( batch:  151 / 286 ) recon_loss: 92.57552337646484  perplexity:  699.5651245117188  codebook loss:  668.7535400390625 \n",
      " total_loss:  159.4508819580078 \n",
      "\n",
      "epoch: 94 ( batch:  201 / 286 ) recon_loss: 113.92161560058594  perplexity:  717.1417846679688  codebook loss:  637.60791015625 \n",
      " total_loss:  177.68240356445312 \n",
      "\n",
      "epoch: 94 ( batch:  251 / 286 ) recon_loss: 122.3541030883789  perplexity:  757.6245727539062  codebook loss:  745.28759765625 \n",
      " total_loss:  196.88287353515625 \n",
      "\n",
      "Validation - epoch: 94  average recon_loss: 1.121265063683192\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 95 ( batch:  1 / 286 ) recon_loss: 91.013427734375  perplexity:  692.9966430664062  codebook loss:  572.8850708007812 \n",
      " total_loss:  148.30194091796875 \n",
      "\n",
      "epoch: 95 ( batch:  51 / 286 ) recon_loss: 91.86109924316406  perplexity:  695.9000854492188  codebook loss:  581.8331909179688 \n",
      " total_loss:  150.04441833496094 \n",
      "\n",
      "epoch: 95 ( batch:  101 / 286 ) recon_loss: 84.72432708740234  perplexity:  699.2656860351562  codebook loss:  584.8363647460938 \n",
      " total_loss:  143.2079620361328 \n",
      "\n",
      "epoch: 95 ( batch:  151 / 286 ) recon_loss: 115.60079193115234  perplexity:  727.1978759765625  codebook loss:  719.8297729492188 \n",
      " total_loss:  187.58377075195312 \n",
      "\n",
      "epoch: 95 ( batch:  201 / 286 ) recon_loss: 103.08539581298828  perplexity:  720.7784423828125  codebook loss:  647.2094116210938 \n",
      " total_loss:  167.80633544921875 \n",
      "\n",
      "epoch: 95 ( batch:  251 / 286 ) recon_loss: 136.28956604003906  perplexity:  750.8528442382812  codebook loss:  847.5069580078125 \n",
      " total_loss:  221.04025268554688 \n",
      "\n",
      "Validation - epoch: 95  average recon_loss: 1.1077944884697597\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 96 ( batch:  1 / 286 ) recon_loss: 86.03671264648438  perplexity:  711.8056030273438  codebook loss:  623.6272583007812 \n",
      " total_loss:  148.39944458007812 \n",
      "\n",
      "epoch: 96 ( batch:  51 / 286 ) recon_loss: 156.69259643554688  perplexity:  739.5010375976562  codebook loss:  858.4631958007812 \n",
      " total_loss:  242.53890991210938 \n",
      "\n",
      "epoch: 96 ( batch:  101 / 286 ) recon_loss: 122.80171966552734  perplexity:  735.2476196289062  codebook loss:  732.0303955078125 \n",
      " total_loss:  196.0047607421875 \n",
      "\n",
      "epoch: 96 ( batch:  151 / 286 ) recon_loss: 124.76551818847656  perplexity:  708.9585571289062  codebook loss:  633.67626953125 \n",
      " total_loss:  188.13314819335938 \n",
      "\n",
      "epoch: 96 ( batch:  201 / 286 ) recon_loss: 120.1414794921875  perplexity:  700.6541748046875  codebook loss:  800.4468994140625 \n",
      " total_loss:  200.18617248535156 \n",
      "\n",
      "epoch: 96 ( batch:  251 / 286 ) recon_loss: 91.69805908203125  perplexity:  717.2806396484375  codebook loss:  612.8379516601562 \n",
      " total_loss:  152.9818572998047 \n",
      "\n",
      "Validation - epoch: 96  average recon_loss: 1.1212323589457407\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 97 ( batch:  1 / 286 ) recon_loss: 94.96467590332031  perplexity:  697.68896484375  codebook loss:  560.3924560546875 \n",
      " total_loss:  151.00392150878906 \n",
      "\n",
      "epoch: 97 ( batch:  51 / 286 ) recon_loss: 103.86920166015625  perplexity:  724.6264038085938  codebook loss:  663.0011596679688 \n",
      " total_loss:  170.1693115234375 \n",
      "\n",
      "epoch: 97 ( batch:  101 / 286 ) recon_loss: 93.25792694091797  perplexity:  726.787109375  codebook loss:  643.5364379882812 \n",
      " total_loss:  157.611572265625 \n",
      "\n",
      "epoch: 97 ( batch:  151 / 286 ) recon_loss: 119.26952362060547  perplexity:  733.3981323242188  codebook loss:  705.6336669921875 \n",
      " total_loss:  189.8328857421875 \n",
      "\n",
      "epoch: 97 ( batch:  201 / 286 ) recon_loss: 89.37196350097656  perplexity:  712.6284790039062  codebook loss:  590.6046752929688 \n",
      " total_loss:  148.43243408203125 \n",
      "\n",
      "epoch: 97 ( batch:  251 / 286 ) recon_loss: 120.46356201171875  perplexity:  747.41748046875  codebook loss:  781.7669677734375 \n",
      " total_loss:  198.6402587890625 \n",
      "\n",
      "Validation - epoch: 97  average recon_loss: 1.163688553704156\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 98 ( batch:  1 / 286 ) recon_loss: 83.32550811767578  perplexity:  655.6300659179688  codebook loss:  503.6745910644531 \n",
      " total_loss:  133.69296264648438 \n",
      "\n",
      "epoch: 98 ( batch:  51 / 286 ) recon_loss: 154.38241577148438  perplexity:  786.3727416992188  codebook loss:  877.8274536132812 \n",
      " total_loss:  242.1651611328125 \n",
      "\n",
      "epoch: 98 ( batch:  101 / 286 ) recon_loss: 83.43030548095703  perplexity:  685.0742797851562  codebook loss:  549.3009033203125 \n",
      " total_loss:  138.3603973388672 \n",
      "\n",
      "epoch: 98 ( batch:  151 / 286 ) recon_loss: 84.63809204101562  perplexity:  658.6921997070312  codebook loss:  500.82550048828125 \n",
      " total_loss:  134.72064208984375 \n",
      "\n",
      "epoch: 98 ( batch:  201 / 286 ) recon_loss: 106.56990814208984  perplexity:  735.4871215820312  codebook loss:  648.4540405273438 \n",
      " total_loss:  171.41531372070312 \n",
      "\n",
      "epoch: 98 ( batch:  251 / 286 ) recon_loss: 103.63216400146484  perplexity:  683.621826171875  codebook loss:  569.6384887695312 \n",
      " total_loss:  160.59600830078125 \n",
      "\n",
      "Validation - epoch: 98  average recon_loss: 1.0585169974300597\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 99 ( batch:  1 / 286 ) recon_loss: 99.56233978271484  perplexity:  690.5827026367188  codebook loss:  586.8663330078125 \n",
      " total_loss:  158.2489776611328 \n",
      "\n",
      "epoch: 99 ( batch:  51 / 286 ) recon_loss: 96.55939483642578  perplexity:  716.0722045898438  codebook loss:  640.3259887695312 \n",
      " total_loss:  160.5919952392578 \n",
      "\n",
      "epoch: 99 ( batch:  101 / 286 ) recon_loss: 80.41625213623047  perplexity:  688.92236328125  codebook loss:  539.6876831054688 \n",
      " total_loss:  134.38502502441406 \n",
      "\n",
      "epoch: 99 ( batch:  151 / 286 ) recon_loss: 86.81019592285156  perplexity:  705.567138671875  codebook loss:  605.4210205078125 \n",
      " total_loss:  147.352294921875 \n",
      "\n",
      "epoch: 99 ( batch:  201 / 286 ) recon_loss: 119.8192138671875  perplexity:  726.7431030273438  codebook loss:  787.2540893554688 \n",
      " total_loss:  198.54461669921875 \n",
      "\n",
      "epoch: 99 ( batch:  251 / 286 ) recon_loss: 111.90800476074219  perplexity:  747.8834228515625  codebook loss:  768.9352416992188 \n",
      " total_loss:  188.80152893066406 \n",
      "\n",
      "Validation - epoch: 99  average recon_loss: 1.1355799900160894\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "epoch: 100 ( batch:  1 / 286 ) recon_loss: 87.92430114746094  perplexity:  701.9061279296875  codebook loss:  586.2353515625 \n",
      " total_loss:  146.54783630371094 \n",
      "\n",
      "epoch: 100 ( batch:  51 / 286 ) recon_loss: 96.25690460205078  perplexity:  723.3490600585938  codebook loss:  662.905517578125 \n",
      " total_loss:  162.54745483398438 \n",
      "\n",
      "epoch: 100 ( batch:  101 / 286 ) recon_loss: 129.3270263671875  perplexity:  717.8623046875  codebook loss:  674.158203125 \n",
      " total_loss:  196.74285888671875 \n",
      "\n",
      "epoch: 100 ( batch:  151 / 286 ) recon_loss: 101.19864654541016  perplexity:  728.0441284179688  codebook loss:  663.3023681640625 \n",
      " total_loss:  167.5288848876953 \n",
      "\n",
      "epoch: 100 ( batch:  201 / 286 ) recon_loss: 82.85418701171875  perplexity:  691.4874877929688  codebook loss:  556.1600341796875 \n",
      " total_loss:  138.47018432617188 \n",
      "\n",
      "epoch: 100 ( batch:  251 / 286 ) recon_loss: 113.50041961669922  perplexity:  720.7320556640625  codebook loss:  799.4937744140625 \n",
      " total_loss:  193.44979858398438 \n",
      "\n",
      "Validation - epoch: 100  average recon_loss: 1.0484175384044647\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "Finish!!\n"
     ]
    }
   ],
   "source": [
    "print(\"Start training VQ-VAE...\")\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "commitment_beta = 0.1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    overall_loss = 0\n",
    "    for batch_idx, x in enumerate(train_loader):\n",
    "        x = x.transpose(2,1)\n",
    "        x = x.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #x_hat, commitment_loss, codebook_loss, perplexity = model(x)\n",
    "        x_hat, commit_and_codebook_loss, perplexity = model(x)\n",
    "        # print(\"x_hat.size() \", x_hat.size())\n",
    "        # print(\"x.size() \", x.size())\n",
    "        recon_loss = 100 * euclidean_loss(x_hat, x)\n",
    "        #recon_loss = mse_loss(x_hat, x)\n",
    "        loss = recon_loss + commit_and_codebook_loss * commitment_beta\n",
    "        overall_loss += recon_loss.item()    \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 2)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % print_step ==0: \n",
    "            print(\"epoch:\", epoch + 1, \"( batch: \", batch_idx + 1, \"/\", len(train_loader),\") recon_loss:\", recon_loss.item(), \" perplexity: \", perplexity.item(), \n",
    "              \" codebook loss: \", commit_and_codebook_loss.item(), \"\\n total_loss: \", loss.item(), \"\\n\")\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "    # Validation Loop\n",
    "    model.eval()  # switch to evaluation mode\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, x_val in enumerate(val_loader):\n",
    "            x_val = x_val.transpose(2,1)\n",
    "            x_val = x_val.to(device)\n",
    "            x_hat_val = model(x_val)[0]\n",
    "            recon_loss_val = euclidean_loss(x_hat_val, x_val)\n",
    "            #recon_loss_val = mse_loss(x_hat_val, x_val)\n",
    "            val_loss += recon_loss_val.item()\n",
    "    \n",
    "    overall_loss /= len(train_loader)\n",
    "    val_loss /= len(val_loader)\n",
    "    print(\"Validation - epoch:\", epoch + 1, \" average recon_loss:\", val_loss)\n",
    "    model.train()  # switch back to training mode\n",
    "    writer.add_scalar('recon_Loss/train', overall_loss, epoch)\n",
    "    writer.add_scalar('recon_Loss/validation', val_loss, epoch)\n",
    "    writer.add_scalar('perplexity', perplexity.item(), epoch)\n",
    "\n",
    "    # restart dead codes and reset usage\n",
    "    #model.quantizer.random_restart_dead_codes()\n",
    "    #model.quantizer.reset_usage()\n",
    "    \n",
    "    model.quantizer.random_restart()\n",
    "    model.quantizer.reset_usage()\n",
    "\n",
    "torch.save(model.state_dict(), \"saved_models/epoch_\" + str(epoch) + model_name)\n",
    "print(\"Finish!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from zip file\n",
      "ZipPoseDataset @ lexicon_jsons/lexicon_poses_norm_test2.zip with max_length=30, in_memory=True\n",
      "Total files 2141\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n",
      "1550\n",
      "1600\n",
      "1650\n",
      "1700\n",
      "1750\n",
      "1800\n",
      "1850\n",
      "1900\n",
      "1950\n",
      "2000\n",
      "2050\n",
      "2100\n",
      "torch.Size([60, 112, 30])\n",
      "torch.Size([60, 112, 30])\n",
      "torch.Size([60, 112, 30])\n",
      "torch.Size([60, 112, 30])\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "test_dataset, test_loader = load_lexicon_data(\n",
    "    df_test_sign, 'lexicon_jsons/lexicon_poses_norm_test2.zip',\n",
    "    normalize_by_mean_pose=True, num_workers=0,  # num workers here has to be 0, cause I am already threading in the dataset object\n",
    "    batch_size=60\n",
    ")\n",
    "i = 0\n",
    "for batch in test_loader:\n",
    "    print(batch.shape)\n",
    "    i += 1\n",
    "    if i == 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test - epoch: 100  Average recon_loss on test data: 1.0480452362034056\n",
      "recon_loss_test_last:  tensor(0.9923, device='mps:0')  test_loss:  1.0480452362034056\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "def test(data_loader, model):\n",
    "    \"\"\"evaluation model\"\"\"\n",
    "    N = len(data_loader)\n",
    "    model.eval()  # switch to evaluation mode\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, x_test in enumerate(data_loader):\n",
    "            x_test = x_test.transpose(2,1)\n",
    "            x_test = x_test.to(device)\n",
    "            x_hat_test = model(x_test)[0]\n",
    "            recon_loss_test = euclidean_loss(x_hat_test, x_test)\n",
    "            #recon_loss_test = mse_loss(x_hat_test, x_test)\n",
    "            test_loss += recon_loss_test.item()\n",
    "    \n",
    "    test_loss /= N\n",
    "    print(\"Test - epoch:\", epoch + 1, \" Average recon_loss on test data:\", test_loss)\n",
    "    return recon_loss_test, test_loss, x_hat_test, x_test\n",
    "\n",
    "\n",
    "recon_loss_test_last, test_loss, x_hat_test, x_test = test(test_loader, model)\n",
    "print(\"recon_loss_test_last: \", recon_loss_test_last, \" test_loss: \", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7324, -0.4336, -0.2795,  ...,  0.4219,  2.6309,  0.4199],\n",
       "         [-0.6162, -0.4597, -0.2869,  ...,  0.4226,  2.6484,  0.4207],\n",
       "         [-0.6392, -0.3948, -0.3213,  ...,  0.4236,  2.6348,  0.4216],\n",
       "         ...,\n",
       "         [-0.7705, -0.5552, -0.2006,  ..., -0.9316,  2.3867, -0.7998],\n",
       "         [-0.7251, -0.5654, -0.2236,  ..., -0.8877,  2.4004, -0.7563],\n",
       "         [-0.7925, -0.5835, -0.0421,  ..., -0.8584,  2.3398, -0.7417]],\n",
       "\n",
       "        [[-0.7319,  0.6021, -0.7559,  ...,  0.6187,  2.6309,  0.7715],\n",
       "         [-0.5845,  0.5298, -0.8755,  ...,  0.5596,  2.7109,  0.6865],\n",
       "         [-0.6821,  0.5068, -0.8022,  ...,  0.5063,  2.8223,  0.5923],\n",
       "         ...,\n",
       "         [ 0.2583,  0.4194, -0.3435,  ...,  0.4983,  2.8477,  0.5098],\n",
       "         [ 0.2476,  0.4238, -0.3606,  ...,  0.4832,  2.8496,  0.4949],\n",
       "         [ 0.1278,  0.4197, -0.3528,  ...,  0.4683,  2.8496,  0.4951]],\n",
       "\n",
       "        [[ 0.7588, -0.0859, -0.2386,  ...,  0.6221,  2.4844,  0.6318],\n",
       "         [ 0.7915, -0.0724, -0.2859,  ...,  0.6362,  2.4824,  0.6460],\n",
       "         [ 0.6538, -0.0898, -0.3074,  ...,  0.6377,  2.4844,  0.6318],\n",
       "         ...,\n",
       "         [-0.3452,  0.3005, -0.4980,  ..., -0.5151,  2.1289, -0.4602],\n",
       "         [-0.3501,  0.2489, -0.5015,  ..., -0.5156,  2.1309, -0.4607],\n",
       "         [-0.4478,  0.2463, -0.4578,  ..., -0.4998,  2.1309, -0.4451]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.9912, -1.6582, -0.2827,  ..., -0.1584,  2.8027, -0.2000],\n",
       "         [ 1.9883, -1.6387, -0.2859,  ..., -0.1586,  2.8027, -0.1847],\n",
       "         [ 1.9766, -1.6514, -0.3064,  ..., -0.1584,  2.7871, -0.2000],\n",
       "         ...,\n",
       "         [ 2.5020, -1.0732,  0.4209,  ..., -0.3132,  2.7539, -0.2915],\n",
       "         [ 2.4492, -1.1172,  0.2401,  ..., -0.3132,  2.6855, -0.2917],\n",
       "         [ 2.2910, -1.1494,  0.0892,  ..., -0.3135,  2.6973, -0.2917]],\n",
       "\n",
       "        [[ 0.7124, -1.7207,  0.2247,  ..., -0.5601,  2.5000, -0.4053],\n",
       "         [ 0.6162, -1.7148, -0.0414,  ..., -0.6094,  2.2227, -0.4705],\n",
       "         [ 0.5244, -1.7188, -0.1897,  ..., -0.5107,  2.1738, -0.4709],\n",
       "         ...,\n",
       "         [-0.4998, -3.1719, -1.6338,  ..., -0.0798,  2.8398, -0.0620],\n",
       "         [-0.5859, -3.0762, -1.5557,  ..., -0.0798,  2.7363, -0.0457],\n",
       "         [-0.6196, -3.0898, -1.3926,  ..., -0.1625,  2.8047, -0.1437]],\n",
       "\n",
       "        [[ 0.5698,  1.4111,  2.4512,  ..., -1.2549,  2.4004, -1.1592],\n",
       "         [ 0.4688,  1.4229,  2.4902,  ..., -1.2070,  2.3809, -1.1113],\n",
       "         [ 0.4458,  1.4512,  2.5410,  ..., -1.1406,  2.3438, -1.0312],\n",
       "         ...,\n",
       "         [ 1.3174,  0.7134,  0.4521,  ...,  0.3450,  2.8887,  0.4526],\n",
       "         [ 1.2041,  0.6074,  0.3760,  ...,  0.4817,  2.6934,  0.6025],\n",
       "         [ 1.2080,  0.4651,  0.3838,  ...,  0.6167,  2.5664,  0.7349]]],\n",
       "       device='mps:0')"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame  0\n",
      "frame  0\n",
      "frame  0\n",
      "frame  0\n",
      "frame  1\n",
      "frame  2\n",
      "frame  3\n",
      "frame  4\n",
      "frame  5\n",
      "frame  6\n",
      "frame  7\n",
      "frame  8\n",
      "frame  9\n",
      "frame  10\n",
      "frame  11\n",
      "frame  12\n",
      "frame  13\n",
      "frame  14\n",
      "frame  15\n",
      "frame  16\n",
      "frame  17\n",
      "frame  18\n",
      "frame  19\n",
      "frame  20\n",
      "frame  21\n",
      "frame  22\n",
      "frame  23\n",
      "frame  24\n",
      "frame  25\n",
      "frame  26\n",
      "frame  27\n",
      "frame  28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video width=\"640\" height=\"480\" controls autoplay loop>\n",
       "  <source type=\"video/mp4\" src=\"data:video/mp4;base64,AAAAIGZ0eXBNNFYgAAACAE00ViBpc29taXNvMmF2YzEAAAAIZnJlZQAAcAxtZGF0AAACrwYF//+r\n",
       "3EXpvebZSLeWLNgg2SPu73gyNjQgLSBjb3JlIDE2NCByMzEwOCAzMWUxOWY5IC0gSC4yNjQvTVBF\n",
       "Ry00IEFWQyBjb2RlYyAtIENvcHlsZWZ0IDIwMDMtMjAyMyAtIGh0dHA6Ly93d3cudmlkZW9sYW4u\n",
       "b3JnL3gyNjQuaHRtbCAtIG9wdGlvbnM6IGNhYmFjPTEgcmVmPTMgZGVibG9jaz0xOjA6MCBhbmFs\n",
       "eXNlPTB4MzoweDExMyBtZT1oZXggc3VibWU9NyBwc3k9MSBwc3lfcmQ9MS4wMDowLjAwIG1peGVk\n",
       "X3JlZj0xIG1lX3JhbmdlPTE2IGNocm9tYV9tZT0xIHRyZWxsaXM9MSA4eDhkY3Q9MSBjcW09MCBk\n",
       "ZWFkem9uZT0yMSwxMSBmYXN0X3Bza2lwPTEgY2hyb21hX3FwX29mZnNldD0tMiB0aHJlYWRzPTE1\n",
       "IGxvb2thaGVhZF90aHJlYWRzPTIgc2xpY2VkX3RocmVhZHM9MCBucj0wIGRlY2ltYXRlPTEgaW50\n",
       "ZXJsYWNlZD0wIGJsdXJheV9jb21wYXQ9MCBjb25zdHJhaW5lZF9pbnRyYT0wIGJmcmFtZXM9MyBi\n",
       "X3B5cmFtaWQ9MiBiX2FkYXB0PTEgYl9iaWFzPTAgZGlyZWN0PTEgd2VpZ2h0Yj0xIG9wZW5fZ29w\n",
       "PTAgd2VpZ2h0cD0yIGtleWludD0yNTAga2V5aW50X21pbj0yNSBzY2VuZWN1dD00MCBpbnRyYV9y\n",
       "ZWZyZXNoPTAgcmNfbG9va2FoZWFkPTQwIHJjPWNyZiBtYnRyZWU9MSBjcmY9MjMuMCBxY29tcD0w\n",
       "LjYwIHFwbWluPTAgcXBtYXg9NjkgcXBzdGVwPTQgaXBfcmF0aW89MS40MCBhcT0xOjEuMDAAgAAA\n",
       "Jj5liIQAN//+9vD+BTY7mNCXEc3onTMfvxW4ujQ3vc4AAAMAAAMAAAMBuaWF/3Z384ggAAADAHbA\n",
       "FQCABGw5uU+EqjA4gDhugaVlyLTCylTTSpHTtWt7gxCSFJU1r3ygwaWsAeavew3/b55IiGQ1bmvm\n",
       "c/j0INXzHzxuhVMo/5EsJeY7U+bV/fzA2wF6JCJOZFWz9Y5EJ4ni2tcrhG5roldnEjKcxhUd6lFD\n",
       "fPxGEA2Yx7JjK2+kApFhOlMen7JRuVdLpmFULD278c/iMwiIz7Aksh2syMF3FOPuFE3Py3kDHooa\n",
       "3eaVJ12IJzww9uaa5Lmo3yoopwZFTxLhxDE4T0rdQgRUlpoQiZssqMF6Qe8zDzztdbsa9HvzBHfh\n",
       "+J4RPfdMDGZsvtpoGvszZHfFf7EyCLyf7g2GUOujcwBRw5gJGtcK/ggvmKyT1MBt1XslEXPhexFc\n",
       "DdrFr1c/baO/WThq9L6fKH9Xo+YmC43SfhcIoDdArMx80C6txD9d6Ij6Fffw9EP+20tGflrTXYVg\n",
       "dywA7QxWgRQcVMoxO0/dhPxIebQLp7yB+7/OwX9wcjSAeNrlTXOXSMGYxnBjxwrghNV8q8pr3OwZ\n",
       "e39wxn9rlLbjoaJuboUPm/3Vkz4YfedH7G20KMHM0nPrKNbYpIJKk5Nb9vFDFZDCKQjsyeYYA80S\n",
       "WVeBlJC1YvRHdy/b+TjgDUe/YKeDZo8VslWGQx3zKkm5EL6ZTfyQSCMH1xOH60FnJ3jEWvq+jcpZ\n",
       "BapXBlR2GEAw1By4oaTTbIkuyepE7+u2i7E+9ZfK4etE0J+JgvH3JcjsRPoAQrtutqZFRC+1C6M7\n",
       "YmCbpFl4An8jeR1bJncCbncu9gqHycokj2uIZfNHiwlCJEnsglPFf3V0ZxLosJCLtrnIW1upg1WJ\n",
       "Xj0xbZmHWF1QSqbyz2DV+RmW/LxMobS5iIX1L3n9tkMYe/4FjQg2qEcovmuuEwk35ef3q56WFCPF\n",
       "1Qacq2B6QNqNkR1B6v78EGp/ik8sUERMGLoqE+VU7IWqUinpCcvjbZDJWO0RRbLqhKSkmd+Cfu7C\n",
       "x8zK2Yq4xA4QAqo4Yosbqhh0LHHmqKPlPFh77QKciCKsrtgVX3ToF7q6xVfvAUyF5tFbbapIRa1w\n",
       "WVO1aDC1uzcsYE+Suzc+d5sfZuGeRVj+6MAFLpQKkdYpgQL7yAy+7O9z1TOwO+NEd80Dzyw7AgvC\n",
       "YbTZDox6AH57ksUrnMimSmMgPCODuv7dMKTxWoBPd2DYLJsr6gHg0zfaqGN8YEA8/NXAUtFUE+qy\n",
       "0t/aJ3LOgMskDFzd5qX4rJJwXaVjsH7lguvhxmhv42tUbcjxFZj7ZN5/KXC8fzl4I2055llLU59h\n",
       "0X7jqg3CBLcNvwf9HNmQgbCxTZuocpA+Afm6LMMQlwqrnEgF32+OhlpQVcz8hJIijOxLrd09Qrei\n",
       "8FkJSlDI9BiOxnPYbhz4g5Vv0WLjuqqO2t76gGvD8XNZrHkQD9nFjNu0qwtz0J6AvbsKpSvJ9Bxd\n",
       "Ua6liDuFv5QDLuuk5ffL8oG3SQ2nvLGs+8bYDY9Apc1W3UvrVUXNOSGbzzpR5dTjaiIqUCAtDtJX\n",
       "ah15EmbaAbly1ZKChUa9UPZb7zhyAOCinPigSUXwguCwJu+8fCwBCK1pZvqGEAu476vmED7TD6GK\n",
       "wTjc8OdDLbYyxTa9FD46Ct/B3/C6bQ6eAspvKjVhZoFq+gpgQlpUqIt1jasZz034hbeN9abyDtQl\n",
       "IGZrEHz3SfycdePAypnQjbkPnxBfciyuVss0tMZ/zkMDEkvzZoyW/ph1EOjuluRdEok8xo5ZvPFn\n",
       "rsNF6Nu1k1Mn8jklZ2IVVtK96aAXtgTmoXsayaLGSbCzHIGAOrv3xVKdCatux7E1VCr8jnWNA0r6\n",
       "kgDQCp0Tsf/5aVMaJS+q8SB0uBmJ0sgtrvrfO2KZJE1RqH50+G0430PXplr9lMUme6oB9hwBq6EZ\n",
       "5CTxZYDvKrrXuJX2efXXQvfQhjc58PMjLQ02Yj+XTPH6CiDRzvroWPlbyNKRMDc1kzp1LoOJ7qq0\n",
       "wRe69N9/uIu5RQrXxnNP6aGOGFkfpyEtkWtaCMDTVn5ZJY5zRBcAlq61kSnm1MsvZcrodqkJqWyJ\n",
       "Y5XCPFF136IlNx2PcC1fPntFf1lkAbTGRln+Nj6K1dpdb6PZ39hvLXLB6nh+ZoWeUNzEDW6EnXBB\n",
       "t6mhMP0QOJx3/5Ix5tTIQLkQK8NPV97gW9qid5DXLoVcOmRoqJTiy4n5DeojsWLlrJYJAH9u5xZg\n",
       "3kvJ71lOeRp0FIvCONJgk/L8yTJlmuFFj+v7YfG7cnvJspwWeRn6Y4OM96LWUUr1+ck34P/h7i1d\n",
       "igYMnxsKJ7pB1DVshUqFUqGTMzWLZ9sOucNfVBiE07vxLrE2NC86tQsFKe8oXkQ3sjSbOJdfOQ/x\n",
       "jMdCZu+0Jr9+Pdtc6aH5ejbFn/YnqKx6P48Ihb6ZhnA682AjgzlE48X+qj8QTgn16jGnfjxWGNGo\n",
       "RzTs5NmX+0vEhb7KfVTN7uIwRwQMR6Um6+ElK08+jGWe4nLNIO+2WkvtwzDLrIAyireGrmv6JMCx\n",
       "mHKpZmFDXR27lWn2e0I6GfCtdGX9+GetVCBCko89KdjY4MMYK87pSsLux/kNP4ynfPOsAsxHLIQY\n",
       "k5Dlb/5MBEMTmSvUH/K7GLFDh7aZj1Qh/miourb22uzRj5yJfTkYCywDIf7MY/0J0fWqppjuidZv\n",
       "KnyAO5n70I/IP8TenoqljccTfhVDNI2PT4RYhwAibGBkbUFM8sgClnHFK0odWDV4reQ+xQOql8+v\n",
       "e6BosvAJnmM/Kt+aSLT+hLY6OSaPlcILkvG57Ofd17ND4NdQ5Qn3G/Xyi0yRyzmhsEDGyHAeXCxG\n",
       "yyAy5aiN+FVDsiGclXMimJvlNFPkRqd2qUI54pJRTyTRL6wsPbM8MfcynY5Hi36S3rMBh2/C87Nk\n",
       "eeWR3dPa21WuF2vEzR9NCIsTGVLbOc7SWC+ITOB7bWi7VOS10ij6Cnkrn4c/vKvYuGSZGK7YADx/\n",
       "Yxv17sqs0uR4uuL2dsKrVCfZWlF0vKBqwNJrm0CNNBGQPLuyrtlfy+BHgmCcyKa7mJeAwFkVdxx2\n",
       "1e26BEXX4TXlVgu+qSriAHFDKNWb5QlrxUs9mAb+Fxt28Y9owZNfft76T1vEWpXRJI3pdtQPtWPY\n",
       "fKSI/Pkj0pfBBfgUlJl3LNPmgRgbqlTJugSHKRIqZJUu2H4yOnCwXb483ZGh6OE9Q4En4NnWJMXW\n",
       "KP5/tlu17TxnNBuEk4K861un0hxg8Gmiu+hVTYYyTCSZTUL7z0W0pAW3pDozecY71KLrSq2T1vTo\n",
       "WB9/fJ0BVPsvydcCh15X0sGrNCqsxd4Cj47H+M3bwF6Snw85nUvK6kHCwefUs0Y9niCtVny7CXmv\n",
       "aWDxSovlXfMc9mFsS9AdLOTzlNkEI6fyvnvzHz1pbd1fAAHCElnbmqUEmhVlENz7jrGTtXZP6S+1\n",
       "wkYUCiIO42MXo3a2ND7AiuvqjPJTi7Ch/usulSszv/5/R189EqSkoGQav90bx030PDNYF7lMyfey\n",
       "pquFTGvLxmREonmRBDUm9bHK0crIA3wt7DoK7olPtPkx/+58UDGOKre+4lWrEcO7lUUXHJ8Gmmd/\n",
       "dN82nv+dUtADT0/UFJHOQxhJYZpOJKYtqp8ACBM7hlJGUQMBoNplVAHq0GtQWei4Hk22J5sVva7/\n",
       "PlHXrtSfBVK3VTjSFXyotvO5qMnTNuLihF+XimuAlZ5nHjaX4PzNjEtnndYMnGdPPTKXVVyg9bN0\n",
       "Ktv43agZBYciXDQAouGnfeTIzZmXL3hPGve0LKsO1r2paheFDvKNiR2rPRpEzNKvlMxaH6GUL+Z7\n",
       "HrzcFN6SFmp1T8IGfJSoB24eN8VrL3lWG7Va0W4VOauRXx79XDB7geot0OcYFJKETF6BH4gqdNjR\n",
       "Q4y13ZfAP1MHnSbfS9WzXYgTr021pw940p40yp8WwnP5bWaazwAfWV7vH7dzBibbkGrU2X5irPa6\n",
       "W0CutY7lDh6/I3Fg+bVPUIxv4A8cbHrUtff0aLXxTp9A9cJxb5+qKxUJWFFpfv70D/l+5e+BGbb9\n",
       "PbKTsda15Q20T2iVOKC+0dm8u3NWCO/2dBdwlqMg5cuUKYBzXZVhYQJpooudwEH+fr6L1MtQ3o0j\n",
       "Db6+Uw82jBC81Vd1iJPK5qe/OIwKHsCMZflIbWrMRB17YxEdPrENsYl1ES5z0jzINU31VzPyFy9E\n",
       "TrsaUwWgGz2P87MUMxvuG9Mmb8vRuyALHTU69xslbZEUiYjoReZzhqH89r7o8EhF5Tz2wee2JuN1\n",
       "EzkiCt+hTKtnDzv+xYODcE+fQsNexdiDb5u0Pq1koUgosQ0Zg+sYdY7mzxPY85k/mJTpkTnOTgdc\n",
       "WMR7Ha6XrrTsY1+aTMMsbxvFNSgfnnG54RX+Do8IZBNdyjGur9cMAq5ORScCKISYyXHV5uWR1uv4\n",
       "YfzPE6VKGffMfOkVl6LFqNjkBiTK8M/mikdTR+9nY7vBjjSM1pB3lK8JKPj9wIvIDJxCc3q5q51S\n",
       "Dpa2wEZTS4zpJ5WJpBbQMNYjITYECnsJukhqnL699fRsYBIErfS+wzp6bQ/HP9mV288YeOvk3aBE\n",
       "FefDPGqaKgkN60xOKxECObf/A+xRrSZ1vPF/VFt4mf9TWSw9AZzfSJmnHFA7PJyzCZVXaIZWjbRa\n",
       "EC94oz4U+I4bjX9vbivf1uPKlOw04QFCOY8IelpNAYM/hhWCHf4XDS42RpGRYuo92UU6AbX9TtjA\n",
       "147sFWSKn7mf67LLdSCjrhAMn5QGGQRQpq44HUDPy/OHBpQNV9snbgDSvO5CqrFyyf1T43rcqQ0y\n",
       "6pszH91uURLjpDcxKUzMdVnGOSRjIHEpVt9l8v+Xdh41AHy279N0ixQ95d/CrniYn28lhNy9fhsm\n",
       "RGcWHzf2MhM+Y6Rvgv+7ZYU3wK4XXDUqYPqjzQY0cI6QNZndQBLULm1Misyp7ww1iShC81hmLboK\n",
       "8fiJYvmBN9NiU9oYRSFIwuB2Ch6ZoY7F3Hs0in57yHpcP6Mg6EN0OgOUvURpjcGjHAAdTnRd0HIp\n",
       "WAAYgFB5afQHDW3GmIei5n7/wApTD+1Lt1VAFmqovc/71xf1o9h8bp3AK4uio7UVH3/wFmtMRsw/\n",
       "Py5dDlQWIdvdkDdJTXB9LCG+Vrlr53Gsj20rlw3j/7uac46Diy7QCpMMKBPAjlZoHiMQ22utT9wG\n",
       "a/fEgAh56DrTzRHG410hwScpeiwh12/SY1rAJAnB+LKBiDn7WeGmOPVKiQ5Vk1YuqrhhXQfau/1r\n",
       "/E4v2AF6VK8GzRAyVKPNXwjskSZHXqWfXhteXbZPsiLLYhbvqkM9yuy1aLZeEPRMV0Z34u0UxQt4\n",
       "o0RvGGdmraKthlPWL9NKXIOAR2+KBictqSOeg1JUDa7gsFYd4M6A+kL2ob5pFeNSlDHdRdq2VdIU\n",
       "baUL03ZN//Al0Qq1zCtJPv5vnl2F57LgPM4ufEtlvCcu1+5h/c3TB1nBcSBnc1NpYSSDyyQb7Z5u\n",
       "MpQ6ifLN44roxUp9djaR/P47h8fnnje7wwOtl8Gzryjxi1suO3MKgxCzor9/NKnSgCy/oiHns8WY\n",
       "G6jdwMg4S3MKNCNdrZvePdb5MTSAIUknzijI8gceTTgBFNmKeo2luMsSyl8IXunDb06BwngzLING\n",
       "vqHb1FXV1+sHUtRMfNT+a9uHqah5e3AuXZD6GjlUqU+MgMa3sLN1qpXqmEoDLqcKQxNmmpUgTMBG\n",
       "ce6L+0zV9Jxar4f6gDguLIROsOUSTH2QvECnJm1csSRtxvOKcSNs/pzidtLjiNbn/r3YV0kaFR6W\n",
       "v6yCpRRT28e2wKXQEEHZha06lSJclv0KZR+GAj4sZUR3LbE0xfrxGNw6QezWy9ac8Hw7er1Dh2ni\n",
       "xJ6z0Q6PM0vrew2DYAQpWeFLwvtfAyKWEpEAmqbS+wzwiQFTdxkHeQgsVJFQqlw5/2TAWfkx7Q2I\n",
       "t1nlFhXpD7VIAi5OtM+3YWORY8rxuwd4GPQZOFIdbsy6yfb6H5Ovty0sR9P0HqIHiFCqgvGdCJdn\n",
       "/WumQgR7zcIZ4iWphHs5tTWzd2sNYo6pBnRDmVabGOekibN6JgwAnwQDYM4kK1uHhXLUajvNTBxr\n",
       "/9bFDfDOqHYx8bLvnqGY59FptJWMh9g2KW3//4zdf42GPpf52ku62akhUg2AYzT26XZD+myCP0ky\n",
       "xKPI+dKcZjvrgRDYlLzKks7ihvoxFabOam7q6/6LXClmafCq8wAO8rARnTUFED7X0RGpAZk9sm/x\n",
       "XrMiM+SKSfz4tD09Z6UNhf2v2D0EiU6ppJCdXFdsIDjxJE+gnFhIqLqOdofaZjCtdNXXS10Pg+gQ\n",
       "WsaPiG9R5tvc5zGgzM/C761exY5tJLaZlTLRRAP2vwoWJ3F3VPsPfTNjZbTb9zw551W8i8K+r1WW\n",
       "i/7gdeH93pNFDK6xpgFrSDGfpaLws0O7xJesrQjin3PnlNtiyo6MEWJbb+xbwRiRdR95rAM9Z8oC\n",
       "OevTrf/bbL/8Zpq/jl+UFWZpvF8dA+13m9WHtErdUSFxoWwLR3R34/7cIju98tuYzFZ4t4zc1Pyx\n",
       "YwYiMlWH4Qw1hglPD4V2lTKNgwRAN9Ol+EGXed4/eiL2+B9Uku2noSORDg54r/WLzARC67lJlpEg\n",
       "i4a9ZbmUcYOo8HicW8IHYkyMUtnYKRNekpwuF3r9+CKI2sUHiQ73JkiIQD++D6gO9TauW9XImPtS\n",
       "3DgVXuhmwUJA/AR/L18lN0Syt/6IPbtKwCmLX+tHjBHyNAb4qgp4DUwx1DNNCp3iqcHF9f7ta0O1\n",
       "MucVZm25Aw6pC5flx1IHzQuxDNpgKDsflqkgjmbJv4J4qwnYePdxxIOcGQLh6bMaagp1qj6IC5TQ\n",
       "lYCHj45BJDDE0OoUOIj5mCH/ZNMyDe31d3pXqCdnSlDwqxJ6jHBr5j2qRqlw/YP2Egtq1/8PBOP8\n",
       "wVPe9In0wL0e5dDp1l87H1C9RZguoH6/Lk25ob+O8EPhGINUk8UV//U4KHcvuAHQxo8GEZvZ2MGA\n",
       "YCoj4wGi4CLw+EEG50wohsK8wHlQJA+kZLU4MHuzBatt57w2nil6t4Nw1vx7YbzgGPKGvVngGbKa\n",
       "XtgSeD3RPqGQE2n9l6N4piwki0v70NVTOsm9IjcSvkPlmqP61a+vlPhiGLIFgdRACAV/P/DYgBfO\n",
       "09WLU8P2m1MDiWLOyElzmkXENBPLRPmFBrquVh/bObmT+WVBalDfspDY8d/AruplCc3egFlDvi9G\n",
       "bM0JvDRWxQ+dFWrmNctqE+qKw/ARaMoxpVlwqc7MbmZrLNv3e1CtRQCZ7MlVBEUJgvKxzhmRXsyx\n",
       "pipbqPZes80cqSyuWbp+2VqtE79HLF4PcRMzdrgkeTi/WrBXR1Yy9d0qJSGunqOHZfPMjHbNbzKC\n",
       "MFLhkZf8bSYU0Odn8GOH3O6bGqbhYewVkIpCx8bFFVL9lrc4hhGtjzxEozm/GH5aVoSwFdOqwf2t\n",
       "MnrMMAy9dWhNCR7iJ6F4ij3IunJ39pU9URJaEs2DGA54i48g/P0gFckrWblcPVngv6UA8drz9LYN\n",
       "oMCuHEOXPN2PMJc5kMCvSRhej9GudlRcbGwqt2wnuVKY4lfwhYiZDWvdHlRiYFmfyz1Ik/buyH+o\n",
       "nMRu4mt3MaSJBXNbjLPimm+ljcmrRMV9NtTkSuCG6Kf1T/TfKl17IVOXxjG93TqmSiwFtETt4LEc\n",
       "dhq7GTWS35K3br/4Ixi4rm4O48t8izD1XtbnMcumQ1wzj/MZ+dLd1j5hcf4nz3+O9cfqeqngfjVF\n",
       "UGoqRTcAOjFM+wcLzvXWejtG3ilz6RpNkqJqUCeqaC+ZsexsZ3y73Pd0vxGn5ZfFz/Jr19eq2LXn\n",
       "JweMLk4/T9hr9fjmtDtf2fSII9XQ0ppUr+uAXkwWr37g4+fPEyKuwPFfHZLmOTTloMRxvkAt8EgS\n",
       "vKTwANaRGZk7PM8mwRbekdTB8sADe8+b/HJXEMeSPMnAaK3X0lRAd98gFZjzUO/3mYRLuNfcnztm\n",
       "z4dIIGXvCDq1jvWLaJ+Paly1sht/R3eX572k6Y2a2a9rMucCl0XLuq5zNxlghNhn3c2Jo9OMC2GE\n",
       "vijqOct6QZD8IEgEthDZals2rrfjvDqNCkbKP2K8RRGpygadZF0yrptOKWyhxib6Sm+cx6WujQNe\n",
       "KprHRoQLM30twlMfKp4IT1FaDN6GcOBj+XHJ9GPte6i1kWuJiyEfAgu9OGG9aNdJVwrE3I71S1nh\n",
       "kkhrVMEXYlq+DLD5b0G9XdroLXAebKmATZOGksi5SHBLPis4f3SmpRq/yD5opXJyhNhbBG4CSW+/\n",
       "no9oPbtvACZT0+Hd6fWI2WcCog1L5qRCejS0KFpRSiLA+zxPK0aFI7x4OQ0hPc2SWSW68TEH8rMv\n",
       "XlTL1Ge6wH/ylDZ/0RO8EYKs5JGkvJDgeZFwC9lC/ft1UmYp6QrbwNsGZ0kQhyEG1IQ84UqycQTY\n",
       "rVmXt0OKKdw5dUfQORPxJQZyUGRHRPSBi/bYlp5C/d5sevyus7v37gNwwq+AmFnrLzPCCl7WKS1C\n",
       "eDbLKUclhM9KG7Y8LUwz/ugpOznIi7A//8CSyGCwek+z/qvxyNzYmz+AtoQYvV3Y+Joc2GWLlRk+\n",
       "F/MBM8kLT8dEM6811hEXPTlZE2l3378rEgzgZZTQHkhrW5RBzgTH9I+C6Tgzu/ukN+ZAsO5xBtLb\n",
       "mF8scjkU0d4XZxGm932deGIBKANtSL1ld4CFZ+NgHrnkxUOvUCpVP9UOFHnNmbIHUq/peJKJmzoA\n",
       "OryDFI2PC/ae3bwW9IREDEyU/WLiyw0Uvbjai+8q3d/H5wQiikvq6liPWUo99NZdO3S/lcPa+tl9\n",
       "An5c89K5NTCWZGr0/FPzzAu672h+q9wjWn4TJ152RxXRlc3HpA23ON7bxvw9oCll84ASoZ7ZTaYy\n",
       "qIjHz63SLsFCsRhCF9fJWZb1QpTt+LPcAulBEY0wmjitQrHi6BXJCt/wj8w3UDA0pFbbKuSPsm6M\n",
       "oaQ9FhBiGwOVb8hvN/so19GA18cPgMBEqQmlsD2Fan5edW8reiymikHv7p1W03fE2mmX9YlG97QU\n",
       "N1JeCyvKL2OxoGZP0nUXZps9iMcBrECEi+Xlc3bTgMtJt0fXHnwancCoUowu72su5fwjZR6fzuXI\n",
       "hlB65TQgJC86Ew3QJaGL10ENPfNxrRZBhDBxU0NsLLeUFm51BB/AfSQGDTL4UCDG85rldEl842PN\n",
       "05PSV6s9aBaYcUihhAN2vmHyfVxKW6HtiMM2uf8f3ik8l2MaD3UwAow4G5UalpminBQpAeIENmX6\n",
       "gZIyWgUi1csc0Mq5GlKtLU/VdnttGG8TR0KVMtVKpqTfzGreQcmmNNp8OmJJlv4AR/xfEZ6JecDM\n",
       "Alr1n7HbxiE/lFoLmGo1Lda39COyfghv1rWX7W+p98Og93rfZmI+95OooEdzA7kAEk/ImOJGqD5t\n",
       "p23vYwNMgG92n1wdmtQizoEjqnFf20ejxXGbCjXAD1yz4Kj1RlWcVsFHtIujiiXz1cQbNylUoCHL\n",
       "OvuV4retwiKVeAunzUalVQ9kf+1HiyC0H4I1Tie6i+gb57FAeszubYGzuf26mGaImjOazLDannc4\n",
       "fEiyhr4S3BY9jxPwzAvt4d8GA+yEIyx+py0iRJVbbnrV0m9JX3zTCZU3uylQ5C+BYVVjopk/GP70\n",
       "XzdbKvl8ejCvbGad+MYSDZ2nhQvS5zC/jA3JcuFF4P7JusAS9wY2UQWnAuVxmQ8NHTrk6PDzAXGV\n",
       "y5RpvUMX32Llm6GKNZeRNjaQpnz7CSgkbmW0inhNrqCV7eQOjcTPzH+4yhqzi/zZ6rdrEuNifZ0j\n",
       "LBQRJph97RElDPFw0QSOpWVAgUnhL5qXzoF31h72K5fX2NkUN3Fn3VlM85jK1auwsaWsCvi/J9NI\n",
       "vlXQA78wgiMYmXyTNrJMA3c0zw65Od6M+jtWeouilcXCXwgzRZ8Ts/FuQjcYYcMqP8Kjg9znIj1c\n",
       "V5IAfF7jKgsZTcBcB2rho+gd7EHepXGu4dQHbLSeEbwkT6AeRvuyjnZZh/CPx/q3ERhovRoSySFS\n",
       "DzMheDGK+Ucyiiv51B4pxWm59NY/fHg9h3qK6NkMT0HkqCGgzXgFmF293l5u77nXsnscBwusIG/O\n",
       "J6S6XhKkvSwi73eOQQ6ReVcdda4oA995loQy5XCERr/diBakRzcXOZnZmFqww4qyOEgw0d34GhIx\n",
       "rRGL0F0eapIIF7hKwVpqrIvd6ZoMQAqejjFeZkufqfbaicONoVPf2/ECUuZUqYeUAi90pRGC6kG+\n",
       "6TapbUNjqMCD3kwfQZfBsgxoTAyZXKIjFDBVLnTNyKrrz61s47X/PEU24vE+iMSHIIss1POc2PqM\n",
       "F0Q5W1sL1G8twRVi3HW2WjJudbdBO0obAQJo7jLymOCTCGOaKj4jzqzgnbXDaXt3Lq3bAAg82r8Y\n",
       "LhrCWpNQXEItlK1QC5SadgABLGu2Csnx4a/nS0g7Gtw8FDZt+zXfszlNvZLe/2XA3N0zxEX826f3\n",
       "IocGtqDkgJq4PofNRR/anUP+LgWxqTqnOBS3Y1mDhNCdIthgmfAZ7C85VyA6v47Xwkwq7BCjpBBl\n",
       "kOC4mS5Riew9HUY6mrXZ/zoMFnsBc6B7KPmRYZMbus0HxxN+Rs/fkmSVfEV7PtBaCgeWB+YDbdUC\n",
       "dv0suw/KXK59NOJuX3H+TCmzGUsw1MpJ/QCX/WPmSqXNOYS3KdB25NCziYsR2RUIEGylksEQG9H4\n",
       "4jXaxFrJqxE6Y33dqaYfEoME8kDkh6bRdHhV6BP3lkyPfkrMKVJEbAKitj3gVMc6gpzhVP//4C93\n",
       "gAY70+TTAgbnpIRpGCyD77EPNA0HMqDAUchnw5tkZbkwa//bPIWKsaE77to5ImkqD6mVPK7PYIww\n",
       "7oUplGwMXDmkPu9mwYaX0PkBa3NFvMzByhI17qcgq190+tMtJKSQpfAGHq5e68uZuS/F7Xut/D9t\n",
       "HEnI3cCW0ZcPZ51JLYxFCiUaPTfUbyYmZlD9CM4WCPi50XHQLHD/Upg5+haoprtaAFUiZyp066RA\n",
       "bR2atl2m0/8Qj2+EQ1HByC70KQeWAJsezK8pOsNT6AbQ+dQ4oHQsBmGsLj7YNTH5bYHYmhIR7+D7\n",
       "Z1EuZP6y3wf2vT5HPynfkgESbEYY/JB/Z6qoI4MhqOtkr0vUTz+rjjNF/XvSEIq4fYKo146IWcmK\n",
       "m3FiOzoWyCjFlrLCaWE02CpLu845LmU2mGM9i8XIzB7yt88Rhx0YoXp70Fzsb9H5fQb0MfGVMOHY\n",
       "qIIqRvjyytSQMa5PlvVMj4rl1I3wRhxZGMkm7/KwGNhNBDan+R73VAbuvpw4NUl4v132wCJwhmir\n",
       "h1SV6/Yzi55iYswaHl0QU9vhoiziPjFd7/zFdFzol6Encn+rBxeEek56ciPm4iX1b0qJDfWWudfK\n",
       "VxsoHXNTWS+gnPNrHfe33OGSr8k2VWez1Jq949bWLU4JNsfhVeO85relpMsKT//wf8m+qSd8Ti+Y\n",
       "UH7qOijw7PBkGbiXzHi2+g37FVOMllz+Stp1xHJ9NcjM8rDeYf8DDVQ/sB6Jnj0tgMegqoUpWkXH\n",
       "wnjkErNwzG3DdP8GVLhP5jOk3a4nhxGXjdpKUnVpXXxa7b/XQ0CmHrdRPNRRWIA8jIzner9NW5l1\n",
       "rIzTtKO52uW1vutyrn1S0Jm6jYlesNoxF7AaFko4q/Be08/yI3CInHWTRyd9CIYIdQjBIScufx7h\n",
       "reTMaeKe0eTHwDS0JNAFtrPbuJyutX6oaear9Tg9GnnKKTxta3zyKEdyWaRV6rfqQSLkn4PstAQL\n",
       "sOAd3jV9uopId+FRlqHS2C047K5BJaQoqinoKHVlhJSWRAmuLHW/HR0Hf/er/2Z+Qz9LsCdndc8S\n",
       "UndtUjUV/QkeKy/ont0E8zhifiGO6EoGTIzvBKZ4bW5ZQmzoyUyq6utG4x8ehlbslUvj011XQdFg\n",
       "fLO83coJ6ItGboLUxrrfW8XXVnTVgyeDAWgDpS+7P2KOIsCQUsOxRpkJJ5OcprySDktFPT6oiBjP\n",
       "JAkoJS4ndDozo2Dh8fVOnrG5e1jN8uElyBNf+Di6gouwc7je39jRtXqhZJ1mEx1/NQ6/Ec7uQwm7\n",
       "1DXIRpT/MVOap638wLlqzVDWhN1yQvPXmk6JlZ0z7dRzj3lurPx4thp4fv3+gqrXvBIiHVrBvh//\n",
       "37JMtN/6KIDqmrh8UwmNLD+X3FqXsD2FPAa14sIONITm4TJjeFsnkGZlOpKx0BMTvhfoDwN85svv\n",
       "QEbq6ASMw2rJix0YKHuU69xj/+7Q+YqX4TOkLcN+b2t3dZhCMJtWPmhOIrVCJ1H33sFVhjXcWkV0\n",
       "RWPX+akz8KlVn9pOnQr3hzCFIev1wEywpbJ/oqvQY5YnECClr+m8BuRPNgcLqIdr6N93tm0IDEE6\n",
       "ROUmy43nWadaFiCuLixzgInfA/SFPbQ/eA/VHwx3pFUS1LcVTLWcayfjAIYsoRhlgE75ibOKHmfY\n",
       "3V5sxraVAKl0K5TIHe2v+tmlVEneVCwQEQj80k3LgoO0W+IIPXMMwI4duRFNWopb+sPF1MEW4ysC\n",
       "O2RPbseql3Z8036sIQ9MT1QNliRUXRAef9w4wkIpxeWk8+bmImFiift881eVO+jpb/8pUU2vxSuq\n",
       "YWDLrODtMV4ZTxI7j6pCcB80r4NVQb2tIwYUdQGK4DlIQew1ofgkyBIP05SpiqEAJBBfSrGjoA0Q\n",
       "AHPli8WeGpsATc0hEhiuwEb80c6ogCEVkZyCwQMKkCJyevZ7SvCXpk7ELYO/iNke6SnQRYwYXm4f\n",
       "EuladqWCT1ZNtd2dLxLT2K/EwTupF6uVX8tKI7cCCi30VmAnk4a5ptW32EFjBn+G4KWTdjp43h52\n",
       "WZhPzgGGAcwloFBnLWidIXBDewASDJ93/U3f1Fmj24BExv4TIgjGD1CgUEsVtMKEJ9LA0Go5zRSA\n",
       "niQ8SPwhA4uAii4DFF/kZ4p385dUA1ExMzViXUoAAAMAAAMAAAMAAAMAAAlZAAAIBUGaJGxDf/6n\n",
       "hAAALEHUSAORlvsGf5sX9y5gddRTJhjYE4YlPhRdUNvPByqZEWBXkSe2cuzK0IMHxdMxT/paBof7\n",
       "77eidSUpJsbJ+4RoqCNyyCl8AYBrI6v4Rer71UBydXJkgAFPto/NR6wj5/JPNl5qW1YYpNU7hTN2\n",
       "LsWBPi7p/XGG0F9qnBcSKsbtASmsJsl3rBBQMGa+itVuuS4wiuqLTCAh/2NX0WGDSBHyVI7vvk3Q\n",
       "wnOP42snd7nynvz0i58qP/k6XA7lEZFpK7UrQ8dxITj+/fB3KPA2HXBgu6wYV9/iMqsP4hMkfq62\n",
       "lL2lChZPQ3H9W05oy6ZKNjT/1AkS+sSAfS9aZhNjl9YA62wAyqaf+Mymcqg4BCCtgAXwWES7kl5B\n",
       "kzMlICpC/Ic2CSOYCIDvPEgwz79qJ/iWpp8ZfijFUeu7walVAWuHTpl0cwG7sYBDxFsJULMlbpAr\n",
       "QOEaHPHP/R5qw71KL3DclmsFsdhOhLathdMpm1rZACPP0f3fflr5hKTT2BICNWnPlpa5T2J3Ni6X\n",
       "RevfaYJZjADSrA1b34bZilgcIBQMyeh+SFSeXJmhInpfzaIIQcT3xzMmtI9zwOl8wS0I4nqEjNTA\n",
       "hmrUw4TygjaBakZ69P9u0lT691cevjo4n33XZYbe14Y0Xn2p/KhgFJhtwcEDdbBbG01l4auALjr6\n",
       "z2VYfaDoZlZwpi/bEHlUCQJ7GBW0X8LL7RcXEwVyg30MxqqyN5vn3iQYCsAiUdemEcCcssHCWOeT\n",
       "ovaCMoNc/dnk4t0a1kxF/cN1tf+ob9MR/e81lze2nrgZ1HXbMgm6qCDQnEa/p8tMmGtzsWPLd08y\n",
       "nQBOondhikqwFeIa071sfLYwnK2lw9ZrrVNh2oDiyPkdYlI+eOhh6C+Uxlk2QiEPSI365pL/JXvO\n",
       "AUa9u5Rna6BNKNm6zV+kHeJZpqzWIx8wpbcHCSVlyc4WCgklKrKHHUbOBsdbpznOxEXNnOIMz3Ga\n",
       "4stKBG0l9VAE9V1to2C/XsPLBkGCnevns60EZ1niriB4IGMtGVuYf58Pre1/HlfDSRuc9MNN6o4I\n",
       "WqONMikzfkk9NivmwdL/4l2u6/6UvZvODJeiiw0CG0lLjDx1fbb9z+ZLdFAmreK9fZmQnkj9aQkc\n",
       "5gb36bz7SmZkDgHHtID1ly0ShF+u1xKITvRlASP0yL2UXpdgEfX7ZCoJJCwRNErllfSPe+QBnbyM\n",
       "ZXYlbyvUdlZ13447xS7grcP2RdnKKmLObD9AHwCeY7T3eVCYvThkDHL3QhNqDQjKkufCv3tEQ4Gw\n",
       "90eF/dSevXz4Oco3mt9u/sTbLAYFAcUEYxIqbmsGW7zk2nuWf9ZZctlUKHb5zh0mz8hbStl3VjpE\n",
       "pPE5T+uIsd7cDjWhDdF4XMEtAfh43yZ+87zWt1bmtOdo2iBTEIxzBYEG2y4ITS8+cCx3UTn1fOYt\n",
       "hIyUkACdvkawgBeK85VhPynVBi3JO97UaeLLIChJkf2u+ui6Mf3vvmZGIslvsAe7Nj5EbIu8gxZr\n",
       "CWvDIhgd0OmqTf3+9x3Cw3+tUmRSEhCIi9NeMPqZ9xDDReu5xzJIYNwSOmoaGu929jf7h7i5XFOs\n",
       "Vn8YlVtM1FqTRLgn/luJPitG0L/tcgscG1US6aDx6jOtUDGZEB/tj/bEaWb8mMT/kcT4wjsU5mfe\n",
       "nxIMzz/N2n3pFmhvBhedLR1gs0P/lJe7eYjWPo0My1SEVuSj27co9e+jCc0R3bI1DsU3OiX9xiAk\n",
       "xWFj5kIbjmwdpZlt5txzsM8uq4YC24kvSInvsJqt/xmZY1Q/THuICyyAd/gznxc/+1pMfCrJIam0\n",
       "uxqXMRwKTzGXkeZSXsrljIIMsBJz8IVIJCVEaMS1UP4bSvx1j5O4cqoMP7wM0PfjTLrGs0S3HHao\n",
       "H5ledaUrYACtHFv06kVtobA74R1Ug2/spWykNMc0hN0cUET0wNOr7HPMd2TK4KfjOC4u2KmB20Cm\n",
       "+jAYEpraUxO7oRtKz530EbR/YAjPskvZ62Pq7r2yAhRxbOlfH1Y6SpaQPGiNqlmQqX/zxpwkyqEz\n",
       "Ei06QbOLhguN4ktJ4TutIzbN9efSVWUref/qRSD8lV2FY+l+TXqodHqgYgewm0s2r19mmWo/N8Fp\n",
       "X3PYuTVY/+QpQMv0dhvCfL/YG0AWc5ov8ZPGjtkBvP3mzuExq8VOdKG+WtpWeCPoHS1b81Qio+2s\n",
       "lnuPSpkL14nFycufefoP2BS96L3+XLL+XbJdiire1YgtW/x0wgjYSr5Jj0NMu+vWK219EpdgII5W\n",
       "q6lAroHOWGMs6oawUpOpj3m75UD1hufVz5kjgqbrF2v19oB/wlkaCbp0vIkbAAyk0XNIr0KjaNKm\n",
       "6jMMytrzyURW51+SmTl1iYvWqsAy8iZ9bB+9eg33+afCx+Uqerm9D0Nfb19TdBuMozlyZ2vZtoWi\n",
       "pwmmPtc6HjdtymCOFwxoi+zHV+uv5C2b4MM2RZFhWP+99ZiMw4waG962yvQvVyHG9c1TsXLXqMPS\n",
       "+H/wORqMVOIhTeIeRt2rPwWKXDZQVn2qh1weotgqQ5lC9Md5lWx3CzQz3pf5q2QQrGHFy++a5R/l\n",
       "UZy6MKG3eG9lD2TVjauq3eqCRYfsuV8e6OZLR69zOgsNJHTelflVHxnEu0GACg/tlM//7v3X1HpC\n",
       "uOmZFjRghjyLtaGN9m5bar8dQMJY8f1G9DIIcimWYxtSj4hlz8CGvVkFSuesWI7iASMAAAMdQZ5C\n",
       "eIV/AAADAOBg+0/v6BWzE6c5YQEQAcaZf8GolDXQlDQOaOGpumfXnKsHnaT3Xc5LzAivDg3OhRaL\n",
       "o5xuHft4rk3/hO2CYuNY/LVxhEH6Dks8IP6zhX98JeYAZ9kj3tlr87umxy5FXBxBQZBDmXUKNcCq\n",
       "gh4aXYXLtuGX4fV4Nf1BMzWF/WEwC4yqw176lDuAjv0NqK87NQy765BYaC1ijWDpkHFuzD5+o/4A\n",
       "nszBRCccASHNyFgLWSNGzvlWDjfOtdFpqfGa2rvt9pXYypT7ByQ5A50durCivt7Vmv8pJVQaQi7a\n",
       "D0bSd3tDIgB5Ffw0UssG6NErVqNEdc5r40wzpW2mc0LzfVE4GYKa14igunTNe6CJa6Enq4CQIwWG\n",
       "sR0tOxI/K0irK9UuTwJEmJQ0WUly7REZaWAXiZUx4O4sUtVTUek9ktf1zvf1xYXrZPfGlEb8bt98\n",
       "AnFI8+AtOXP7uC6JZh7G8yFUzshsW5HtYGfd1t9pemG7d50UOVoR/8ugErhcwJnhSn8SU0ht/SQu\n",
       "ytAB/CzXsrSTiFMLWuc/r/PeAk0V1O71KtZKvuME4z3A20b8T0WIViEjY2Yc2TjYWj+MSFLZ+SK1\n",
       "dODiEgKEChJaHLoVwxZTpvSh8UXs5ELr7mqTOaTotjFLhlraQ7nKtK09mPfXAFRes2cvsfe3O/al\n",
       "V9JmFDqhAjisAuZ6dy8Ee9tQ+q2/7RROgdZWzaewgZYs/JchNmI9HyQkP1OhiSIz8vrXUsMESKzA\n",
       "SDE2on6qmlrNSiH/6aoibH8O5S/yjx7sIXkFoUIuaclSixYNW2JgVGe55r/qYgdP1gCP09t3klQj\n",
       "g6syuCCChgpgHn7rdAMvCvLtNhLDpuj5/Cw7A7chSAXYFhqNO6SfH5Dic/RRxVLBdeUiNuEZRcV7\n",
       "xaRPeG6tCU35Z7pUjImMxYclqPZ/uafhio7VZpetHU9pWMVXYyzFGBlo9eg/0SrSs31ChhDWdOrL\n",
       "ad5SbOeZxtsNebs+OgKaMilf6yh4E+D8t/JYCkyIf2AAeBTaWZ0Nv2VUDXHtcJ05krK2gYEAAAHj\n",
       "AZ5hdEJ/AAADASV9/aIIEEiukk6YE8gBa0mT8BtWn9XXAU/BRruNenMJS8SzOs1FyFFXM62HyzP3\n",
       "Vgak2GCVWuFIYJLR5IdY9Wtjvlp5QX4n5YpYm8r5YVAJnLoU/hvso1rIQDvP0rGyzRkz37ohV5e2\n",
       "Hd6UEP5a1BTfV38MAnt5tSznj8lsh+/aop0FBSqqNJFOT0bLnG4t5vHqNZarPDDyscS6vJweNm8Z\n",
       "P2e0Xbk/t6+kFVzgFxCA4ErM0E/rpFfIiGO5zt7745NdG5kHjj4VIdGFG5RbtAnbZuVG59FwkWiw\n",
       "1Lg6xt1pzuZEC2oYE0XyLkK3ODUVSc9U95VcZjgD23bAjImyvjxgk5QWZnd+wb5KcY5Qavb/06VU\n",
       "WxUOzwcZIiWKesqv/GQbJ9dviXiLJHn+PWqpw5kbGlPWvTazuiBnm623ySwrtyjRMi/bT1kewWMV\n",
       "FxpoCuvSGNdfmvSx004uM+ifm1t5IteK29jsbLoM8GFRItrEqvdFLY09PrhGmCgRUvkXneVBLLZ6\n",
       "9f3OPIFIKud06zvyOIvkLl4VCX/UrtQ0MVxdqrZyLwsvYHSkVD8m+FUugxRVgXQM0WsDVY1BieOH\n",
       "9o6TrPAiwL+wPPVDvPcL4VgkitERTLxKmAFxAAACcAGeY2pCfwAAAwEd3Ca2a8v0LXNOpJ6AAdVL\n",
       "BsHtb6psXABadBZzlrvCHxu6v3/TR8lvHqihpSOs2IWuZTK7y7IxRKTj9r5BVrK2j0q/+MUKScIi\n",
       "zcn4vdQ0urzylZ29lrunxIYHJ8yKF43N34/IdTvwiJYFA41fhG1Hm6nlLrs/t74KRS4f62wtBH7i\n",
       "kHhsuJcMIKE+Oju26eKoQwoe4wi0N1aIuY1nK/k+awNZRn047VqtrOjWNkaLTOVP+bs8fsg5Eltx\n",
       "w/JVs96RWINfaCO7d5djg5Eyp+uNBVhMgwyek/M6AdaCty/ZMONad5Y3SkKBqwlfLppK0KlbsAz/\n",
       "96XUvNIeDLGS9skdb6Os+lnwxJPr6kC7LWT1EoSUnmTX0J6mKk94iKMKfaCZUUbta7OlhwFk//8J\n",
       "h7UQawpjT2x5ei4dVMYIiYO3DpTu2Dwp6apeIJJ/3HgyFcUBwecWkvLD4ncoG5qECic2+KqwTOnJ\n",
       "J7ieSSeh17GbgdkEkmOAsRIvmMxqS1lX9BSWViXVm4K7E6/OQ9Rh6cUfAy/YvO6/+HOKpO2osu31\n",
       "OFqseB+QaBYiELcaUfKQZbcEZL7KYagnzV76Yw+sWMkihyoeZtavywcZYEbZ8Xf/Soa7ynFUdvWA\n",
       "EmT4KhdD35G8ebCGCSIF2KOgp19Xxx4k5TjLEcP+e89Yx8tPidaEPfffRA1aMF9IYAJnIpYI10mX\n",
       "Zub6VNOpE46JsSsGlVP1tR+2K41Hpc4A1e/BQkXX//Pg4jaurCz3UITvVoSYdHWdcEjixNJcKKnu\n",
       "Kwt2sSGEE8zAGPZ2mQiYJmnZ3pBObJOCowBxwQAACn9BmmhJqEFomUwIb//+p4QAAAMBphdhU1wH\n",
       "825QnblttBI1QCTtGt0bkY7DL8oqOQwO3wc21Y4dk8toL8K8k976iYl4zAULqpkfMOQTyaRCr0yj\n",
       "ytAzLn6BC0naz8QruQ+9X1wn6Mejngf/I2aNWdEvRPnvC0kmaBfrP/3vtMWx7gfi6M806G52x68S\n",
       "mKmVn/iMy3lIdsNzpK9fHOmeB5nj431r9rjEY3JxM6DXynA0run5H0ZbXoq/PGj6NWzuDIKpFM5K\n",
       "x5Fb0i0TFgKpJqfz60ZwKHQ6suonLKvdjwtTSYBQkzOLIdafa5wU/bdsmxRy6ytOcw/PISBhjnnJ\n",
       "5SCVaGnX+9P1D3taOHSt/Lxwd/282sDQgUaER8o0FXcKehhMLDEt+TRxXWoH675KeOfQ6o9b1RvU\n",
       "Rfhm3d9LbitRneRMOPVUmc/gT2cUfed6XmV64dy+uRJ8HrHEU0hawryOP52ISG3gbYbkYv9k8NN/\n",
       "ByGUCly6dAE2IloYLBbBUmZQneX3H2HPYB8hJeFiGl2uZUYTsBxRtSH30LOkqH5y187XP9AlAmfS\n",
       "ulekZJR//UtWO+SK6adyk+ElT97KHKbaIaxqI8McYq03VerqsrmbShnao4QQh3sA03xLhzrMuZEm\n",
       "JUGgZ3r8G+tOQn266iHxBgHdOHGd/vQm7aXV1ORFeEn4b8qzFd/Of85ouWI8VJsvctq8HcDjHl4X\n",
       "h8O+BiQzO8lgrKmFwn3jTBaceCHezpxjO9M6IY3+qrr5gDhvu8zB4TSd45KhVJZcpwQrsHhjBoSU\n",
       "6Ucu2SeKlu8X+DTs6HyhuXjH9CNlBS97QKlJvgVduquOQctol9YdFBQl23FtJkrsGmLVzcfrdCVJ\n",
       "KM8/I89AmXwWrycOWRdCecRS4Nn7VUhLXdN3Q+SFuQmoZiAiwcFAj/PpDGYMhhKzQVL76obx9JsO\n",
       "bN1Okoi9pcJ1dKegbY5iZbgCQoZp3TxJfFL6WCcMTaRWBVMsCKb90xgxTXVkR6ehF4fWpD76q9LY\n",
       "h5sr+Xq/iNY6c5GzSctP0/bEowkPrQ6DA8el0cnEiPuoMJrczEeB+ttmqFkEETSYksMAQTWJ3GFA\n",
       "KFA3VRBKLlPkGoHlG7LXKa2qD3Cz74Lrji6FVMuufGtJwOkMt63ZIJK37lxaoY/sj17MAV3dIm+C\n",
       "C+6rYUHpnLDufvfGnQVUSD/GscXV87AOXSXwNQ1PCCXjBG7osqwdiOEiWtzlRCo07zwcgWFqmbAd\n",
       "8UI/1zed4Qaoifez/mQAo5lEUusz029W1GPGNSkCkvwTqsz0kSp8c+GQOsg6c8lxowExgGr7GyHd\n",
       "Ra2a7qmSUraopoY+3Tn7NRBsEbvwmR69pX6L5v1ny8mXL0Io+TqUauY3DqS/aYxMhWv1PrFzAys1\n",
       "BAdaOg5Eh3mSX6/GcRVn8mdIuIrgGOrnNxOykLpnARF/NF6GXR9LEH0ZOpRqnzEukqfR4AyqCONa\n",
       "GqvSFX0sfEskvBVROFD06kOwYK2ee2xaiynfAwP828Pcb1CJeRvgau8AMqmj4z9vByn1xHk6YYZm\n",
       "amHCmye9YtNWQl9mDM/JCWYdDFghljOcABU0en2JejqnLAI8DR4WB8FT5m7ATdMPdWKnCKxGbzFF\n",
       "/mLst9ZyyPUB4aBxjF1IKsuJaxwfHEgM4jvbXB1lRWq5RvzFpYcgfi2wfzjgMTb+w2WJRytC438t\n",
       "oO4fOUyh2R2XC8NmKYXdAeWBRTi826dCsNSX1zyaFq4dKxujDZsLRIVNVz67uiJP9HkqDpjXy9nh\n",
       "48isRUZyknKkyJGnpXeDCCUnLKIM0I2Z5hZqKrQZ7hS/PqKm35+QmSDoAU5AuRDWiGV/F8PXGCz8\n",
       "iA+5OBgu1AL6L5By/PdEuYlZshzQmZbYZw2qybopYJKNx2GA0LXlBXM3/2/hAwY/ikowqhAy6wQu\n",
       "vhHgjM1GzTwhzKD1RyeexeCW9TYorDLPN6bqRolr5sRTWZDuaVwhOb7oRDXwbj1yaey4/yo72FUl\n",
       "w4X3gWJshU0zyIgPFwpazkj8YHgOjYvu7I9lCtsnLLPQBPp87DyJUZ4XJtgyl6nGpEp+DXx1p+g6\n",
       "mwUH93Kuc9wSnpI5SwXa6jV3kDXhKpl1jKO3QIYXrsfcZvlMKl9iRKMDASLXXASYu85ZSR84C41Z\n",
       "X/pK2GXCt8wCDlDTC+cq6d8+qZHy1l6JK/6bxlO/xaDx18Ycj3bDPTaEPMnWHHpW5ER9uh4uRuC8\n",
       "8tZ8KHQLfbhFQhvvpAZ4BKbj5Tj1PocTQ28bEvBlAseHxiJtFtARo77tLntPivkqqOyljvOo0f8K\n",
       "LU99UeRW0ENA/WGzAStXNs/el8ljO0tejN+OOQgwVvsZA/KNIs+46RP6c8olDZQ4BPGk3b1gDCqn\n",
       "sx3HPCldP8hieNReJ3f6WbB4YM/rRcGClO9HUYkeCDo1q64VAJc6ofBz1/3R+Tj/Eqsmy6EH0uGR\n",
       "D9hOwyiZnrp6UjOi+KvzepqVtQGgyHQyxTIyMXySrthvntyQ8tVabTJoadnXq+U2b+NEUSD2wDzz\n",
       "USHeQyLdZ1pQ/GgiINkwvcn2f1KUQ+owrdV1IZ8gXgeo9+1mfjrPUgDHY++5Pqdm8BkdAPdwZdbY\n",
       "Eyqg/fd8Y2xWneozSbd1WTe51EMYwaT+wcKcg0cqloM3VXuWLERy5s4yf/Ej6Y2xXWgfWSQeDpX6\n",
       "nf/p6CUele3G95Strm+YwaJAJASP8Zau5zqtVvNIu715yUTKzolTSdI4kRu/W/g4jwx54cBX3IEU\n",
       "3tpYNtvBWyZriAcHSGX3nM5L2qPlT2+gF0EHEpB77YhV7A5BNY4WmLMqtuWTwyU6CV3mamiKyzxX\n",
       "NYpbfz9M56sZ/A4cjUqDM9gVwmdtIKCBiSUW601XBIgGbWrMdinDey6lf7qM0pVHb5ZOmRX6Pupn\n",
       "06eMqaY+XjR1bUQEjOgMPSBOtctqFPCeI0ZkStVoplxz9bzi4bFlVxQRbzxF/YqGEFI+0+/UQynW\n",
       "sgyn+GSq9x2pgSM2izMvcJa7obeVY8qiIxqvPZZI7MBAxMQ4h0wWeKeur/qaX3OUcoMbV9pWzJi2\n",
       "zoJf4J8TpLQA5ABJgmB0FfN2HqtvR8M8mhh33iIXWyODjY/+wZbpq12yQgfrJqtdEQc9PodAe3IC\n",
       "Kq0+V9/QobI3gIJy3rvwgXWMH3lVtOg7dX1VolJaPnJA/ewWZdzyJylv6rbAbsIWtT28LoznD/cJ\n",
       "3Id7ka314dNzR73wKGMga5Im0FBQPNZiRG6/9dEAiGcVhL9p1GhjQGOhYLvG9dNSRUE1WeIKnzM1\n",
       "ZH0nMb2791lYqc6cJiIq2SiefOrtSmKKo7lZYOnpW72DZA8pyR+DGx7MW2OtAJPH4CTn7PY1ta66\n",
       "8EYq+Y0MW5J7wZuFCtq5Sg11TGpcamgVjwW+BzmSBhqMaqT1nlpow/6surnWmajsMiHfslRvHwrk\n",
       "awQNb7SsufmWQbPhsXBcCbwN5K0My/95a9bMN9lo25VlVSECSEfcu5qRlSuG4UWHjI5H5BJBANQO\n",
       "3EEJlnqo7ah+S9wtE3ko1RhjaOmJvNk8h6qx/wr1Vm7KydQfqGAccQAAA6tBnoZFESwr/wAAAwFZ\n",
       "s5iVZ0VH29JmHsQmhYjG4jsBF4fw0L3o85vitPzd5A/Ayzo2+BB6V7Rh9v3t0/JromcLNuFqIoQI\n",
       "saGt/zc5hajis7vcyoro9vEt8rdNsyNHzishyKxKkHUIVmM1UVWLU/EmzlXtWQ6iL49yUiYldGQm\n",
       "ODca90+XgLN61H05Bk5L4h8GoEy2KCYv9++YQ8+AnUtFvHTNAF7ont5Y+qFo5GuG4fi6sZvGka1r\n",
       "sQpH37w14Ek2YRHcL95tSKsenj0nxBdGGnVD3Ex/i5hKRKlHapNUxZXcYIz/lPqXiuC1yKYiMWi9\n",
       "9HJN2l7gkLMUW3XuampD0RV43Vb6ziZS0WTTh+XMoASi3mJsSQIdxtOkoaa61BGSDCgC4uI3Ws2b\n",
       "xpPAMWvWykgFnCo+qMYE4SMgHSwbhBO9CDBYOyeW+1EYd46Okx6QpAXFv9FvU9STpv9t9BTnp8PV\n",
       "lDCQRMkAk2q/TI26bAtq5xp+cxS1gwvOzsUAaox3mXDZONRvemAVAxCqEyRGnxi0MIo6p7QaFESs\n",
       "OCy3JZvrqunA+LI3WQxEexh9PB3D9X/Sj01XyG5orCBCpnBuquWJzHI4j6vJPVThgN3W7Xfil1LJ\n",
       "QrYdI5z8Vz2PPtOiu85ldjTql83JIjEvhMvr68RuYPkk6l0BHCvMdW8nW6gdUt5HTxCf6BPTagGL\n",
       "iyCpux9XMxrI8V0ckUUnLUIA7AlIScXWlHp9MwiI9ZAJEi+HBLcSKoOfzlLQsgBBooq52W+tKD8q\n",
       "UMTi5OusO2nKjVml9YpoxdA62Rht1oY8bwFT1sQcKhZqTal7RK7q8qziFv7RQyblS3HtK78WRRAA\n",
       "BFyb1FpIbOON7+qGQ7uz79Brb9M0WOjrbQF6g+bs5Wumm5n69cMsSSHHOlmkp6Xi4hsPtFnWhaYe\n",
       "/ajZFl+d3EIKtmOHLjh8DnHRoWZYBdAYxYHurtIh44QzUVo6TE9U3UFrtIbuHr9089Bqyo+iMk09\n",
       "w/g0qFVj/iok/5UVVkMOZeG8yC7CCHfyA27tleHJts+Dt5O8aFhVL+5dlHoOlwYJ7E9kYeKDUDhc\n",
       "2WRjvIZ2ZAW+D/5ppAQAws25oMWZLQYZbXb2APY1ob7BcLGQ6PKEeNIx4aKtHyLG8b88QSLY1dlf\n",
       "aDVWR8LJTznrlEDyfoUWAgz/rVVwNHy6cWN75BRkT2dj9b4YwKQgsQs9WXbP/TljSoGaAraPbLF/\n",
       "Gumfv1IvZYLQ0DfowYEAAALlAZ6ldEJ/AAADAbqZeDllFYJFGrgeLF/01MPDKbOvjpVwgA42azJR\n",
       "bJB7N6HRmHonnoJzTv0C7rH8gQK64HlSBb8PuknK0nKJVwsUNxZj19U4rCaZXtyiqC1jwHjnmfeU\n",
       "BTcighbQwenRDRPtkJDA2tJtVRyhM2CBpU9PtXcaIr7H4IadwmXZ83RV91lSYmfW4LlaNRpwPhSV\n",
       "7N2AsWnQ4hkn/FwK7PFcE4POXj/6AzU9SrMaVmj7eCqRU4KRmuSvJca4s9phYIS1s9gMCo0WKVCi\n",
       "eJP+a9HAJ8wLwhZLj3QZDXrx/2pI/oYuLKUgrEZo49bBOf7MZPIJMldjFdHODVIZ1KMzvsCHElhj\n",
       "b7yKy1gR+/kbPbnyXR86LYA7qN2u1MHnS+PCaIohulbGI6lkz728o0oA+wqdBYBrCuUktN9DRhPZ\n",
       "oGNMh8O2wJYyczeALhDLWCcgTOCWXcKOIXHPxjQy7FtebzqpNkCTBpD6zZjZhiAyOpZHB6S44vBT\n",
       "qW9qjinrtbYPBOLYYczMRLhw3HG+Qv/uw8zbRgZy40eccyp1tCyaPUvuZgk2I1Tn20ufs9MhHYcc\n",
       "mw07+uE4UhFVxVChFIUJIuOB3BsdQKcb1c6pkY7Xh1lijQ/8oigf+UfXk/dLgJbwydq5c3euaew3\n",
       "tib1sEpMDCTGWwgpes3J2Z3B1CyqI5S9HgfFZ+stQf83e7Me6Nh8TCxh0l/iHZRI5Ds4sLvF48gz\n",
       "tuJ26FH/Ybt8oXhdhOmtqtSIn6rnhG24ZgHpUS3vUNocUVvyI0g1ORpA7JCrPZsuR4XOqJLQ/nhK\n",
       "4hwx9QzlHsps756RY4//bOAP6MDY5UXvMWwHykgy+bgPOPsIu+uckHSnRomtzNmQoFt3+Ajq92rU\n",
       "f2XUVl3gBPDWFBxUJUfBU7MzIkimE80c3pJa0Y19zrgvEIYnxShdzTvEV0sMwk5dNd+ALNrX3g/V\n",
       "hVriBgcuW4E+l4UhY9k3xoGBAAABvQGep2pCfwAAAwG6PCod0OUgVxOd+q8gA5OADtkrhzL6FsGZ\n",
       "NILJBaw5NmxSTI21MYnX4067SC0RbEqYkq0H5WHfp+lE8BQeO41NfXx6bVoGQf7kk+L7kUBmsN98\n",
       "YiOGmpnoAUMdSAR26z9oBdkhl+5n4JoVWh3mffiR+rvPLM8KDXB0RqnB32wquy+SU1oB9IXBVppi\n",
       "K4mVaP9gp+RbrLGQLwEE3d5whjPrWMQbijuDnaMS7suEBzgMN+3xr/PWkj6y32P5u2S70NochHK+\n",
       "3L2Pv+MBIXhO/3ilIj33a/t3ewBk5L9+d0aM1R9ET4U7IpUcsHyqz1vIg3QX+r6S1IK2Y2InzeP+\n",
       "Opafd4k8A4vfDCj9j/gPmZR8GMCzIm7Kj1yI31UNS8tnlYRnpQCzG021vIRiSfYUqNPIbHRsrEH6\n",
       "Gyxf/Z3u6pLZT+OxODY6vy7EPN4eVyhBGGWTN6WHe63fc/VzXif+i8tCOgqcCXj7AvMklwgUmD00\n",
       "MpdEAREnBIxppyFnJunmYAil4Wy5nEPpLmMo9PpAfnYeNtHvc7/C//WojtKFSC/TU7xRhYnhHu26\n",
       "d/otpLtvQAf0AwIAAASdQZqsSahBbJlMCG///qeEAAADAaYjGcYAOepKBxAonuZTU6+pmN7r4wEw\n",
       "9jmzM+9cf4eUX1SAjOxtctvpXBTMhbfBLDZie//efVyKpVibOo5TgIbWr4Ts31hGOqOEu2dNG2FI\n",
       "QoiXyqpjvc/u3bkPEKruhvT669aVk7l8M9XXbNuI650BsBloBmvOWQSGP1uEo21Qv0+tWkoHisc1\n",
       "WrdIF8cPjnooEG8mPdVksH/c1s/zESlg8lwS/LPGf8LdP4Y6r6XagfoTCbNRFIJ9oafKmaGwYiff\n",
       "J8zcKvhTPF4E8cfPOEnSfG3VGMuv3KrRxCG4dXkIwHmNmrPjuyAd7nxd2nCb7micYt/qwMQ0Cjrc\n",
       "eAatykaOYgITKYuQrqvXSPO43pUqyW0Hpf4AHyPVd3uAenfQJAoxn4jhlWfHWAPgP5/OBqTA6MA3\n",
       "hPjD6wCLtp1K3dZQ00Ot7hX6/L3f+VoYy/SRTSKug6bc9PnW7f2bK/P2VHq6dR7wmchu5o6R4sL5\n",
       "DYNcTdQtdmA37flcf9WPeNoINaBXV67ALnIq4oeMKnM8W0RQ8b7dCfHa9beXJXix5bMQidXtqfpb\n",
       "bmr8UzGt0ywwazPSiwpBI/b+CQaENmuafm3cgmbZw7U1gTXNvX9qtqYswYfDnTKLlgYAXtwHLlp2\n",
       "YAE/4Qj6rgHCL1S5ERkRMthjcJz4rivsq+jtHZ7xKfU7SGCIwXYZT5fqFGARLFaNrXNpnV3s8wvx\n",
       "/bjQevn72esX/YXoT2tmqyvCTMkHUXVvhu1NMDjQ/ZtTI6asapd6DulSlp2CFK4raB9cREXo7nVk\n",
       "JuZp3qh6n+PC+1jNcsGzKRdkpoyERlFOS8/qr/AOnfX2ZAXXvdauYiwTIVvC+fq3FDMsdLYLl+W7\n",
       "TQcs1XoDAe0wOsNCqky4VfKEGz0ZoFFrAgOUR94uk7spxKrGpuBao8jxuuDFX5x7bmv4/dQ5mbkz\n",
       "o/vyovtAfJ0k9CYyFMbiOh9q0wVIlldI3FXPFrAJ4lPCEO1dVYpW9YTJ09+6B8YlHOoIAz24mpFB\n",
       "2F5YFk7ynSnqtpPYGT0y4rzlIXYhONF0jHj6L8z15HDB23fbCrhG/RwUolFbRnG0r9bh/a3RU1Ee\n",
       "SvVOgUM8UxjD96aId4iV7e1/QiGA2Fr2kpwM078UJgRgY87tJZUkxeRhPHR0xS/QV7m62Ag1mcAP\n",
       "ifhtCtC9ytrJ6ekhdh8Fi+a80WDwbEFlIEgz2C6QrrA5zfZa66G5I7TYvH2Cx/+U96MRrOCXX6DV\n",
       "tO2fP1l11r8EHdp9xOGgEECJGR/JOp8Ck5IV8XXQKn9Skd/x+DqvLtA6SNjcH1+9LzXd8AR7SRSQ\n",
       "efCK3eq0I93j5Rnge7MM1TsnvkFzSrfeabtCZNiFzJMcDDz+8AOsbZ0FDzrajPDPDK9pAm/3ZQiK\n",
       "i3mOaCkoY4ky/SsgH1Wj7omLoDaASRgf+z8XReOxnG2Xol/CHDzJJc0LaBegnth+mFsSythYiOzQ\n",
       "+b14yMzyNd51YaG7OMBvnkTU/jD20enkGKT+/yAeCiu64AGVS6d5S4MejE8g5HaFKA1KzdE2FbAA\n",
       "AAFuQZ7KRRUsK/8AAAMBWJDs4TI9LHdIdSy9O06AC+K+BhBE40X68iZs3NWwfsynVkJBwkdEpMbw\n",
       "7wqd+i4z97aigB/GiVNSF8EZyLLGvUueyrBUqxgUuXHCRp8CUae71wa2mRjo9kiJmhetIRNLGnmI\n",
       "U2H+j9TtdZuttuEFO+/X93ElHWS6bc/E3BRPtmBODLAMVwHnkjb6QoalKVrCI/zT/3t7Ocst7laF\n",
       "iYzsAmR5avsDNPaLBaDlaBr5bKFaF+7TIVDtVx4JhsJlxlJcrOq3eeZTdPcOK9bU/pP4c/oLNf78\n",
       "PV8u5FIahUULfAVJQ/dj1Ds8X1k7Nrx/v+W2xNIL32ixg+JgaJ0wocxBDMyW2UsUJYtHAb2zp0KP\n",
       "Y3Dp9EGrAZGp1jyO7o4hLCyojaa59rhKx4fO11ZGOj7hMCf09iQvDJxYdVxE1bqIG6Fy00FB4Edw\n",
       "GqRQbdHQAxDxMLJtfr86V9gASxJCWo0wXgNnAAAAmgGe6XRCfwAAAwGwfxDEmZq3C9kvFbA12ML5\n",
       "BE/F+rBwarzuHVYgHBcLFGS8C9+wAhNtAUjKpLIq0aUMsv6xpjHgEFf+SWwAbWlvpwSVPOywoUkF\n",
       "Z4mvmI16UctfsCNj/t8E/X143YAxN8SFDz83pl6xznrdwZhKL1WGvx3s7BUskcew/i/vHhjmj48h\n",
       "vwTza94+CtKYCK8ABQQAAADGAZ7rakJ/AAADAbB5HFIAkaYmrgN6r8Joc7Host5PSr4ATVs3oPQm\n",
       "dANNTsK5KPjqQM/qzOFwMES2ENg/4WzhZx04NIVi8g4QSzr8pldX9VsgdWOLWlcvcmJ1etOxygkB\n",
       "eB0wKhVIUGAERKpb1FPc3Uu0spmOnfTrNIpxszzntJa6T/QHFvygwGZYpePlS9Jyp1LTLuVwVU8T\n",
       "5PLPCGV1s8fcTXt3bI8+Kg3Bez+DpIvvOnvKzHx+hH2JTo/4XS1UJ4LnQAEXAAAEJEGa8EmoQWyZ\n",
       "TAhv//6nhAAAAwGmLJFIAUF9j4oISlhQY05X76N47vnBTWdXzNkjt0R2Bz7EHpMsKcbe9AFojrhc\n",
       "G2SP2Po/Bl39JTU+cIQxkOZszCPA+/A8zMi/7scBrSj7053bTkPPLQGezhyqWiv5LEeyMhgaJROR\n",
       "yKKok0VDuwHWmtxqbSHXmN30yL9zqI588UkFhhRoP3uUVWIqDeFgL256eGYd+MENNRH3bjc6Mzq4\n",
       "4kRUnSps+vVFSZHc5/mZs3PUmDs5DIPwRK7E4Fg/hF1fXQmqCZ6oE4kVQb5vys8xdqhrmOejbPq8\n",
       "Qo1rbZOXrn3XgNcxM/r7T3ywtKfTMzvYrgyX+hBM5w7EIcAzobOKIxkDLkAsJXm42hAJIKPY2Uxm\n",
       "MPKCLUH3zhtrgUcEeS1FgTL8r9RPbzdWcBjfETW1dfxCt8+KX63RKU35dQIo8C0Mj56mnRvLkM/W\n",
       "VmaazdF9z+DBgo+shy3h4E+22BCy0GcU7QcAf1JE5DpjVoEd52OwM9v4BjMmtS9IVdmlVgzIxeH3\n",
       "woADubx1Tp//p3astFkD69C4hs9r6kP+Uz5ujHS6KIWnffgZlMLfOb/IHOE0YHI0ROS/mvehYgHP\n",
       "64WBmkyHFnel+XFebl+wDkGp21ocnPLcVuuCQZoJLkoT/EvI+9KKaCZ3OXr/tOTLGcMIpSltCaiV\n",
       "BiRTG8NlE+Qb+c7LlLVlR48UrgyU+GL43THmdn80hNCr9BClrBl0XwlqlneiymzgcznkxIbDD41r\n",
       "RCTNx8PRoWnfywLMgFui+M5rqsaK4lYM4ZEKLqwlCNyI9mYm9diZ3+DPjGe+jt+6jXyAiVW/rO6v\n",
       "k4xZbZ31TDGwHMa5XN6mQpkpIPDQ+e7GTeZIj9/ZB762XFhs4PNzo9mXiKaJNQIMro95fEpOq8YP\n",
       "EzPYYOOpVp3q1678/ACHxiX3EbAWnzyrEJzuYQhUApjU5u0tr4L9S5pONBjT6beq3DBHS6HuS9qF\n",
       "aqJ23zi0moY7+jg9akWwUQ0Sb/AAZpgYKTrEuP6YoS1V7Z+WI358XhL15OxuSepgzy1KzrCg0DQe\n",
       "eoLDWrq2f1nz6MNeQQgsmjFfumajYRe6LgT2nrr+DkDfQZn3zAMnNXM1lr2Rougd0EaXU3oc0zJH\n",
       "NbBVFRMrEdG3EV91I58O4OWNuw4auriItmx1FyzZNt3chw1KxlK6zEafEz8VnYAXA/BUYOsjhRFb\n",
       "ylxWCkoT/3L8ysNXBhFvVgVFER+laf58PqKF/RNusZucFsqslENLfIiTyY1aFGlB1hSJXc/XOiwS\n",
       "N25rLWno5bY9IqYlCDvMQV+gG8RhFDzBeT7fTCt/A6Tu9Wrqeq3j2wipB603AmkhKONBFtc9Glw1\n",
       "sqUPP4W2Y8U7DDT7mqnlaTA+7PehagfAK+EAAAD7QZ8ORRUsK/8AAAMBWWie9Lp6JNBv0znkkAJf\n",
       "jeTb4Jh2h7s+aOQjyvOPSlAcdncKagCtnITvrMxeL2pP4yS5tmF65kHME2CxwVWsKIMfVTtIDOee\n",
       "0Gg6oBu4vwXwqhUpLCR/aWg0EoFKwii4XbDVHFVrQ309azg05U4jLjl+nO/oR1F6te9qN4+BlP3v\n",
       "VX2hMxArbryzmbInux7DgEEX8fRjjYjeelyUQai4nKung5UuDA/RanBalT25TjplpackYl3f4gbn\n",
       "ObAu2w/rYX1lDlsOUqUX2nsePhCZ+RPRnv9mOcDMkXrsJhLvilDHakDqdpp0XtFKkQUqMCEAAAB4\n",
       "AZ8tdEJ/AAADAbnyfSj8QxI/dICqe4Sz9i3WbEgs9KXABnkqTtGeq3sRWqre6s044Hm/A3O3sHnt\n",
       "hloEgDvCIOoJxGCRkyop2BIXd+Ko5f41R408bav5Q3Nuj8aIuue8W3IvZssVdiQQv3J0ZJeKFNsA\n",
       "3rQzwAJvAAAAQgGfL2pCfwAAAwG5SfA2gjfk2NUP2FSoNOu/eSaSOFiMVfIcsy++im7Op412gpcW\n",
       "JSfN8a7AB19cC8ji93rwPqAB6QAAAv9BmzRJqEFsmUwIZ//+nhAAAAZtGigAUFx3XOfw0MxbTbZv\n",
       "/2Z71UMT3/HO91xOKKbnEfjJJeONfj616CYyUMKWGc6P3UhQj7L+8/+N/T85YUFfytj7CxgYuzpL\n",
       "5inAMVthTt4l6p6VyIgBO09AWMoFS0LLDEQfH39wdyi6DocuA8WSfouppNBh08utAmzpd0SL+arc\n",
       "nNsiLMNwkNWIscnTjN0ZXAO22uHVLUIqt8mFFO6iftnyBpDXIQMCK5x9aDjP4g+MYLjca8frNsJJ\n",
       "xjv90smr9cwIv6mYNLA0F5vy6YoXlpGA2rA4iz3LFlYsG3uUVpuAwbw1ycu4LYlrqE2GPSFncLqq\n",
       "7VlZ5hjQsnXqoCuGz30xQpDcqGhJT1OZJWEYS1ACg6B3THGhYxdx9VmD+9qQELFRyrEN3N6ejUhY\n",
       "VFMGSVTf8gh/8/7V50hnsamO6dLGqezn9AEY+Rrubnqj2Qksft00hTemyJKleQ7shmh2aaYTkle0\n",
       "M9ysk0xcF+oCvtWrSr5gflcghqMRBEBuCmbbKgucc6uAfH5vWFrhsQOrAGXZhBtcvk9s9PKhw7Ft\n",
       "mdauiwzpi5SM0sQ95sj/JAOSjXlaJJoNOM7FKZ+xM5T8gntsMmngpf+OYIg7S7oIM9gEykNWoAAt\n",
       "JgJRQi0qbvHCRnjHrACV4vcNnARchVq0YgQiWP/0a/DstrFflhs+gk+ppC29SBCaubEmbOzhLPZ/\n",
       "ObQQfWVTIhfvSOsdNdBVu7DXuSC55sbEGU6Mf2QS/VCqo6GkeMYn7nNGxehHjynKNoDdPtLAPNgJ\n",
       "Ntsf6/OlB6Yhxhdnnc+RdN4NtnY4NTzPTNX/YJFUQGRtBUHb1/AnvALxOJBRJ/G47seMxV6qVbyu\n",
       "3EJsFf/TmXc1lNufP2XU7zyI/5EnqHuXpH2i9UPsBlaX/t/SWj6EWZFnSE7c2mBqoWjUTjvTzCRp\n",
       "9a/lxUmh7TZvSPcwsoXuFhqoyzxux2Bsy2WJXm7MGAboXALrl/Q6HsFtWcsfvmbBmQAAANZBn1JF\n",
       "FSwr/wAAAwFZalXzBGuhnir5f+VqMPQQaxXCdAB8lrAFg25X1unf/7GUtLnqrap/5HeoPYvQvrwM\n",
       "0zCpGVYAfjK4RgV0uMcczuO/UddHbBktK81GrWSr+nAgaMNC6ycEp4fzLnHiGLLoGof4oW0sW1lA\n",
       "uHBwjeWHEoJM2pTSBSdBkwLmlYx3s/osjLNUUf9MRPg1+Ca8U11c3VJ12KOl8LLEqHw05kfNLrJp\n",
       "khQlvd3I+bl2DJDCh2bw7TIsOU2zjhdWPDiqsiCDnwcAgGonMgpJAAAAkgGfcXRCfwAAAwGwQUBg\n",
       "AEixgxa0qyv5HTqOjrbHCtqpoATMXN0hL0unLE3jYGH05OOrSfY/uXkzUhR3PgznMGSPmI+90Xtd\n",
       "TwudEshZ2rjkLgWZXoAXsUwQ/d93MKpA/eO6FUEfrb5u4bFxsHkBe2sAnBFBhUVVYENEGTTBdIVr\n",
       "ErzNDTDlx+deoXjXmdOQANmAAAAAfAGfc2pCfwAAAwEl2+lAze8spppXEQZI+L12R7x3U4JgAQjs\n",
       "mgdHapjyjJa84kT8urpG/+3Lwu+DI4Sp2v86SJUFs6+uAbNza3nECDyCWE7ymygibpP4sBFEWEar\n",
       "nYgE6Yd28KbdyMH+/xJH3PV5vl++FWqE65jSyVTABRQAAAK3QZt3SahBbJlMCGf//p4QAAAEFyNu\n",
       "U4AIyBGurfmskc7/RXu/nVUau03Kbre/SPN6nS8ER5j3+1zOGCQEuDdYSsgstlL4YoL3mgk7vdZh\n",
       "MADNy39js/EAEdkc+K7nP7TOawVHpEY3RA4vIeqyvd8Agm+G9k0UEQxKi/3YK4Oz4VRis97zXA2/\n",
       "1MOIm0erchIFMFuD+FYB7xI72hKc5FOvF2JZwAFUCOQ8zURrOIj9bzmziKtqKX7pFsZb6fox2/7H\n",
       "YXCtOyP68VOYDZgCyJFjqpEkCmoVCMkJM2dm4q0Ezc211YB4tzmjP2nfFdee9fZ51QMNYhveXVpq\n",
       "FfeBE8A3Bi5WhUceRYVRSHaCi07Q/sMZb9D270oTSV+UJsUiRvuKEmjiwKzqfCa0zk8+iVw7qNTX\n",
       "5pyUioCPcumAXd2a039xFCm3bt5eAs7sEtV8FgGSw0788uff3QCKvwU0/X/Et3Q0xnaZ8x/Ah20s\n",
       "3o7sLRrK0l1bREDrzwu/hF0MszjJEy6jeP3zuQz4X9ys5i8KxLyWhpqwAAMdNpc/RsQvLy3jUYGx\n",
       "XFQkjut4ELDHBt6JP8TTmw5BViAbZn5QosjjKl1XtYdIBW/esvjRiYgZD36cdLIH7cQdfnmH6XC/\n",
       "ocu79JhmwrYRdDDrd8adAeI70B/fOOiLRI6KZO4UcJ6LY1FM4s7C7RhuoI1aGnmG6W+fhr98Jx1G\n",
       "9w4dZybVbHlOBt8YqH5E1vy1oLF8qfC17ropQH0C1qCz2Omg5z+GLawF8JeM3kirNVopE1HoCIqe\n",
       "iEDQuavtlbn+7By4lAf0NZplPlvjz9iGe9cOeczUijW5x1lAxlzOkrKp6W76BSzNJo7aTDSoOqEr\n",
       "Sc1oKwrIBMXIg0VmFhIwt1BnT9Vgs0wlNDOJgjxVrhUgOpfQfPYsSAdyNUYAQsEAAACQQZ+VRRUs\n",
       "J/8AAAMBJhcwEVBx8uF8LhbJDwH+HO10B9F54vzrzjdfAI6j0K4TE6RXnuF0h4UAJfX45IcYY+bD\n",
       "CxIW78GTP8NL/EUNIRczkbtAaKZIuZ5nJ/mJgYlPaQhVzDuuDppxNrsRilqKWQ2CGm4BKD38wz4A\n",
       "t46kIlpUM/O+kbcaB0jkDVVTJumrpPSAAAAAqQGftmpCfwAAAwG5WS1LFN15yFCTUAF9zH/KaSlK\n",
       "AfeDj8jmU6Vsfdo8kL5aN1ft8JQSNRZcnfKyJUwjWmqKg3zG3etWBUtyFBZJTQyMyJlsNl0H3aj0\n",
       "pt7FkcGtUafdi4psPmgzvEEgDyiHnXwHGXdV2eyWJrBX97addmtuztopv7udLnxGUoT7w+dM4A1m\n",
       "d8WjqHCgfrlJc9/E7Tu/tHQmcHYq2aMAEXEAAAVYQZu7SahBbJlMCFf//jhAAAAZIWRnjQAOOX8G\n",
       "QBaYwEdZuGZG4R9JtDHXPzG8UikY3gAJMV1QF2/nHu/DYXSN2+RgWrrv0/BhWLt4k8A25ZcYkKgy\n",
       "jHWAo0UmsDgqqy3jh2wRugILGjRWGtD87TJ7Ma9kG13JXK+sHMBZmJ5XFYBiOPxcf/I5AA0eS0Aj\n",
       "FuteUWpZJ66ZHMC3ZtYM35u+8w4XrjY+FxAbtX2Eks6iqytiy5jnwQ54eNqjZE5ZucczHp41A9p6\n",
       "lOA4ierR+lvJSyh6CwfMH6Zu6rUEuuucgvUHvryP3jfqudIAzHnsV/TtxlvqKkNwSwPNhEskNgAh\n",
       "+qvAZiiwOagjj8KvRF5HmyaI3ycJynbdvFDQKTBBETbbSfG2PcWid8BralgYzMaYf3MgmVajCur3\n",
       "vQvLb9QQ9r8QzNwDG8GzM3OBlXVOtkBstzaJqebZ9YULqbaBB3f66Gj1ZQiSfVIxillOWM38G2Vf\n",
       "P2JephZmIsZJYRhje2WRSdRY4vSXU+0+uQi1zYhEizgP3gdwnaTKhy/AceU1puu1KJXJ9S75L5ei\n",
       "6194ov/ANZiSkBIkfFsqY3545aiapetVGgVGjvgKM3XZA4+r58W4KtGR24TTZJ8/dNc1LAUw8EtE\n",
       "7Sl98Rq93ZzDHNEGQiAvLpW975FMWKJO7BJaekQDD5qbfuK5TlSh+xw1GDGj99Hku1KQmd0ToxvC\n",
       "srX+XF65y/SC3ne9UQu8uC+TwI0EGwx29xKMfY9C4B1KywVBU4oEeo9Rd1d20tLTMLsYtzk8oauB\n",
       "HorhcoZTMhUgkQ3SbKtcq/2SC8qNw3H4C4ZmbV0LeJg9cIMyQKXxnlVPeQCv8QlW1KJviQE2suWo\n",
       "1AwILxStiTG8q1iWY/95fk4qvcgMolbeJhM4MqratR6sw3QrFbHHe56D7QSJiNyp4lCnpnbZB8+O\n",
       "wKXdbDg8qEYRPHlX1hWAX6PhDsOvtfZJqfBIzWkEUDuhhgASuJVVkBEEB3cmpCrv3tiaugo0Bkz6\n",
       "zRzrLX/yaLxYX+dQL5/wkJ7ZcgvusV2MsQcxODYrXjAw9XgMw/IBUnqVv2FabP5JqNVWf9P0kZVB\n",
       "4FD67ZKTBnBt44kL60tXTdg7IhoicmsOA2Pb7Q4VQUqA6vYnKmb7XfD8uGyvB6md3PYzfNPBaCNo\n",
       "FZHIef9awe9cAYylUZLlja1rnmmAEtIo3Rb/0O2a+kAladksTyFDY2niG52RLnhuObsEDOkEV46w\n",
       "2U1ZkbtJN7df7CFRUtN+h5zkMQAyomSa+NVuTvJ30Xx8KYan/PVCqoOctlhccgFet2Al3qu8hosv\n",
       "Z5WKV28heaIVzYhf46DahEa39H9Tjpivn5VbquoSSYqEFAGYRJyOLInAmGJXIMQkIa/6o8Iqa5hy\n",
       "pFVGyyXee9FXV/mWmUAUAFWgjwwmlqw27AgwHOuW3WBnyAmzvcyH/mE2igluuWamXSNeBDedovoF\n",
       "vRFqTjc1q+kCNapoQUwHtsYYk1FhJb2bDkMAsU/dBotwMt1QwJ1BJLJ4LQTD41n1sOCRjLXQ365E\n",
       "iA0N/G5Yvfg66prnWrgL2iWkjkXhb4cpE7QbWmPUY5r2mNzbcVRPEAdWly0maJI7Kaqxomo6MB2P\n",
       "X4UbPFuAU4qDSQ1IpnwR1NnJa9FLv1T1Ualncxp6NdfHy9sWvNI+XgWHTdmilpgE85dLnX5SnClS\n",
       "GiWmdObCvGkbf6LX3HiqKbNChwZdMGP5MULOh//gLE65ExPnr/kb0ePljBCNcytmV5K0dvSOu3bs\n",
       "Z2VRXwZawoCDXUWEqsVpEbImUEJRkCJVRZ6nwFNBAAADWEGf2UUVLCv/AAADAVj04UDBD0uZPJMc\n",
       "85SoRr/o0yADZ/3We2P53+i6rAmaglGd6tk8E0kCiE3B9tthl15EWFnfg78gCszL5WwTmknl+cu6\n",
       "piDNfpgKG17X5djh8Mv8EU4aN2q4UqstGnqN6dcyQrKlKhOtc2Zdy2qr7hxcMJNJbVsR2Bi9ZyEg\n",
       "smd78UEjb2wc02qsuiof2ZNn/AucnJeplf/4EJKswVVZ0jlmtOzn/Z5FEw+XJPSMcfS/QPgPHXav\n",
       "DztPg/j5yMjb/DAkZPke1JfdVM38aweC3qAKb3EoTx+dmr7xWtbAptysN5ipUQ5je9iJN5YVG18G\n",
       "n3UScqsDYcEdzG1pZorYY2pZSabqgvzkSbNOLjagxYxFNUHdsarNQaJDlO9yVPfCJUtDKA0rqhyq\n",
       "Tg4A4Or1pF8Mw818yQ5n+d4mxTf9bz5DARiQKf+eain6FbF60YRFpXBidXoMD8TYzbg20xEIS2iB\n",
       "+1rgNalIYMoJ2kWGIS26GcBL+ENnqP+NWeKxskcxUK+ZldaOpPnG7SofPUvxs0vH29Fd6+TswChw\n",
       "p8goDtyYIbEIWxE3OGb2NBu7NPEqA12PybAeDO4hMYt4V2atRlst8UsLuBTr5Yw1BAYRicKlKdWg\n",
       "Ga591qu4+ZScm29G8b57p+D9GtYYvhrfDOtbPmhbsEldw5YYdpt7fbbW03iJ4zXtCLkjxwcETqRv\n",
       "max2Eve/EmjdXIctUu2FD+HhHUWa7aHHNAD3amrY0V+MAh/+YUgvGWbKIQ9MbABJQTrBn77L+o3e\n",
       "J77euR5j50L6pHNpodizD/MnqtsjqIoanx1fs6ZLxl6ajnhU8Cezu/9/xyRVe4g1LRtIdec87VwI\n",
       "uF7qQS4xOSX//NKUNZRUgaNBqpC+pwIexx7oBbc7NuZQH/ECJnUDzCEI1SSaRiZWYNET/T3MuSvL\n",
       "1TpZk+6BQSpuzC45ngbdaw5L22whqRDIyW83nxF+VO476akBihcZSHbS5Qi0U1SAr/hyG/qOpMDl\n",
       "ElJqGdR3ZXfO1e8svPDytp0183wpPNK4944JDmJGYVfZgkxdQYCxLD2PCy2oIrBQvRciVcwKyIyn\n",
       "5mek8gv0M2SLG9yC/VnY57835+of9P4D85js9FIhfjoQekAAAAEaAZ/4dEJ/AAADAbnz1R8so/6c\n",
       "j5O0QIhqlSneemuZFMd51vyML0WQkw3EABtBmpkBt9oBsJr/XNkOJgVr8g91RnaKJKkxQulXl6gy\n",
       "4IncmAdrgI02HjEHPC8xTNI8wOCENDYV50/Sij7k5TYN278eJOi3p37yJaj02MGQjPSBcJeVofoz\n",
       "dFf3zn0SKkLOvfquNB3xjsX6jDezT//hsBHGivw0n1OQkBNVGjdpm8FGdiflPSmqb1ZeEimkKXBU\n",
       "RRy3JqjzwVxt2s4u6lxYCWAksINHq+FD7V3DjJHJQRVHFX2Hr64fdGqnkYdCI1qjBwxGEfATAPfn\n",
       "GD1eKTdQjtoD5DM0Gd4/7WyI3GjKSwQzMPiimcE0QfOQAKqBAAAByQGf+mpCfwAAAwGwbxmpUn+V\n",
       "WgZ4AaRcofI5EYYTeps4OJMdpSw6I8bZ8NGNwAQ0kSOmIPCrTSSW/A0/f+FZGx1wOCuzvC05hQE8\n",
       "Xv0cH9o/WbeqP27jv/rWd33eVx/xpHuds7VgkZutRsVu45Y0NJiuWakbdv9n8j91wJflUkb5glmp\n",
       "471i4MLsA596Wg+uisvSeNtLWirDL6r2zB8n6LYDYA10a//8ILf/lhUZ5zPy9GqMrTDMnf0oEr4D\n",
       "bBJmKl5a3+DRCfVyE891vfE+b4gkfiIhk17a75jwhKrPZVqfDLk2KHuO9riF5l/txGx3NWglSbor\n",
       "57pwn3FiQVxCBLV0E1YOpUm+SndtmqmUNEcV9aRNFmOHu1TRSHQw8unxlaRKC8hJffcpNhAHzf/7\n",
       "LDvDN2e342qpmaqpsTsMsEZmaOMrPuUN03DPgefod29OgmvrtsH5+Kt45MVIuYFakjKlZGHIkEg5\n",
       "M2sAEaERKN5mjRIG2CgALNJFkXlrIco8bwaByRy6JCumQLnv4eAjd6KKTcXTf8JNDd94HN0GuYvl\n",
       "Scvbkm5klVPkTzkH5jJzxa5IhXBitjX4uUR80KQXVmnvWNXPReD7AqYAAAK0QZv8SahBbJlMCE//\n",
       "/fEAAAMAPJqYF9fb6alkgEAIvyJNgBVyFcy491mBP49T+M8kuptCgvD7Y7yJnYfevtg55HZdtcsP\n",
       "DkdBYh4qoxcFdxBRfkvl1mi2KWrClUbuwtLvQXbXtdEAHUzRPgVUBurfg4mQg9ACCCGJrHEvp3KU\n",
       "HyurcJwQm3PXJuYsDrUCsRxLPLFrPlFjvUNluBVMlYakJ9sF9YwjR9FxwebSPj8FwFd3pXxtX1Jf\n",
       "oQJZAEjuMULKzmeOUOjHd1haMqlILxzGpOL8xMhst2PezznAj2sFtDCb1r1z3KFlmmCslUke0wuL\n",
       "ViO/M0sOpjlvNE7kFkl3pPnBhPsdRJfD4g3PwHrzLbb/xVYESCnwSGy4N344dLZ/xVTaBegGPSmt\n",
       "TH0KCTOJQRe65FdjY+wGss3ciuEGUCbyYpdrkNmZY/IUe7J78zmH6xl+IN3cfcgdceU8Bo1rPNMP\n",
       "brMbGbP/hAdYTdZ6UJpQxp+wMbp2cubNlOb14OPkW8v4U9SARClFi+Dnhy2KOIvZsLtr/TyhLFzj\n",
       "yHoHVRLN4Jp2rviBovN0G8aLHHadFxbIRPHw3+iYW5pFLy/ZuuRYdH20roOZ8PDZ2KuYRWPBiZM9\n",
       "ucJR7/w7oOe6XdLZW7ilefpieX5hSyPiAl+oiZ6fWen08GtNeqq5/9o5IVIdcGcXZ8ZWGIbDpy9K\n",
       "0dLJzVgg89Rou9nRyY7/+7YlXBIhB1CRAVFRjj7SaAihmYvtLbzgj/+zEaybF6ZC0GI/r6zUGwzh\n",
       "hL9EVPl0qMOzJlngGr74pgpeNBc61KxbnAXy+wNtgnkOesrM6n0a1mstMyjr5NcfQ4Acnar4l1zV\n",
       "/htpxIFHvO+EaR9nlolYOJeZzgw4kMWhewM8lr++qIrdYLJ8g3MXy8+FbQ4e74RlP11ABQUAAASF\n",
       "bW9vdgAAAGxtdmhkAAAAAAAAAAAAAAAAAAAD6AAAA2YAAQAAAQAAAAAAAAAAAAAAAAEAAAAAAAAA\n",
       "AAAAAAAAAAABAAAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAgAA\n",
       "A7B0cmFrAAAAXHRraGQAAAADAAAAAAAAAAAAAAABAAAAAAAAA2YAAAAAAAAAAAAAAAAAAAAAAAEA\n",
       "AAAAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAABAAAAAAoAAAAHgAAAAAAAkZWR0cwAAABxlbHN0\n",
       "AAAAAAAAAAEAAANmAAADAAABAAAAAAMobWRpYQAAACBtZGhkAAAAAAAAAAAAAAAAAAAyAAAAK4BV\n",
       "xAAAAAAALWhkbHIAAAAAAAAAAHZpZGUAAAAAAAAAAAAAAABWaWRlb0hhbmRsZXIAAAAC021pbmYA\n",
       "AAAUdm1oZAAAAAEAAAAAAAAAAAAAACRkaW5mAAAAHGRyZWYAAAAAAAAAAQAAAAx1cmwgAAAAAQAA\n",
       "ApNzdGJsAAAAt3N0c2QAAAAAAAAAAQAAAKdhdmMxAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAAAAoAB\n",
       "4ABIAAAASAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGP//AAAANWF2\n",
       "Y0MBZAAe/+EAGGdkAB6s2UCgPaEAAAMAAwAAAwDIDxYtlgEABmjr48siwP34+AAAAAAcdXVpZGto\n",
       "QPJfJE/FujmlG88DI/MAAAAAAAAAGHN0dHMAAAAAAAAAAQAAAB0AAAGAAAAAFHN0c3MAAAAAAAAA\n",
       "AQAAAAEAAADwY3R0cwAAAAAAAAAcAAAAAQAAAwAAAAABAAAHgAAAAAEAAAMAAAAAAQAAAAAAAAAB\n",
       "AAABgAAAAAEAAAeAAAAAAQAAAwAAAAABAAAAAAAAAAEAAAGAAAAAAQAAB4AAAAABAAADAAAAAAEA\n",
       "AAAAAAAAAQAAAYAAAAABAAAHgAAAAAEAAAMAAAAAAQAAAAAAAAABAAABgAAAAAEAAAeAAAAAAQAA\n",
       "AwAAAAABAAAAAAAAAAEAAAGAAAAAAQAABgAAAAACAAABgAAAAAEAAAeAAAAAAQAAAwAAAAABAAAA\n",
       "AAAAAAEAAAGAAAAAAQAAAwAAAAAcc3RzYwAAAAAAAAABAAAAAQAAAB0AAAABAAAAiHN0c3oAAAAA\n",
       "AAAAAAAAAB0AACj1AAAICQAAAyEAAAHnAAACdAAACoMAAAOvAAAC6QAAAcEAAAShAAABcgAAAJ4A\n",
       "AADKAAAEKAAAAP8AAAB8AAAARgAAAwMAAADaAAAAlgAAAIAAAAK7AAAAlAAAAK0AAAVcAAADXAAA\n",
       "AR4AAAHNAAACuAAAABRzdGNvAAAAAAAAAAEAAAAwAAAAYXVkdGEAAABZbWV0YQAAAAAAAAAhaGRs\n",
       "cgAAAAAAAAAAbWRpcmFwcGwAAAAAAAAAAAAAAAAsaWxzdAAAACSpdG9vAAAAHGRhdGEAAAABAAAA\n",
       "AExhdmY2MS4xLjEwMA==\n",
       "\">\n",
       "  Your browser does not support the video tag.\n",
       "</video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting video for comparison\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "#for i in range(5):\n",
    "sample_id = 14 #i\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)  # Create subplots with 1 row and 2 columns\n",
    "\n",
    "def unnormalize_all_frames(tensor):\n",
    "    unnorm_frames = []\n",
    "    for i in range(tensor.size(1)):\n",
    "        column = tensor.detach()[:, i]\n",
    "        unnorm_frame = utils.unnormalize_mean_std(column)\n",
    "        unnorm_frames.append(unnorm_frame)\n",
    "    # Stack the normalized columns back into a tensor\n",
    "    unnormalized_tensor = torch.stack(unnorm_frames, dim=1)\n",
    "    return unnormalized_tensor\n",
    "\n",
    "def draw_sample_video(n):\n",
    "    ax1.clear()\n",
    "    ax2.clear()\n",
    "    print('frame ', n)\n",
    "    artists = []\n",
    "    \n",
    "    tensor = x_test.transpose(1,2)[sample_id].cpu()\n",
    "    tensor = unnormalize_all_frames(tensor)\n",
    "    \n",
    "    # Plot the first data\n",
    "    utils.draw_from_tensor(tensor[:, n], ax1)\n",
    "    ax1.set_xlim(-2, 2)\n",
    "    ax1.set_ylim(2, -2)\n",
    "    ax1.set_aspect('equal')\n",
    "    ax1.set_title('Original')\n",
    "\n",
    "    tensor2 = x_hat_test.transpose(1,2)[sample_id].cpu()\n",
    "    tensor2 = unnormalize_all_frames(tensor2)\n",
    "    \n",
    "    # Plot the second data\n",
    "    utils.draw_from_tensor(tensor2[:, n], ax2)\n",
    "    ax2.set_xlim(-2, 2)\n",
    "    ax2.set_ylim(2, -2)\n",
    "    ax2.set_aspect('equal')\n",
    "    ax2.set_title('Reconstructed')\n",
    "    \n",
    "    #plt.show()\n",
    "    return artists\n",
    "\n",
    "ani = FuncAnimation(fig, draw_sample_video, frames=29, interval=30, blit=True)\n",
    "\n",
    "from IPython.display import HTML\n",
    "HTML(ani.to_html5_video())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
