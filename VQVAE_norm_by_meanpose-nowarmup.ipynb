{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: torch in /conda/envs/control_v11/lib/python3.8/site-packages (1.12.1)\n",
      "Collecting torch\n",
      "  Downloading torch-2.3.1-cp38-cp38-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: torchvision in /conda/envs/control_v11/lib/python3.8/site-packages (0.13.1)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.18.1-cp38-cp38-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: filelock in /conda/envs/control_v11/lib/python3.8/site-packages (from torch) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /conda/envs/control_v11/lib/python3.8/site-packages (from torch) (4.11.0)\n",
      "Collecting sympy (from torch)\n",
      "  Downloading sympy-1.12.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in /conda/envs/control_v11/lib/python3.8/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /conda/envs/control_v11/lib/python3.8/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /conda/envs/control_v11/lib/python3.8/site-packages (from torch) (2024.3.1)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==2.3.1 (from torch)\n",
      "  Downloading triton-2.3.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: numpy in /conda/envs/control_v11/lib/python3.8/site-packages (from torchvision) (1.23.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /conda/envs/control_v11/lib/python3.8/site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /conda/envs/control_v11/lib/python3.8/site-packages (from jinja2->torch) (2.1.5)\n",
      "Collecting mpmath<1.4.0,>=1.1.0 (from sympy->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading torch-2.3.1-cp38-cp38-manylinux1_x86_64.whl (779.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m114.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m105.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m124.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m102.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m104.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m111.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m111.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m136.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-2.3.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.0/168.0 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.18.1-cp38-cp38-manylinux1_x86_64.whl (7.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m112.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.12.1-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m149.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m99.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: mpmath, triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.12.1\n",
      "    Uninstalling torch-1.12.1:\n",
      "      Successfully uninstalled torch-1.12.1\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.13.1\n",
      "    Uninstalling torchvision-0.13.1:\n",
      "      Successfully uninstalled torchvision-0.13.1\n",
      "Successfully installed mpmath-1.3.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 sympy-1.12.1 torch-2.3.1 torchvision-0.18.1 triton-2.3.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import utils\n",
    "import pandas as pd\n",
    "import io\n",
    "import random\n",
    "import time\n",
    "import zipfile\n",
    "\n",
    "import utils\n",
    "import importlib\n",
    "utils = importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, kernel_size=3, stride=2):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        \n",
    "        # FM mod\n",
    "        self.strided_conv_1 = nn.Conv1d(in_channels=112, out_channels=512, kernel_size=kernel_size, padding=1)\n",
    "        self.strided_conv_2 = nn.Conv1d(in_channels=512, out_channels=512, kernel_size=kernel_size, padding=0)\n",
    "        \n",
    "        self.residual_conv_1 = nn.Conv1d(in_channels=512, out_channels=512, kernel_size=kernel_size, padding=1)\n",
    "        self.residual_conv_2 = nn.Conv1d(in_channels=512, out_channels=512, kernel_size=kernel_size, padding=1)\n",
    "        \n",
    "        self.proj = nn.Conv1d(in_channels=512, out_channels=16, kernel_size=3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.strided_conv_1(x)\n",
    "        x = self.strided_conv_2(x)\n",
    "        \n",
    "        x = F.relu(x)\n",
    "        y = self.residual_conv_1(x)\n",
    "        y = y+x\n",
    "        \n",
    "        x = F.relu(y)\n",
    "        y = self.residual_conv_2(x)\n",
    "        #print(y.shape)\n",
    "        #print(x.shape)\n",
    "        y = y+x\n",
    "        \n",
    "        y = self.proj(y)\n",
    "        print(\"Encoder output: \", y.shape)\n",
    "        return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQEmbeddingEMA(nn.Module):\n",
    "    \"\"\"\n",
    "    After every epoch, run this for random restart:\n",
    "    random_restart()\n",
    "    reset_usage()\n",
    "    \"\"\"\n",
    "    def __init__(self, n_embeddings, embedding_dim, commitment_cost=0.25, decay=0.999, epsilon=1e-5, usage_threshold=1.0e-9):\n",
    "        super(VQEmbeddingEMA, self).__init__()\n",
    "        self.commitment_cost = commitment_cost\n",
    "        self.decay = decay\n",
    "        self.epsilon = epsilon\n",
    "        self.usage_threshold = usage_threshold\n",
    "        self.n_embeddings = n_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # init_bound = 1 / n_embeddings\n",
    "        embedding = torch.Tensor(n_embeddings, embedding_dim)\n",
    "        # embedding.uniform_(-init_bound, init_bound)  # try other types on initialization \n",
    "        # Xavier initalization is designed to keep the scale of the gradients roughly the same in all layers\n",
    "        nn.init.xavier_uniform_(embedding)\n",
    "        self.register_buffer(\"embedding\", embedding)\n",
    "        self.register_buffer(\"ema_count\", torch.zeros(n_embeddings))\n",
    "        self.register_buffer(\"ema_weight\", self.embedding.clone())\n",
    "\n",
    "        # initialize usage buffer for each code as fully utilized\n",
    "        self.register_buffer('usage', torch.ones(self.n_embeddings), persistent=False)\n",
    "\n",
    "    def update_usage(self, min_enc):\n",
    "        self.usage[min_enc] = self.usage[min_enc] + 1  # if code is used add 1 to usage\n",
    "        self.usage /= 2 # decay all codes usage\n",
    "\n",
    "    def reset_usage(self):\n",
    "        print(\"Reset usage of embeddings between epochs\\n\")\n",
    "        self.usage.zero_() #  reset usage between epochs\n",
    "\n",
    "    def random_restart(self):\n",
    "        #  randomly restart all dead codes below threshold with random code in codebook\n",
    "        dead_codes = torch.nonzero(self.usage < self.usage_threshold).squeeze(1)\n",
    "        print(\"Are there any dead codes on this epoch? \", len(dead_codes))  # torch.any(dead_codes != 0))\n",
    "        rand_codes = torch.randperm(self.n_embeddings)[0:len(dead_codes)]\n",
    "        with torch.no_grad():\n",
    "            self.embedding[dead_codes] = self.embedding[rand_codes]\n",
    "\n",
    "    def encode(self, x):\n",
    "        x_flat = x.detach().reshape(-1, self.embedding_dim)\n",
    "\n",
    "        distances = (-torch.cdist(x_flat, self.embedding, p=2)) ** 2\n",
    "\n",
    "        indices = torch.argmin(distances.float(), dim=-1)\n",
    "        quantized = F.embedding(indices, self.embedding)\n",
    "        quantized = quantized.view_as(x)\n",
    "        return quantized, indices.view(x.size(0), x.size(1))\n",
    "    \n",
    "    def retrieve_random_codebook(self, random_indices):\n",
    "        quantized = F.embedding(random_indices, self.embedding)\n",
    "        quantized = quantized.transpose(1, 3)\n",
    "        return quantized\n",
    "\n",
    "    def forward(self, x):\n",
    "        # M, D = self.embedding.size()\n",
    "        x_flat = x.detach().reshape(-1, self.embedding_dim)\n",
    "        \n",
    "        distances = (-torch.cdist(x_flat, self.embedding, p=2)) ** 2\n",
    "\n",
    "        # find closest encodings\n",
    "        # min_encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
    "        # min_encodings = torch.zeros(\n",
    "        #     min_encoding_indices.shape[0], self.n_embeddings).type_as(z)\n",
    "        ### min_encodings.scatter_(1, min_encoding_indices, 1)\n",
    "\n",
    "        indices = torch.argmin(distances.float(), dim=-1)\n",
    "        encodings = F.one_hot(indices, self.n_embeddings).float()\n",
    "        quantized = F.embedding(indices, self.embedding)\n",
    "        quantized = quantized.view_as(x)\n",
    "        \n",
    "        if self.training:\n",
    "            self.ema_count = self.decay * self.ema_count + (1 - self.decay) * torch.sum(encodings, dim=0)\n",
    "            n = torch.sum(self.ema_count)\n",
    "            self.ema_count = (self.ema_count + self.epsilon) / (n + self.n_embeddings * self.epsilon) * n\n",
    "            dw = torch.matmul(encodings.t(), x_flat)\n",
    "            self.ema_weight = self.decay * self.ema_weight + (1 - self.decay) * dw\n",
    "            self.embedding = self.ema_weight / self.ema_count.unsqueeze(-1)\n",
    "\n",
    "        self.update_usage(indices)\n",
    "        \n",
    "        codebook_loss = F.mse_loss(x.detach(), quantized)\n",
    "        e_latent_loss = F.mse_loss(x, quantized.detach())\n",
    "        commitment_loss = self.commitment_cost * e_latent_loss\n",
    "\n",
    "        # preserve gradients\n",
    "        quantized = x + (quantized - x).detach()\n",
    "\n",
    "        avg_probs = torch.mean(encodings, dim=0)\n",
    "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
    "\n",
    "        return quantized, commitment_loss, codebook_loss, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, kernel_size=3, stride=2):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        '''\n",
    "        self.in_proj = nn.Conv2d(input_dim, hidden_dim, kernel_size=1)\n",
    "        \n",
    "        self.residual_conv_1 = nn.Conv2d(hidden_dim, hidden_dim, kernel_1, padding=1)\n",
    "        self.residual_conv_2 = nn.Conv2d(hidden_dim, hidden_dim, kernel_2, padding=0)\n",
    "        \n",
    "        self.strided_t_conv_1 = nn.ConvTranspose2d(hidden_dim, hidden_dim, kernel_3, stride, padding=0)\n",
    "        self.strided_t_conv_2 = nn.ConvTranspose2d(hidden_dim, output_dim, kernel_4, stride, padding=0)\n",
    "        '''\n",
    "        # FM mod\n",
    "        self.in_proj = nn.Conv1d(in_channels=16, out_channels=512, kernel_size=3)\n",
    "\n",
    "        self.residual_conv_1 = nn.Conv1d(in_channels=512, out_channels=512, kernel_size=kernel_size, padding=1)\n",
    "        self.residual_conv_2 = nn.Conv1d(in_channels=512, out_channels=512, kernel_size=kernel_size, padding=1)\n",
    "\n",
    "        self.strided_t_conv_1 = nn.ConvTranspose1d(in_channels=512, out_channels=512, kernel_size=kernel_size, stride=1, padding=0, dilation=1)\n",
    "        self.strided_t_conv_2 = nn.ConvTranspose1d(in_channels=512, out_channels=112, kernel_size=kernel_size, stride=1, padding=0, dilation=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print(\"x shape that is passed into Decoder:\\n\", x.shape)\n",
    "        x = self.in_proj(x)\n",
    "        \n",
    "        y = self.residual_conv_1(x)\n",
    "        y = y+x\n",
    "        x = F.relu(y)\n",
    "        \n",
    "        y = self.residual_conv_2(x)\n",
    "        y = y+x\n",
    "        y = F.relu(y)\n",
    "        \n",
    "        y = self.strided_t_conv_1(y)\n",
    "        y = self.strided_t_conv_2(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting it together here\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, Encoder, Codebook, Decoder):\n",
    "        super(Model, self).__init__()\n",
    "        self.encoder = Encoder\n",
    "        self.codebook = Codebook\n",
    "        self.decoder = Decoder\n",
    "                \n",
    "    def forward(self, x, epoch):\n",
    "        z = self.encoder(x)\n",
    "        # warm up model with no quantization\n",
    "        #if epoch >= 8:\n",
    "        z_quantized, commitment_loss, codebook_loss, perplexity = self.codebook(z)\n",
    "        x_hat = self.decoder(z_quantized)\n",
    "        #else:\n",
    "        #    x_hat = self.decoder(z)\n",
    "        #    commitment_loss, codebook_loss, perplexity = torch.tensor(0.0), torch.tensor(0.0), torch.tensor(0.0)\n",
    "        \n",
    "        return x_hat, commitment_loss, codebook_loss, perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "device = str('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.set_default_device(device)\n",
    "batch_size = 256\n",
    "img_size = (32, 32)  # (width, height) # NOT USED\n",
    "input_dim = 112\n",
    "hidden_dim = 512\n",
    "latent_dim = 16\n",
    "n_embeddings= 512\n",
    "output_dim = 112\n",
    "commitment_beta = 0.30\n",
    "lr = 2e-4\n",
    "epochs = 200  # 50\n",
    "print_step = 50\n",
    "sequence_length = 30\n",
    "\n",
    "\n",
    "encoder = Encoder(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=latent_dim)\n",
    "codebook = VQEmbeddingEMA(n_embeddings=n_embeddings, embedding_dim=latent_dim)\n",
    "decoder = Decoder(input_dim=latent_dim, hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "\n",
    "model = Model(Encoder=encoder, Codebook=codebook, Decoder=decoder).to(device)  # .to(DEVICE, torch.bfloat16)  # mixed precision training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def calculate_beta_log(n, total_iterations=50, initial_beta=0.35, final_beta=0.001, smoothing_factor=0.1):\n",
    "    # a scaling factor based on the initial and final beta values\n",
    "    scale_factor = (np.log(initial_beta) - np.log(final_beta)) / np.log(total_iterations) * smoothing_factor\n",
    "    \n",
    "    # beta value for the current iteration\n",
    "    beta = np.exp(np.log(initial_beta) - scale_factor * np.log(n))\n",
    "    \n",
    "    if beta < final_beta:\n",
    "        beta = final_beta\n",
    "    return beta\n",
    "\n",
    "betas = []\n",
    "for n in range(1, 51):\n",
    "    beta = calculate_beta_log(n)\n",
    "    betas.append(beta)\n",
    "    # print(f\"Iteration {n}: beta = {beta:.6f}\")\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(betas)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Beta')\n",
    "plt.title('Beta values over iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# OLD STUFF\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sslc_pose = pd.read_csv('sslc_pose_2.csv', encoding='utf-8')\n",
    "train_df, test_df = train_test_split(sslc_pose, test_size=0.2, random_state=42)\n",
    "\n",
    "import importlib\n",
    "import utils\n",
    "utils = importlib.reload(utils)\n",
    "\n",
    "train_dataset = utils.PoseDataset(\n",
    "    df=train_df, root_dir='./SSL_video_eaf/SSLC_poses/',\n",
    "    sequence_length=sequence_length, normalize_by_mean_pose=True\n",
    ")\n",
    "test_dataset = utils.PoseDataset(\n",
    "    df=test_df, root_dir='./SSL_video_eaf/SSLC_poses/',\n",
    "    sequence_length=sequence_length, normalize_by_mean_pose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZipPoseDataset @ ./SSL_video_eaf/SSLC_poses_norm.zip with max_length=112, in_memory=True\n",
      "Total files 512\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import utils\n",
    "utils = importlib.reload(utils)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "# from sign_vq.dataset import ZipPoseDataset, PackedDataset\n",
    "# TODO: add some kind of collate function to the DataLoader\n",
    "\n",
    "\n",
    "def load_data(path_to_data, output_dim=112, normalize_by_mean_pose=True, num_workers=0, batch_size=256):\n",
    "    \"\"\" load up to torch.utils.data.DataLoader \"\"\"\n",
    "    \n",
    "    sslc_pose = pd.read_csv('sslc_pose_2.csv', encoding='utf-8')\n",
    "    \n",
    "    if path_to_data.endswith(\".zip\"):\n",
    "        dataset = utils.ZipPoseDataset(\n",
    "            path_to_data, in_memory=True,\n",
    "            dtype=torch.float32,\n",
    "            max_length=output_dim,\n",
    "            df=sslc_pose\n",
    "        )\n",
    "        \n",
    "        training_dataset = dataset.slice(50, None)  # TODO: change to 10%\n",
    "        test_dataset = dataset.slice(0, 50)   # TODO: change to 10%\n",
    "        shuffle = True  # Shuffle is only slow without in_memory since the zip file is read sequentially\n",
    "        num_workers = 0  # Reading from multiple workers errors out since the zip file is read sequentially\n",
    "\n",
    "        # training_iter_dataset = utils.PackedDataset(training_dataset, max_length=sequence_length, shuffle=shuffle)\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            training_dataset, batch_size=batch_size,\n",
    "            num_workers=num_workers\n",
    "        )\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset, batch_size=batch_size,\n",
    "            shuffle=False, num_workers=num_workers\n",
    "        )\n",
    "    else:\n",
    "        train_df, test_df = train_test_split(sslc_pose, test_size=0.2, random_state=42)\n",
    "        \n",
    "        training_dataset = utils.PoseDataset(\n",
    "            df=train_df, root_dir='./SSL_video_eaf/SSLC_poses/',\n",
    "            sequence_length=sequence_length, normalize_by_mean_pose=normalize_by_mean_pose\n",
    "        )\n",
    "        test_dataset = utils.PoseDataset(\n",
    "            df=test_df, root_dir='./SSL_video_eaf/SSLC_poses/',\n",
    "            sequence_length=sequence_length, normalize_by_mean_pose=normalize_by_mean_pose\n",
    "        )\n",
    "\n",
    "        kwargs = {'num_workers': num_workers, 'pin_memory': True} \n",
    "\n",
    "        train_loader = DataLoader(dataset=training_dataset, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "        test_loader  = DataLoader(dataset=test_dataset,  batch_size=batch_size, shuffle=False,  **kwargs)\n",
    "\n",
    "    return training_dataset, test_dataset, train_loader, test_loader\n",
    "\n",
    "\n",
    "training_dataset, test_dataset, train_loader, test_loader = load_data(\n",
    "    './SSL_video_eaf/SSLC_poses_norm.zip', output_dim=output_dim,\n",
    "    normalize_by_mean_pose=False, num_workers=1,\n",
    "    batch_size=256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbsAAAGiCAYAAAB+sGhNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAArh0lEQVR4nO3de3hU5aHv8d8EyIQImYiEXCBguGziBQmEgkE3l0NqghyFyragnHI5CELRCqFa4tMNj3b7pCKIR4qNbCtgq9XSAlba4sZwcauRSyBbREgNAoFAAoKZIVwSTN7zR2QkkAQCWZPw8v34rAdmzfvO/DLG+bnWrDXLZYwxAgDAYkGNHQAAAKdRdgAA61F2AADrUXYAAOtRdgAA61F2AADrUXYAAOtRdgAA61F2AADrUXYAAOs5Vnb79u3TxIkTFRcXp5YtW6pLly6aM2eOysvL65x35swZTZs2TTfddJNatWqlkSNHqri42KmYAIDrgGNlt3v3blVWVurVV1/Vzp07tWDBAmVmZurpp5+uc96MGTP03nvvafny5dq4caMOHTqkBx54wKmYAIDrgCuQXwT9wgsv6Le//a2++uqrGu/3er2KiIjQW2+9pX/7t3+TVFWat9xyi7Kzs3XnnXcGKioAwCLNA/lkXq9Xbdq0qfX+nJwcnT17VsnJyf518fHx6tixY61lV1ZWprKyMv/tyspKHT9+XDfddJNcLlfD/gAAAMcZY3TixAnFxMQoKKhhdkAGrOzy8/O1cOFCzZs3r9YxRUVFCg4OVnh4eLX1kZGRKioqqnFORkaGnnnmmYaMCgBoAg4cOKAOHTo0yGPVu+xmzZql559/vs4xu3btUnx8vP92YWGhUlNT9eCDD2rSpEn1T1mH9PR0paWl+W97vV517NhRBw4cUFhYWIM+FwDAeT6fT7GxsWrdunWDPWa9y27mzJkaP358nWM6d+7s//uhQ4c0ePBg9e/fX4sXL65zXlRUlMrLy1VSUlJt6664uFhRUVE1znG73XK73RetDwsLo+wA4BrWkB9F1bvsIiIiFBERcVljCwsLNXjwYCUmJmrJkiWX3PeamJioFi1aKCsrSyNHjpQk5eXlqaCgQElJSfWNCgCAJAdPPSgsLNSgQYPUsWNHzZs3T0ePHlVRUVG1z94KCwsVHx+vzZs3S5I8Ho8mTpyotLQ0rV+/Xjk5OZowYYKSkpI4EhMAcMUcO0Bl7dq1ys/PV35+/kUfMJ472+Hs2bPKy8vTqVOn/PctWLBAQUFBGjlypMrKypSSkqJXXnnFqZgAgOtAQM+zCwSfzyePxyOv18tndgBwDXLifZzvxgQAWI+yAwBYj7IDAFiPsgMAWI+yAwBYj7IDAFiPsgMAWI+yAwBYj7IDAFiPsgMAWI+yAwBYj7IDAFiPsgMAWI+yAwBYj7IDAFiPsgMAWI+yAwBYj7IDAFiPsgMAWI+yAwBYj7IDAFiPsgMAWI+yAwBYj7IDAFiPsgMAWI+yAwBYj7IDAFiPsgMAWI+yAwBYj7IDAFiPsgMAWI+yAwBYj7IDAFiPsgMAWI+yAwBYz7Gye+6559S/f3+FhoYqPDz8suaMHz9eLper2pKamupURADAdaK5Uw9cXl6uBx98UElJSfrd73532fNSU1O1ZMkS/2232+1EPADAdcSxsnvmmWckSUuXLq3XPLfbraioKAcSAQCuV03uM7sNGzaoXbt26t69u6ZOnapjx47VOb6srEw+n6/aAgDA+ZpU2aWmpuqNN95QVlaWnn/+eW3cuFFDhw5VRUVFrXMyMjLk8Xj8S2xsbAATAwCuBfUqu1mzZl10AMmFy+7du684zOjRo3X//ferR48eGjFihFavXq0tW7Zow4YNtc5JT0+X1+v1LwcOHLji5wcA2Klen9nNnDlT48ePr3NM586drybPRY/Vtm1b5efna8iQITWOcbvdHMQCAKhTvcouIiJCERERTmW5yMGDB3Xs2DFFR0cH7DkBAPZx7DO7goIC5ebmqqCgQBUVFcrNzVVubq5KS0v9Y+Lj47Vy5UpJUmlpqZ588kl9+umn2rdvn7KysjR8+HB17dpVKSkpTsUEAFwHHDv1YPbs2Vq2bJn/dq9evSRJ69ev16BBgyRJeXl58nq9kqRmzZrps88+07Jly1RSUqKYmBjdc889+tWvfsVuSgDAVXEZY0xjh2hIPp9PHo9HXq9XYWFhjR0HAFBPTryPN6lTDwAAcAJlBwCwHmUHALAeZQcAsB5lBwCwHmUHALAeZQcAsB5lBwCwHmUHALAeZQcAsB5lBwCwHmUHALAeZQcAsB5lBwCwHmUHALAeZQcAsB5lBwCwHmUHALAeZQcAsB5lBwCwHmUHALAeZQcAsB5lBwCwHmUHALAeZQcAsB5lBwCwHmUHALAeZQcAsB5lBwCwHmUHALAeZQcAsB5lBwCwHmUHALAeZQcAsB5lBwCwnmNl99xzz6l///4KDQ1VeHj4Zc0xxmj27NmKjo5Wy5YtlZycrC+//NKpiACA64RjZVdeXq4HH3xQU6dOvew5c+fO1csvv6zMzExt2rRJN9xwg1JSUnTmzBmnYgIArgMuY4xx8gmWLl2q6dOnq6SkpM5xxhjFxMRo5syZ+vnPfy5J8nq9ioyM1NKlSzV69OjLej6fzyePxyOv16uwsLCrjQ8ACDAn3sebzGd2e/fuVVFRkZKTk/3rPB6P+vXrp+zs7FrnlZWVyefzVVsAADhfkym7oqIiSVJkZGS19ZGRkf77apKRkSGPx+NfYmNjHc0JALj21KvsZs2aJZfLVeeye/dup7LWKD09XV6v178cOHAgoM8PAGj6mtdn8MyZMzV+/Pg6x3Tu3PmKgkRFRUmSiouLFR0d7V9fXFyshISEWue53W653e4rek4AwPWhXmUXERGhiIgIR4LExcUpKipKWVlZ/nLz+XzatGlTvY7oBADgQo59ZldQUKDc3FwVFBSooqJCubm5ys3NVWlpqX9MfHy8Vq5cKUlyuVyaPn26/uM//kN//etftWPHDo0dO1YxMTEaMWKEUzEBANeBem3Z1cfs2bO1bNky/+1evXpJktavX69BgwZJkvLy8uT1ev1jnnrqKZ08eVKTJ09WSUmJ7r77bq1Zs0YhISFOxQQAXAccP88u0DjPDgCubVafZwcAgFMoOwCA9Sg7AID1KDsAgPUoOwCA9Sg7AID1KDsAgPUoOwCA9Sg7AID1KDsAgPUoOwCA9Sg7AID1KDsAgPUoOwCA9Sg7AID1KDsAgPUoOwCA9Sg7AID1KDsAgPUoOwCA9Sg7AID1KDsAgPUoOwCA9Sg7AID1KDsAgPUoOwCA9Sg7AID1KDsAgPUoOwCA9Sg7AID1KDsAgPUoOwCA9Sg7AID1KDsAgPUoOwCA9Sg7AID1HC2748ePa8yYMQoLC1N4eLgmTpyo0tLSOucMGjRILper2jJlyhQnYwIALNfcyQcfM2aMDh8+rLVr1+rs2bOaMGGCJk+erLfeeqvOeZMmTdKzzz7rvx0aGupkTACA5Rwru127dmnNmjXasmWL+vTpI0lauHCh7r33Xs2bN08xMTG1zg0NDVVUVNRlPU9ZWZnKysr8t30+39UFBwBYx7HdmNnZ2QoPD/cXnSQlJycrKChImzZtqnPum2++qbZt2+r2229Xenq6Tp06VevYjIwMeTwe/xIbG9tgPwMAwA6ObdkVFRWpXbt21Z+seXO1adNGRUVFtc57+OGH1alTJ8XExOizzz7TL37xC+Xl5WnFihU1jk9PT1daWpr/ts/no/AAANXUu+xmzZql559/vs4xu3btuuJAkydP9v+9R48eio6O1pAhQ7Rnzx516dLlovFut1tut/uKnw8AYL96l93MmTM1fvz4Osd07txZUVFROnLkSLX13377rY4fP37Zn8dJUr9+/SRJ+fn5NZYdAACXUu+yi4iIUERExCXHJSUlqaSkRDk5OUpMTJQkrVu3TpWVlf4Cuxy5ubmSpOjo6PpGBQBAkoMHqNxyyy1KTU3VpEmTtHnzZn388cd67LHHNHr0aP+RmIWFhYqPj9fmzZslSXv27NGvfvUr5eTkaN++ffrrX/+qsWPHasCAAbrjjjucigoAsJyjJ5W/+eabio+P15AhQ3Tvvffq7rvv1uLFi/33nz17Vnl5ef6jLYODg/XBBx/onnvuUXx8vGbOnKmRI0fqvffeczImAMByLmOMaewQDcnn88nj8cjr9SosLKyx4wAA6smJ93G+GxMAYD3KDgBgPcoOAGA9yg4AYD3KDgBgPcoOAGA9yg4AYD3KDgBgPcoOAGA9yg4AYD3KDgBgPcoOAGA9yg4AYD3KDgBgPcoOAGA9yg4AYD3KDgBgPcoOAGA9yg4AYD3KDgBgPcoOAGA9yg4AYD3KDgBgPcoOAGA9yg4AYD3KDgBgPcoOAGA9yg4AYD3KDgBgPcoOAGA9yg4AYD3KDgBgPcoOAGA9yg4AYD3KDgBgvYCU3aJFi3TzzTcrJCRE/fr10+bNm+scv3z5csXHxyskJEQ9evTQ3//+90DEBABYyvGye+edd5SWlqY5c+Zo27Zt6tmzp1JSUnTkyJEax3/yySd66KGHNHHiRG3fvl0jRozQiBEj9PnnnzsdFQBgKZcxxjj5BP369dMPfvAD/eY3v5EkVVZWKjY2Vo8//rhmzZp10fhRo0bp5MmTWr16tX/dnXfeqYSEBGVmZl40vqysTGVlZf7bPp9PsbGx8nq9CgsLc+AnAgA4yefzyePxNOj7uKNbduXl5crJyVFycvL3TxgUpOTkZGVnZ9c4Jzs7u9p4SUpJSal1fEZGhjwej3+JjY1tuB8AAGAFR8vu66+/VkVFhSIjI6utj4yMVFFRUY1zioqK6jU+PT1dXq/Xvxw4cKBhwgMArNG8sQNcLbfbLbfb3dgxAABNmKNbdm3btlWzZs1UXFxcbX1xcbGioqJqnBMVFVWv8QAAXIqjZRccHKzExERlZWX511VWViorK0tJSUk1zklKSqo2XpLWrl1b63gAAC7F8d2YaWlpGjdunPr06aO+ffvqpZde0smTJzVhwgRJ0tixY9W+fXtlZGRIkp544gkNHDhQ8+fP17Bhw/T2229r69atWrx4sdNRAQCWcrzsRo0apaNHj2r27NkqKipSQkKC1qxZ4z8IpaCgQEFB329g9u/fX2+99ZZ++ctf6umnn1a3bt20atUq3X777U5HBQBYyvHz7ALNifMzAACBc82dZwcAQFNA2QEArEfZAQCsR9kBAKxH2QEArEfZAQCsR9kBAKxH2QEArEfZAQCsR9kBAKxH2QEArEfZAQCsR9kBAKxH2QEArEfZAQCsR9kBAKxH2QEArEfZAQCsR9kBAKxH2QEArEfZAQCsR9kBAKxH2QEArEfZAQCsR9kBAKxH2QEArEfZAQCsR9kBAKxH2QEArEfZAQCsR9kBAKxH2QEArEfZAQCsR9kBAKxH2QEArBeQslu0aJFuvvlmhYSEqF+/ftq8eXOtY5cuXSqXy1VtCQkJCURMAIClHC+7d955R2lpaZozZ462bdumnj17KiUlRUeOHKl1TlhYmA4fPuxf9u/f73RMAIDFHC+7F198UZMmTdKECRN06623KjMzU6GhoXr99ddrneNyuRQVFeVfIiMjnY4JALCYo2VXXl6unJwcJScnf/+EQUFKTk5WdnZ2rfNKS0vVqVMnxcbGavjw4dq5c2etY8vKyuTz+aotAACcz9Gy+/rrr1VRUXHRlllkZKSKiopqnNO9e3e9/vrrevfdd/WHP/xBlZWV6t+/vw4ePFjj+IyMDHk8Hv8SGxvb4D8HAODa1uSOxkxKStLYsWOVkJCggQMHasWKFYqIiNCrr75a4/j09HR5vV7/cuDAgQAnBgA0dc2dfPC2bduqWbNmKi4urra+uLhYUVFRl/UYLVq0UK9evZSfn1/j/W63W263+6qzAgDs5eiWXXBwsBITE5WVleVfV1lZqaysLCUlJV3WY1RUVGjHjh2Kjo52KiYAwHKObtlJUlpamsaNG6c+ffqob9++eumll3Ty5ElNmDBBkjR27Fi1b99eGRkZkqRnn31Wd955p7p27aqSkhK98MIL2r9/vx555BGnowIALOV42Y0aNUpHjx7V7NmzVVRUpISEBK1Zs8Z/0EpBQYGCgr7fwPzmm280adIkFRUV6cYbb1RiYqI++eQT3XrrrU5HBQBYymWMMY0doiH5fD55PB55vV6FhYU1dhwAQD058T7e5I7GBACgoVF2AADrUXYAAOtRdgAA61F2AADrUXYAAOtRdgAA61F2AADrUXYAAOtRdgAA61F2AADrUXYAAOtRdgAA61F2AADrUXYAAOtRdgAA61F2AADrUXYAAOtRdgAA61F2AADrUXYAAOtRdgAA61F2AADrUXYAAOtRdgAA61F2AADrUXYAAOtRdgAA61F2AADrUXYAAOtRdgAA61F2AADrUXYAAOtRdgAA61F2AADrOVp2H374oe677z7FxMTI5XJp1apVl5yzYcMG9e7dW263W127dtXSpUudjAgAuA44WnYnT55Uz549tWjRossav3fvXg0bNkyDBw9Wbm6upk+frkceeUTvv/++kzEBAJZr7uSDDx06VEOHDr3s8ZmZmYqLi9P8+fMlSbfccos++ugjLViwQCkpKU7FBABYrkl9Zpedna3k5ORq61JSUpSdnV3rnLKyMvl8vmoLAADna1JlV1RUpMjIyGrrIiMj5fP5dPr06RrnZGRkyOPx+JfY2NhARAUAXEOaVNldifT0dHm9Xv9y4MCBxo4EAGhiHP3Mrr6ioqJUXFxcbV1xcbHCwsLUsmXLGue43W653e5AxAMAXKOa1JZdUlKSsrKyqq1bu3atkpKSGikRAMAGjpZdaWmpcnNzlZubK6nq1ILc3FwVFBRIqtoFOXbsWP/4KVOm6KuvvtJTTz2l3bt365VXXtGf/vQnzZgxw8mYAADLOVp2W7duVa9evdSrVy9JUlpamnr16qXZs2dLkg4fPuwvPkmKi4vT3/72N61du1Y9e/bU/Pnz9dprr3HaAQDgqriMMaaxQzQkn88nj8cjr9ersLCwxo4DAKgnJ97Hm9RndgAAOIGyAwBYj7IDAFiPsgMAWI+yAwBYj7IDAFiPsgMAWI+yAwBYj7IDAFiPsgMAWI+yAwBYj7IDAFiPsgMAWI+yAwBYj7IDAFiPsgMAWI+yAwBYj7IDAFiPsgMAWI+yAwBYj7IDAFiPsgMAWI+yAwBYj7IDAFiPsgMAWI+yAwBYj7IDAFiPsgMAWI+yAwBYj7IDAFiPsgMAWI+yAwBYj7IDAFiPsgMAWI+yAwBYz9Gy+/DDD3XfffcpJiZGLpdLq1atqnP8hg0b5HK5LlqKioqcjAkAsJyjZXfy5En17NlTixYtqte8vLw8HT582L+0a9fOoYQAgOtBcycffOjQoRo6dGi957Vr107h4eENHwgAcF1qkp/ZJSQkKDo6Wj/84Q/18ccf1zm2rKxMPp+v2gIAwPmaVNlFR0crMzNTf/nLX/SXv/xFsbGxGjRokLZt21brnIyMDHk8Hv8SGxsbwMTXHyOjr+XTPh3R1/LJyDR2JAC4JJcxJiDvVi6XSytXrtSIESPqNW/gwIHq2LGjfv/739d4f1lZmcrKyvy3fT6fYmNj5fV6FRYWdjWRcZ4SndQybdRC/UN7VOxf30WRelxDNU4DFa4bGjEhAFv4fD55PJ4GfR9vUlt2Nenbt6/y8/Nrvd/tdissLKzagob1vnLVQVM0Q8v0lY5Uu+8rHdEMLVMHTdH7ym2cgABwCU2+7HJzcxUdHd3YMa5b7ytXw/RrnVa5zHf/nO/cutMq1zD9msID0CQ5ejRmaWlpta2yvXv3Kjc3V23atFHHjh2Vnp6uwsJCvfHGG5Kkl156SXFxcbrtttt05swZvfbaa1q3bp3+67/+y8mYqEWJTmqk5svIqPISn81VyihI0kjN10FlsksTQJPiaNlt3bpVgwcP9t9OS0uTJI0bN05Lly7V4cOHVVBQ4L+/vLxcM2fOVGFhoUJDQ3XHHXfogw8+qPYYCJxl2qhT323RXY5KGZ1Sud7QRv1M9zqcDgAuX8AOUAkUJz7YvB4ZGXXTz/SVii/acdldeSpQR51W6EXzXHKps9rpS70sl1yBigvAItflASpoHMd0QnsuKjqpgw7qDu1Qit5XrAoummdktEfFOq7SwAQFgMtA2aFGpTpz0Tq3zqi3qs55bKFvZerYcjuh045lA4D6ouxQo1YKuWhdL22XW+WSpAPqoIOq/QT+1mrpWDYAqC/KDjW6Sa3VRZH+z9066IBidVCSVKZgbVevGue55FIXRaqNWgUsKwBcCmWHGrnk0uOq+hLv83dfStI29VZZDVt+5/xMQzk4BUCTQtmhVuM0UKEKVu/zdl8eVHsdVIcaxwfJpVAFa6wGBjImAFwSZYdahesGzdMd6nDe7stt6i3VsNUWJJdccmmFfs4J5QCaHMoOtfLpqLZorv/2dvVW+QUHnri++6elgvV3pese9Qx0TAC4JMoOtfqdHtMJfS1J6qX7NVO/VGdVv2p8Z7XTSxqnQr1K0QFoshz9ujBcuz7Vn5WtP0mSWusmTdVihStSP9O9Oq5SndBptVZLtVErDkYB0ORRdriIT0f1mn7qv/1/9RuFK1JS1W7Lm9RaN6l1Y8UDgHqzdjemXd/4GViv63H5dFSS1Fc/Un+NauREAHB1rN2y+7i71H6AFH63dOO/Sq3vkFzNGjtV0/ep/qJP9I4kqZXa6BG9wm5KANc8a8su+mGpbIv0zyclUy41ay2FJ31ffp6+UrOLv7T/yp0+Lfl8UliY1PLa/Kosn77Wa5rqv121+zKqERMBQMOwdjfmI1HSomnStnelsmVS6yckNZP2z5e2DpbWeaRNd0p5T0pH3pXKv77CJ/roI+mBB6RWraSoqKo/H3hA+vjjhvxxAuL83Zc/0AjdpdGNnAgAGoa117N7+HWv9pSGadcRyffdF/gHN5O63yQlnZUSDksd8qWW/yNVHq66/4Z4KfxfpRvvrtoCbBknuerag/fb30rTpknNmknffvv9+ubNpYoK6ZVXpClTHPtZG9ImrdB8jZRUtfvyRe1kqw5Ao3DienbWlt1T3jfUPqyd2powBfnC5T3SRkePtNKB4hD980gz7SqWir+75FrkCWnwN1KfI1LcXqnV/qr1LaKlNueVX7XP/T76SBowoO4jYVwu6b//W7rrLkd/5qvl09eaqdvk1RFJ0s/0pu7Ww42cCsD1yomys/Yzuzf1iY6pTGdcZyWPqpZuVfe1Uogi1Fq9TkXIfaSjXEfa68viSG070kbHisNUfqilbj0k3VYo9frUqPMKqfm3LlWGSs0SpXYDpQ4b3ld4UCs1qzhRe4hmzaQFC5p82S3Rz/xF10fDdZceauREANCwrN2yO+T9QK3DWuqUvtXXOq1jOv3dn2fO+/PMReu+UZlMebB0NFo6EiMVt1eLA+11y/abdcvONrrtQJBuOyS1KpO+DarUoZhvVNLhn2oWla3kfy5Wjy/yqgcKCpJKS5vsQSubtVLz9IAk6QbdqBe1UzcqupFTAbiesWVXD3uVXO2Kauc27rpcYt63cskXHKxv2oeopL1b38itbxSiErl1rCJUHxzrojcPddMNW3qr0ye3qvtn0bp9552K+DRJq38cqh5fTK3+gJWVVUdpNsGyO6Fj+k99/5ni/9VCig6Alawtu676VGH+ujv/KJNL/91V0/qyMmnzlqrP4Db+Qcp/1j/C16G5doQP0JiNF2zVSVVbdg30fyYNrfruy/v5nA6AtawtuxDdohBdZcl8+aW0Zk3Vsn591bl07dtLQ4dKJW2lnBypokIR+lb/S+sunt+8uTR8eJPcqtusVfpIb0mq2n05SZmcPA7AWtaW3RU5ebKq1M4V3J49UosWVUddPvuslJoq3XZb1VGW547GrEtFhTRjRmCy18OFuy8n6GV2XwKw2vVddsZIX3zxfbl9+KFUXi7FxVVtvaWmSoMHV50ofqG77646j+6nP637PLsmeCTmEj0hr4olSYm6T/+qMY2cCACcdf2VndcrZWV9X3AHDkghIVWl9sILVQXXrdslzib/zpQpUo8eVacXrFxZdTBKUFDVrssZM5pk0W3Ru/pIb0qSblA4uy8BXBfsL7vKSul//uf7cvvkk6qtsO7dpZEjq8ptwIAr/1ztrruqlmvguzFLdVyL9aj/9gS9rDaKacREABAY9pbdn/9ctVtyzRqpuFi64QZpyBBp4UIpJaVqV2VDatmyyZbcOefvvuyt/61/1f9p5EQAEBj2lt3EiVW7GMeOrfr87a67pODgxk7VaLbqr/pv/UFS1e7LyXqV3ZcArhv2lt2uXVJ8fGOnaBIu3H05Xv+P3ZcArivWXuJHMbyZn7NU01WiIklSbw3TAP2kkRMBQGDZW3aQJG3Ve/pQv5ckhcrD7ksA1yXKzmJVuy8n+29X7b5s34iJAKBxUHYWW6oZ/t2XvXSvBmpsIycCgMZB2VmqavflG5Kqdl8+qsXsvgRw3aLsLFSqb/Sf1Y6+fIndlwCua5SdhZZphr7RYUlSLw3VQI1r5EQA0LgcLbuMjAz94Ac/UOvWrdWuXTuNGDFCeXk1XPPtAsuXL1d8fLxCQkLUo0cP/f3vf3cyplW26W/aqGWSzh19ye5LAHC07DZu3Khp06bp008/1dq1a3X27Fndc889OnnyZK1zPvnkEz300EOaOHGitm/frhEjRmjEiBH6/PPPnYxqhVJ9o1fPO/pynBboJnVoxEQA0DS4jDEmUE929OhRtWvXThs3btSAWq4FN2rUKJ08eVKrV6/2r7vzzjuVkJCgzMzMi8aXlZWprKzMf9vr9apjx446cOCAwproFcKd8p+a6r8gaw8la6b+zFYdgGuOz+dTbGysSkpK5PF4GuQxA/p1YV6vV5LUpk2bWsdkZ2crLS2t2rqUlBStWrWqxvEZGRl65plnLlofGxt75UEtsEof6FcKb+wYAHDFjh07du2VXWVlpaZPn6677rpLt99+e63jioqKFBkZWW1dZGSkioqKahyfnp5erRxLSkrUqVMnFRQUNNiLFCjn/m/mWtsqJXdgkTvwrtXs12ruc3vo6towqq+Ald20adP0+eef66OPPmrQx3W73XK73Ret93g819S/3POFhYVdk9nJHVjkDrxrNfu1mjsoqOEOKwlI2T322GNavXq1PvzwQ3XoUPcBE1FRUSouLq62rri4WFFRUU5GBABYzNGjMY0xeuyxx7Ry5UqtW7dOcZdxwdSkpCRlZWVVW7d27VolJSU5FRMAYDlHt+ymTZumt956S++++65at27t/9zN4/Go5XdX9R47dqzat2+vjIwMSdITTzyhgQMHav78+Ro2bJjefvttbd26VYsXL76s53S73ZozZ06Nuzabums1O7kDi9yBd61mJ/f3HD31wOWq+bD3JUuWaPz48ZKkQYMG6eabb9bSpUv99y9fvly//OUvtW/fPnXr1k1z587Vvffe61RMAIDlAnqeHQAAjYHvxgQAWI+yAwBYj7IDAFiPsgMAWO+aL7t9+/Zp4sSJiouLU8uWLdWlSxfNmTNH5eXldc47c+aMpk2bpptuukmtWrXSyJEjLzqZ3WnPPfec+vfvr9DQUIWHh1/WnPHjx8vlclVbUlNTnQ16gSvJbYzR7NmzFR0drZYtWyo5OVlffvmls0FrcPz4cY0ZM0ZhYWEKDw/XxIkTVVpaWuecQYMGXfSaT5kyxdGcixYt0s0336yQkBD169dPmzdvrnN8U7ksVn1yL1269KLXNSQkJIBpq3z44Ye67777FBMTI5fLVev38J5vw4YN6t27t9xut7p27VrtaPJAqW/uDRs2XPR6u1yuWr+K0SmNdem3a77sdu/ercrKSr366qvauXOnFixYoMzMTD399NN1zpsxY4bee+89LV++XBs3btShQ4f0wAMPBCh1lfLycj344IOaOnVqvealpqbq8OHD/uWPf/yjQwlrdiW5586dq5dfflmZmZnatGmTbrjhBqWkpOjMmTMOJr3YmDFjtHPnTq1du9b/rT6TJ0++5LxJkyZVe83nzp3rWMZ33nlHaWlpmjNnjrZt26aePXsqJSVFR44cqXF8U7ksVn1zS1VfY3X+67p///4AJq5y8uRJ9ezZU4sWLbqs8Xv37tWwYcM0ePBg5ebmavr06XrkkUf0/vvvO5y0uvrmPicvL6/aa96uXTuHEtas0S79Ziw0d+5cExcXV+v9JSUlpkWLFmb58uX+dbt27TKSTHZ2diAiVrNkyRLj8Xgua+y4cePM8OHDHc1zuS43d2VlpYmKijIvvPCCf11JSYlxu93mj3/8o4MJq/viiy+MJLNlyxb/un/84x/G5XKZwsLCWucNHDjQPPHEEwFIWKVv375m2rRp/tsVFRUmJibGZGRk1Dj+xz/+sRk2bFi1df369TOPPvqoozkvVN/c9fm9DxRJZuXKlXWOeeqpp8xtt91Wbd2oUaNMSkqKg8nqdjm5169fbySZb775JiCZLteRI0eMJLNx48ZaxzTE7/g1v2VXE6/XW+e3Zefk5Ojs2bNKTk72r4uPj1fHjh2VnZ0diIhXZcOGDWrXrp26d++uqVOn6tixY40dqU579+5VUVFRtdfb4/GoX79+AX29s7OzFR4erj59+vjXJScnKygoSJs2bapz7ptvvqm2bdvq9ttvV3p6uk6dOuVIxvLycuXk5FR7rYKCgpScnFzra5WdnV1tvFR1WaxAvrZXkluSSktL1alTJ8XGxmr48OHauXNnIOJelabwel+NhIQERUdH64c//KE+/vjjxo5z2Zd+u9rXPKDXswuE/Px8LVy4UPPmzat1TFFRkYKDgy/6vKmuSwk1FampqXrggQcUFxenPXv26Omnn9bQoUOVnZ2tZs2aNXa8Gp17Tetz6Sancly4y6Z58+Zq06ZNnTkefvhhderUSTExMfrss8/0i1/8Qnl5eVqxYkWDZ/z6669VUVFR42u1e/fuGufU97JYTriS3N27d9frr7+uO+64Q16vV/PmzVP//v21c+fOS35hfGOq7fX2+Xw6ffq0/6sQm5ro6GhlZmaqT58+Kisr02uvvaZBgwZp06ZN6t27d6NkcurSbzVpslt2s2bNqvHD1POXC/8jKiwsVGpqqh588EFNmjTpmsldH6NHj9b999+vHj16aMSIEVq9erW2bNmiDRs2NOncTnI6++TJk5WSkqIePXpozJgxeuONN7Ry5Urt2bOnAX+K609SUpLGjh2rhIQEDRw4UCtWrFBERIReffXVxo5mpe7du+vRRx9VYmKi+vfvr9dff139+/fXggULGi3TuUu/vf32244/V5Pdsps5c6b/+zNr07lzZ//fDx06pMGDB6t///6X/NLoqKgolZeXq6SkpNrWXUNcSqi+ua9W586d1bZtW+Xn52vIkCFX/DhO5j73mhYXFys6Otq/vri4WAkJCVf0mOe73OxRUVEXHSzx7bff6vjx4/X6996vXz9JVXsRunTpUu+8dWnbtq2aNWtWr8tcNYXLYl1J7gu1aNFCvXr1Un5+vhMRG0xtr3dYWFiT3aqrTd++fRv8GqOXK9CXfmuyZRcREaGIiIjLGltYWKjBgwcrMTFRS5YsueQF/xITE9WiRQtlZWVp5MiRkqqOUCooKLjqSwnVJ3dDOHjwoI4dO1atRK6Ek7nj4uIUFRWlrKwsf7n5fD5t2rSp3kei1uRysyclJamkpEQ5OTlKTEyUJK1bt06VlZX+Arscubm5knTVr3lNgoODlZiYqKysLI0YMUJS1a6erKwsPfbYYzXOOXdZrOnTp/vXBfqyWFeS+0IVFRXasWNHk//S96SkpIsOe79WL0OWm5vryO9xXYwxevzxx7Vy5Upt2LChXpd+u6rf8Ss9gqapOHjwoOnatasZMmSIOXjwoDl8+LB/OX9M9+7dzaZNm/zrpkyZYjp27GjWrVtntm7dapKSkkxSUlJAs+/fv99s377dPPPMM6ZVq1Zm+/btZvv27ebEiRP+Md27dzcrVqwwxhhz4sQJ8/Of/9xkZ2ebvXv3mg8++MD07t3bdOvWzZw5c6bJ5jbGmF//+tcmPDzcvPvuu+azzz4zw4cPN3Fxceb06dMBy22MMampqaZXr15m06ZN5qOPPjLdunUzDz30kP/+C39X8vPzzbPPPmu2bt1q9u7da959913TuXNnM2DAAMcyvv3228btdpulS5eaL774wkyePNmEh4eboqIiY4wxP/nJT8ysWbP84z/++GPTvHlzM2/ePLNr1y4zZ84c06JFC7Njxw7HMjZE7meeeca8//77Zs+ePSYnJ8eMHj3ahISEmJ07dwY094kTJ/y/w5LMiy++aLZv3272799vjDFm1qxZ5ic/+Yl//FdffWVCQ0PNk08+aXbt2mUWLVpkmjVrZtasWdOkcy9YsMCsWrXKfPnll2bHjh3miSeeMEFBQeaDDz4IaO6pU6caj8djNmzYUO39+tSpU/4xTvyOX/Nlt2TJEiOpxuWcvXv3Gklm/fr1/nWnT582P/3pT82NN95oQkNDzY9+9KNqBRkI48aNqzH3+TklmSVLlhhjjDl16pS55557TEREhGnRooXp1KmTmTRpkv/NpKnmNqbq9IN///d/N5GRkcbtdpshQ4aYvLy8gOY2xphjx46Zhx56yLRq1cqEhYWZCRMmVCvpC39XCgoKzIABA0ybNm2M2+02Xbt2NU8++aTxer2O5ly4cKHp2LGjCQ4ONn379jWffvqp/76BAweacePGVRv/pz/9yfzLv/yLCQ4ONrfddpv529/+5mi+2tQn9/Tp0/1jIyMjzb333mu2bdsW8MznDsm/cDmXddy4cWbgwIEXzUlISDDBwcGmc+fO1X7Xm2ru559/3nTp0sWEhISYNm3amEGDBpl169YFPHdt79fnv4ZO/I5ziR8AgPWa7NGYAAA0FMoOAGA9yg4AYD3KDgBgPcoOAGA9yg4AYD3KDgBgPcoOAGA9yg4AYD3KDgBgPcoOAGC9/w+KZc1X3N6NiAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "utils.draw_from_tensor(test_dataset[0][:,0], ax)\n",
    "ax.set_xlim(-2, 2)\n",
    "ax.set_ylim(2, -2)\n",
    "ax.set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks much better size wise after trimming the empty frames at the start and the end of the video\n",
    "\n",
    "Compare normalized pose to unnormalized on a random frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([112])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset[10][:,0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "utils.draw_from_tensor(training_dataset.__getitem__(1)[:,0], ax)\n",
    "ax.set_xlim(-2, 2)\n",
    "ax.set_ylim(2, -2)\n",
    "ax.set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "utils.draw_from_tensor(utils.unnormalize_mean_std(training_dataset[3][:,0]), ax)\n",
    "ax.set_xlim(-2, 2)\n",
    "ax.set_ylim(2, -2)\n",
    "ax.set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD STUFF\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} \n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "test_loader  = DataLoader(dataset=test_dataset,  batch_size=batch_size, shuffle=False,  **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_loader_sign = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "#data_iter = iter(train_loader)\n",
    "#print(data_iter)\n",
    "#type(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: numpy 1.26.4\n",
      "Uninstalling numpy-1.26.4:\n",
      "  Would remove:\n",
      "    /conda/envs/vqvae-anya/bin/f2py\n",
      "    /conda/envs/vqvae-anya/lib/python3.9/site-packages/numpy-1.26.4.dist-info/*\n",
      "    /conda/envs/vqvae-anya/lib/python3.9/site-packages/numpy.libs/libgfortran-040039e1.so.5.0.0\n",
      "    /conda/envs/vqvae-anya/lib/python3.9/site-packages/numpy.libs/libopenblas64_p-r0-0cf96a72.3.23.dev.so\n",
      "    /conda/envs/vqvae-anya/lib/python3.9/site-packages/numpy.libs/libquadmath-96973f99.so.0.0.0\n",
      "    /conda/envs/vqvae-anya/lib/python3.9/site-packages/numpy/*\n",
      "Proceed (Y/n)?   Successfully uninstalled numpy-1.26.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0myes: standard output: Broken pipe\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting numpy==1.23.2\n",
      "  Downloading numpy-1.23.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
      "Downloading numpy-1.23.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m103.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "Successfully installed numpy-1.23.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#!y| pip uninstall numpy\n",
    "#!pip install numpy==1.23.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.17.0-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting click!=8.0.0,>=7.1 (from wandb)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: platformdirs in /conda/envs/vqvae-anya/lib/python3.9/site-packages (from wandb) (4.2.2)\n",
      "Collecting protobuf!=4.21.0,<5,>=3.15.0 (from wandb)\n",
      "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /conda/envs/vqvae-anya/lib/python3.9/site-packages (from wandb) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /conda/envs/vqvae-anya/lib/python3.9/site-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /conda/envs/vqvae-anya/lib/python3.9/site-packages (from wandb) (2.31.0)\n",
      "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-2.2.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting setproctitle (from wandb)\n",
      "  Downloading setproctitle-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: setuptools in /conda/envs/vqvae-anya/lib/python3.9/site-packages (from wandb) (69.5.1)\n",
      "Requirement already satisfied: typing-extensions in /conda/envs/vqvae-anya/lib/python3.9/site-packages (from wandb) (4.11.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /conda/envs/vqvae-anya/lib/python3.9/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /conda/envs/vqvae-anya/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /conda/envs/vqvae-anya/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /conda/envs/vqvae-anya/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /conda/envs/vqvae-anya/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Downloading wandb-0.17.0-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m143.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m151.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m156.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentry_sdk-2.2.0-py2.py3-none-any.whl (281 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.1/281.1 kB\u001b[0m \u001b[31m152.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading setproctitle-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m133.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: smmap, setproctitle, sentry-sdk, protobuf, docker-pycreds, click, gitdb, gitpython, wandb\n",
      "Successfully installed click-8.1.7 docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 protobuf-4.25.3 sentry-sdk-2.2.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.17.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    }
   ],
   "source": [
    "!wandb login --relogin d1253183cef81ffd1fb36585541869e753355bd6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manna-klezovich24\u001b[0m (\u001b[33mannaklezovich1997\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/project/wandb/run-20240705_114302-dejal7eu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/annaklezovich1997/vqvae_on_ssl/runs/dejal7eu' target=\"_blank\">restart-fromzip</a></strong> to <a href='https://wandb.ai/annaklezovich1997/vqvae_on_ssl' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/annaklezovich1997/vqvae_on_ssl' target=\"_blank\">https://wandb.ai/annaklezovich1997/vqvae_on_ssl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/annaklezovich1997/vqvae_on_ssl/runs/dejal7eu' target=\"_blank\">https://wandb.ai/annaklezovich1997/vqvae_on_ssl/runs/dejal7eu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training VQ-VAE...\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(16.5492, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.8473109006881714\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.01726767048239708\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 3.937825918197632\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.02983541414141655\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.9096412658691406\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.03572901338338852\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.269587755203247\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.04091662913560867\n",
      "After scaling - encoder.proj.weight: grad norm 4.472853660583496\n",
      "After scaling - encoder.proj.bias: grad norm 0.0716012492775917\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.034325264394283295\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.1463557779788971\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 0.4174172878265381\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.12599481642246246\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 0.40542277693748474\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.1573837697505951\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 1.0466336011886597\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.387546181678772\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 0.5273382067680359\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.3482806384563446\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.0261827539652586\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0002447423757985234\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.05581255257129669\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00042287056567147374\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.027066193521022797\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0005064031574875116\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.03216787427663803\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.000579929503146559\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.06339573115110397\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0010148361325263977\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.0004865072260145098\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0020743655040860176\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.005916240159422159\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0017857804195955396\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.00574623653665185\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0022306698374450207\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.014834403060376644\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0054928637109696865\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.007474198006093502\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.004936336074024439\n",
      "epoch: 1 ( 1 ) recon_loss: 16.1344051361084  perplexity:  13.931652069091797  commit_loss:  0.09536366909742355 \n",
      "\t codebook loss:  0.3814546763896942  total_loss:  16.549236297607422 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(18.3798, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.2683265209198\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.040698084980249405\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 4.590198516845703\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.06675565987825394\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.357867956161499\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.0780671015381813\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 3.020404815673828\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.08874896168708801\n",
      "After scaling - encoder.proj.weight: grad norm 5.899179935455322\n",
      "After scaling - encoder.proj.bias: grad norm 0.14945261180400848\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.7476385831832886\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.1478576809167862\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.0936832427978516\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.12466850876808167\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.2883481979370117\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.14522621035575867\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 5.898299694061279\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.3248522877693176\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 3.1215453147888184\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.30669382214546204\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01984032429754734\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003559730830602348\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.04014899581670761\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0005838903016410768\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.02062351442873478\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0006828279001638293\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02641851082444191\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0007762586465105414\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05159823223948479\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0013072140282019973\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.00653935456648469\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.001293263747356832\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.01831277459859848\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0010904354276135564\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.020015446469187737\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.001270247041247785\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.051590535789728165\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0028413787949830294\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.02730315551161766\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.002682552207261324\n",
      "Are there any dead codes on this epoch?  0\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(16.2409, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.7252368927001953\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.020122280344367027\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 5.139950275421143\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.03399525210261345\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.3061039447784424\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.03228411823511124\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.5352768898010254\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.03190604969859123\n",
      "After scaling - encoder.proj.weight: grad norm 5.313270568847656\n",
      "After scaling - encoder.proj.bias: grad norm 0.055866096168756485\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.3931890726089478\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.08486602455377579\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.9875435829162598\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.07142646610736847\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.751206874847412\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.08093887567520142\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 9.49925422668457\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.19682207703590393\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 6.0010528564453125\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.20722100138664246\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01788853295147419\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00013208322343416512\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.03373878076672554\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00022314580564852804\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.015137331560254097\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0002119138662237674\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.016641629859805107\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00020943221170455217\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.03487646207213402\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0003667066339403391\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.009144932962954044\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005570630310103297\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.026174349710345268\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00046884536277502775\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.024623028934001923\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0005312850698828697\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.06235337257385254\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0012919456930831075\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.039391081780195236\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0013602045364677906\n",
      "epoch: 2 ( 1 ) recon_loss: 12.959810256958008  perplexity:  10.474032402038574  commit_loss:  0.7587777972221375 \n",
      "\t codebook loss:  3.03511118888855  total_loss:  16.24091911315918 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(22.7242, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 14.347514152526855\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.33043134212493896\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 24.18254280090332\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.43748918175697327\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 15.665550231933594\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.431509792804718\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 19.895235061645508\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.4115263521671295\n",
      "After scaling - encoder.proj.weight: grad norm 41.97404861450195\n",
      "After scaling - encoder.proj.bias: grad norm 0.6392419338226318\n",
      "After scaling - decoder.in_proj.weight: grad norm 6.063298225402832\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.3205461800098419\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 15.609400749206543\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.23992618918418884\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 14.25207805633545\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.2174426019191742\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 27.755035400390625\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.32809481024742126\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 15.255796432495117\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.21448327600955963\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.020968768745660782\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00048292253632098436\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.03534257784485817\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0006393866497091949\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.02289506606757641\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0006306478753685951\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02907671220600605\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0006014421815052629\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.06134470924735069\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0009342465782538056\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.00886145792901516\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00046847545308992267\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.02281300537288189\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00035065002157352865\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.02082928828895092\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00031779048731550574\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04056374356150627\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0004795077256858349\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.02229621261358261\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0003134654543828219\n",
      "Are there any dead codes on this epoch?  486\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(19.5023, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 18.249370574951172\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.3651745915412903\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 29.23023796081543\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.47158634662628174\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 17.67582893371582\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.4517696797847748\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 21.740074157714844\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.43257662653923035\n",
      "After scaling - encoder.proj.weight: grad norm 46.06964874267578\n",
      "After scaling - encoder.proj.bias: grad norm 0.6821542382240295\n",
      "After scaling - decoder.in_proj.weight: grad norm 6.808406829833984\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.3455857038497925\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 17.47467041015625\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.262084037065506\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 15.93967056274414\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.23997677862644196\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 28.52983283996582\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.340719074010849\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 16.773988723754883\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.2339819222688675\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.023967426270246506\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004795943677891046\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.038388919085264206\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0006193480221554637\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.023214178159832954\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0005933222128078341\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.028551869094371796\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0005681153852492571\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.06050460785627365\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0008958929101936519\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.008941677398979664\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00045386768761090934\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.02294998988509178\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00034420256270095706\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.020934030413627625\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003151684650219977\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.037469055503606796\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0004474762245081365\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.02202976681292057\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0003072952385991812\n",
      "epoch: 3 ( 1 ) recon_loss: 11.97754192352295  perplexity:  9.041422843933105  commit_loss:  1.7458819150924683 \n",
      "\t codebook loss:  6.983527660369873  total_loss:  19.50229835510254 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(16.7772, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 18.21114158630371\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.21441654860973358\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 28.534778594970703\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.27860215306282043\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 16.584436416625977\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.27268514037132263\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 19.27799415588379\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.27028024196624756\n",
      "After scaling - encoder.proj.weight: grad norm 40.35642623901367\n",
      "After scaling - encoder.proj.bias: grad norm 0.4401264488697052\n",
      "After scaling - decoder.in_proj.weight: grad norm 6.093508243560791\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.23544007539749146\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 15.968351364135742\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.18303251266479492\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 14.714583396911621\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.18049587309360504\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 27.195093154907227\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.2616339325904846\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 17.594505310058594\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.18585807085037231\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.025806959718465805\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00030384911224246025\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.04043655842542648\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003948063822463155\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.02350175939500332\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003864213649649173\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.027318792417645454\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00038301339372992516\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.0571889765560627\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.000623702013399452\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.00863509438931942\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00033364148112013936\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.02262870781123638\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.000259374879533425\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.020851995795965195\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0002557802072260529\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.038538090884685516\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0003707607393153012\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.02493312396109104\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00026337895542383194\n",
      "Are there any dead codes on this epoch?  470\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(11.3160, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 4.18278169631958\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.047306641936302185\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 6.570212364196777\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.06653320044279099\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 3.729827642440796\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.0659995898604393\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 4.406737804412842\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.0687365010380745\n",
      "After scaling - encoder.proj.weight: grad norm 9.399243354797363\n",
      "After scaling - encoder.proj.bias: grad norm 0.12560051679611206\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.9148359298706055\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.09161748737096786\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 5.171727657318115\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.0754832848906517\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 5.130702495574951\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.08504881709814072\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 12.106614112854004\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.15941619873046875\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 7.934797763824463\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.1343652307987213\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01974625512957573\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00022332719527184963\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.031016938388347626\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00031409275834448636\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.017607929185032845\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003115736471954733\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02080351486802101\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00032449420541524887\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04437234625220299\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005929403123445809\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.009039638563990593\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00043251170427538455\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.02441491186618805\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003563447098713368\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.024221237748861313\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00040150206768885255\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.05715341866016388\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007525787805207074\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.03745892643928528\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0006343171116895974\n",
      "epoch: 4 ( 1 ) recon_loss: 8.895214080810547  perplexity:  17.93915557861328  commit_loss:  0.5629382133483887 \n",
      "\t codebook loss:  2.2517528533935547  total_loss:  11.316020965576172 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(10.3543, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.449239730834961\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.032102201133966446\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 4.254882335662842\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.045707035809755325\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.533738374710083\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.05058605968952179\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 3.180616855621338\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.05449683219194412\n",
      "After scaling - encoder.proj.weight: grad norm 6.450360298156738\n",
      "After scaling - encoder.proj.bias: grad norm 0.08809827268123627\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.878389835357666\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.06289763003587723\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 5.035820484161377\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.051488444209098816\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 4.406529426574707\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.05181019753217697\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 10.87995433807373\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.12608495354652405\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 6.668776988983154\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.12025073915719986\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.014297550544142723\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00018739808001555502\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.024838073179125786\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.000266816932708025\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01479081530123949\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00029529843595810235\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.018566997721791267\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003181278007104993\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.03765427693724632\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005142777226865292\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.010965187102556229\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00036716778413392603\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.02939683385193348\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00030056617106311023\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.025723319500684738\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.000302444415865466\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.06351223587989807\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007360267918556929\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.0389292947947979\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0007019692566245794\n",
      "Are there any dead codes on this epoch?  457\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(9.0414, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 4.226138591766357\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.04188549518585205\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 7.093254566192627\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.06551428139209747\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 3.943399667739868\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.06438209861516953\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 4.26207971572876\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.05756364017724991\n",
      "After scaling - encoder.proj.weight: grad norm 8.871237754821777\n",
      "After scaling - encoder.proj.bias: grad norm 0.09416589885950089\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.786976933479309\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.07654795050621033\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 4.594961643218994\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.06336263567209244\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.8611040115356445\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.061151668429374695\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 8.11264705657959\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.13288207352161407\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 5.1374711990356445\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.13545745611190796\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.023922547698020935\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00023709767265245318\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.040152184665203094\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00037085116491653025\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.022322067990899086\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00036444226861931384\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.024125993251800537\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003258456417825073\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.050216663628816605\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005330369458533823\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01011538878083229\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00043330847984179854\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.026010308414697647\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00035867150290869176\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.02185622602701187\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00034615607000887394\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.045922573655843735\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.000752194260712713\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.029081247746944427\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0007667726022191346\n",
      "epoch: 5 ( 1 ) recon_loss: 7.731611251831055  perplexity:  20.865684509277344  commit_loss:  0.3050924241542816 \n",
      "\t codebook loss:  1.2203696966171265  total_loss:  9.04137134552002 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(9.2497, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.9654812812805176\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.02165662683546543\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 4.926961898803711\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.03598668426275253\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.6084916591644287\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.033187396824359894\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.831803798675537\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.025147918611764908\n",
      "After scaling - encoder.proj.weight: grad norm 6.167661190032959\n",
      "After scaling - encoder.proj.bias: grad norm 0.04170463606715202\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.5298454761505127\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.050903964787721634\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 4.020448207855225\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.043588221073150635\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.575000762939453\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.043225064873695374\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 8.868073463439941\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.10181926935911179\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 5.809826850891113\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.11042483150959015\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.019568761810660362\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00014290880062617362\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.032512277364730835\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0002374706818955019\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.017213042825460434\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00021899861167185009\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.018686644732952118\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00016594732005614787\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.040699463337659836\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0002752025902736932\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.010095219127833843\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00033590756356716156\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.026530331000685692\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0002876320795621723\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.02359089069068432\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00028523567016236484\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.05851907655596733\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006718899239785969\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.03833816573023796\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0007286767940968275\n",
      "Are there any dead codes on this epoch?  441\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(8.5227, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 3.67598032951355\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.0440773144364357\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 5.866654872894287\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.058171894401311874\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 3.172147035598755\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.050575852394104004\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 3.7541897296905518\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.047540709376335144\n",
      "After scaling - encoder.proj.weight: grad norm 8.16119384765625\n",
      "After scaling - encoder.proj.bias: grad norm 0.08005565404891968\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.4293321371078491\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.051298510283231735\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.7969162464141846\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.04202808067202568\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.299297332763672\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.03950459882616997\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 7.840963363647461\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.08386972546577454\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 5.258134841918945\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.08472689241170883\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.023010751232504845\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.000275913393124938\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.03672384098172188\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00036414203350432217\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019856875762343407\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00031659266096539795\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02350032329559326\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0002975933894049376\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05108711123466492\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005011291359551251\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.008947274647653103\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.000321116327540949\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.023767782375216484\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00026308567612431943\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.020652806386351585\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0002472892520017922\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04908253997564316\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005250042886473238\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.0329146534204483\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005303699290379882\n",
      "epoch: 6 ( 1 ) recon_loss: 6.744803428649902  perplexity:  31.943044662475586  commit_loss:  0.41470056772232056 \n",
      "\t codebook loss:  1.6588022708892822  total_loss:  8.522688865661621 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(9.3307, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 5.787548542022705\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03585429862141609\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 9.187201499938965\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.058503277599811554\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 5.067040920257568\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.05198359861969948\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 5.723280906677246\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.038689710199832916\n",
      "After scaling - encoder.proj.weight: grad norm 11.94653606414795\n",
      "After scaling - encoder.proj.bias: grad norm 0.0649985820055008\n",
      "After scaling - decoder.in_proj.weight: grad norm 2.07417893409729\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.055910203605890274\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 5.341382026672363\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.04563167318701744\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 4.86186408996582\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.04229248687624931\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 10.590335845947266\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.07432084530591965\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 7.235363483428955\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.07421156764030457\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.024904299527406693\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00015428401820827276\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.03953328728675842\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00025174443726427853\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.021803896874189377\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00022368974168784916\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.024627752602100372\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00016648502787575126\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05140693485736847\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0002796942717395723\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.008925364352762699\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00024058623239398003\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.02298440970480442\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00019635686476249248\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.020921003073453903\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0001819880708353594\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.045571088790893555\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0003198087797500193\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.03113436885178089\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0003193385200574994\n",
      "Are there any dead codes on this epoch?  427\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(8.5386, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.617706537246704\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03119068220257759\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 4.213876724243164\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.04787769168615341\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.1978814601898193\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.03840889409184456\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.669044256210327\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.02025357075035572\n",
      "After scaling - encoder.proj.weight: grad norm 6.126925945281982\n",
      "After scaling - encoder.proj.bias: grad norm 0.035172849893569946\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.1135832071304321\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.04303683713078499\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.074460029602051\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.03625024855136871\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.6446475982666016\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.031573981046676636\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 6.47946310043335\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.06823796033859253\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 4.273263931274414\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.07636954635381699\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02122858725488186\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00025294438819400966\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.0341729074716568\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003882695746142417\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.017823968082666397\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00031148127163760364\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.021644916385412216\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00016424863133579493\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04968700185418129\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0002852382021956146\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.009030729532241821\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0003490121162030846\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.024932680651545525\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00029397549224086106\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.021447069942951202\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0002560527645982802\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.0525459423661232\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005533834919333458\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.034654516726732254\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0006193274748511612\n",
      "epoch: 7 ( 1 ) recon_loss: 6.052894115447998  perplexity:  36.5571403503418  commit_loss:  0.5804593563079834 \n",
      "\t codebook loss:  2.3218374252319336  total_loss:  8.538599014282227 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(9.9835, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 4.777607440948486\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03381384164094925\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 7.731192588806152\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.049162596464157104\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 4.0905890464782715\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.04852993041276932\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 4.5306525230407715\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.044416800141334534\n",
      "After scaling - encoder.proj.weight: grad norm 9.998979568481445\n",
      "After scaling - encoder.proj.bias: grad norm 0.07112859189510345\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.7853937149047852\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.04943228140473366\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 4.2539448738098145\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.04035895690321922\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.4778714179992676\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.03967179358005524\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 7.741558074951172\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.08887559175491333\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 5.423463821411133\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.10003156214952469\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02581179514527321\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00018268472922500223\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.041769012808799744\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0002656088618095964\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.022100066766142845\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0002621907915454358\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.024477580562233925\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0002399689401499927\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05402110144495964\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.00038428371772170067\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.009645877406001091\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0002670658868737519\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.022982623428106308\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0002180457959184423\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.018789762631058693\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00021433326764963567\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04182501509785652\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.00048016474465839565\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.02930113859474659\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005404366529546678\n",
      "Are there any dead codes on this epoch?  411\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(8.4371, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 4.397669315338135\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.041560638695955276\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 6.732699871063232\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.06051408872008324\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 3.7951738834381104\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.062116969376802444\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 4.486292839050293\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.06648865342140198\n",
      "After scaling - encoder.proj.weight: grad norm 9.40245246887207\n",
      "After scaling - encoder.proj.bias: grad norm 0.11119777709245682\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.457077980041504\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.06374727934598923\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.5155446529388428\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.04987171292304993\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.1146795749664307\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.05036276578903198\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 6.897889137268066\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.10433732718229294\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 4.3724212646484375\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.1110057681798935\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.026349617168307304\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0002490198239684105\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.040340472012758255\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00036258366890251637\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.022739630192518234\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00037218770012259483\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.026880623772740364\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.000398381584091112\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.056336890906095505\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006662662490271032\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.008730407804250717\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00038195602246560156\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.021064169704914093\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00029881749651394784\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.01866229809820652\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003017597191501409\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.0413302406668663\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.000625160348135978\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.026198336854577065\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0006651157746091485\n",
      "epoch: 8 ( 1 ) recon_loss: 5.5187907218933105  perplexity:  38.919498443603516  commit_loss:  0.6821363568305969 \n",
      "\t codebook loss:  2.7285454273223877  total_loss:  8.437088012695312 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(8.2545, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 4.049106121063232\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.020789042115211487\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 5.940299987792969\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.02838394045829773\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 3.7364842891693115\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.026867344975471497\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 4.682456970214844\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.031179092824459076\n",
      "After scaling - encoder.proj.weight: grad norm 10.68030071258545\n",
      "After scaling - encoder.proj.bias: grad norm 0.053831521421670914\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.2986969947814941\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03688690438866615\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.259582996368408\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.030763251706957817\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.8803579807281494\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.031516142189502716\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 6.851652145385742\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.08018721640110016\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 4.673596382141113\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.08987264335155487\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.023733947426080704\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00012185554078314453\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.03481923043727875\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00016637324006296694\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.021901505067944527\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00015748367877677083\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02744635008275509\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00018275711045134813\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.06260287016630173\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0003155349113512784\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.007612348068505526\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0002162135933758691\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.01910613477230072\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00018031965009868145\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.01688329689204693\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0001847327221184969\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.040161147713661194\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0004700195277109742\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.027394412085413933\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005267909727990627\n",
      "Are there any dead codes on this epoch?  401\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(8.1251, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 4.1741766929626465\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.027241436764597893\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 6.549142837524414\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.04078706353902817\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 4.015134334564209\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.03997953608632088\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 5.063326835632324\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.03755883872509003\n",
      "After scaling - encoder.proj.weight: grad norm 11.815390586853027\n",
      "After scaling - encoder.proj.bias: grad norm 0.06539406627416611\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.6063448190689087\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.04512922465801239\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.9869823455810547\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.036990001797676086\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.629434108734131\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.03571437671780586\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 7.152914047241211\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.07388921082019806\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 4.737998008728027\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.07412248849868774\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02240930125117302\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0001462471846025437\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.03515944257378578\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00021896765974815935\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.021555475890636444\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00021463238226715475\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02718275599181652\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00020163673616480082\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.06343159079551697\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.00035107170697301626\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.008623752743005753\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00024227879475802183\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.021404340863227844\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00019858291489072144\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.01948482170701027\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00019173465261701494\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03840082138776779\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.00039667842793278396\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.02543620951473713\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0003979307657573372\n",
      "epoch: 9 ( 1 ) recon_loss: 5.017202377319336  perplexity:  46.23525619506836  commit_loss:  0.7270739078521729 \n",
      "\t codebook loss:  2.9082956314086914  total_loss:  8.12513542175293 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(6.5385, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 3.765458106994629\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03738655522465706\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 6.167129993438721\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.05426071584224701\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 3.315387010574341\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.05301036313176155\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 3.710918426513672\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.05440125614404678\n",
      "After scaling - encoder.proj.weight: grad norm 8.372528076171875\n",
      "After scaling - encoder.proj.bias: grad norm 0.09333713352680206\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.5208063125610352\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.055294912308454514\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.738870859146118\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.04310586303472519\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.24104905128479\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.039967313408851624\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 6.740793704986572\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.07351968437433243\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 4.533323287963867\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.06596655398607254\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.0243112500756979\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0002413820184301585\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.03981737047433853\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00035032813320867717\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.02140541933476925\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003422553709242493\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.023959122598171234\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00035123550333082676\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05405627191066742\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006026205373927951\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.009818912483751774\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0003570052795112133\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.024139592424035072\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.000278308114502579\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.02092546410858631\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0002580443979240954\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04352116584777832\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0004746714257635176\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.029268885031342506\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00042590551311150193\n",
      "Are there any dead codes on this epoch?  397\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(7.0447, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.456973075866699\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.01500764861702919\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 3.692923069000244\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.022624921053647995\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.0938332080841064\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02302166074514389\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.6082451343536377\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.024236945435404778\n",
      "After scaling - encoder.proj.weight: grad norm 5.412923812866211\n",
      "After scaling - encoder.proj.bias: grad norm 0.04343894496560097\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.7973653674125671\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.02712668664753437\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.0561351776123047\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.022635646164417267\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 1.8900116682052612\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.021575527265667915\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 4.670202255249023\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.0515228770673275\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 3.2729272842407227\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.0672793909907341\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02443179488182068\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00014923395065125078\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.036721907556056976\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00022497904137708247\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.02082078345119953\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00022892416745889932\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.025936022400856018\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00024100877635646611\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05382535234093666\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0004319507861509919\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.007928889244794846\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00026974399224855006\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.020445918664336205\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00022508569236379117\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.01879400946199894\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00021454400848597288\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04643983766436577\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005123362643644214\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.03254552558064461\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0006690167938359082\n",
      "epoch: 10 ( 1 ) recon_loss: 4.393520355224609  perplexity:  49.4118537902832  commit_loss:  0.6206833720207214 \n",
      "\t codebook loss:  2.4827334880828857  total_loss:  7.044707298278809 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(5.6560, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 4.027315139770508\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.024929892271757126\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 6.378488063812256\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.04318825900554657\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 3.7877750396728516\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.03927210345864296\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 4.611326694488525\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.025874540209770203\n",
      "After scaling - encoder.proj.weight: grad norm 10.386512756347656\n",
      "After scaling - encoder.proj.bias: grad norm 0.042904358357191086\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.6307177543640137\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.0405135378241539\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.811802387237549\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.03503330424427986\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.014779806137085\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.03197835758328438\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 5.659807205200195\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.05975724756717682\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 3.6266133785247803\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.07387832552194595\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.024424627423286438\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00015119335148483515\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.03868388757109642\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0002619256265461445\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.022971879690885544\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0002381751692155376\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02796650491654873\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00015692241140641272\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.06299152225255966\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0002602038439363241\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.009889882057905197\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00024570414097979665\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.02311759628355503\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00021246795949991792\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.018283860757946968\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0001939404901349917\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03432526811957359\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.00036241227644495666\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.021994473412632942\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004480529751162976\n",
      "Are there any dead codes on this epoch?  382\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(5.9647, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.5262959003448486\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.04193419963121414\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 3.8503196239471436\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.06096499785780907\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.2947354316711426\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.06004805490374565\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.9594168663024902\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.06780774146318436\n",
      "After scaling - encoder.proj.weight: grad norm 6.678757667541504\n",
      "After scaling - encoder.proj.bias: grad norm 0.12107739597558975\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.9933704733848572\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.06493110209703445\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.5839169025421143\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.050397783517837524\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.549137592315674\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.04797878488898277\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 5.3929548263549805\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.08942548930644989\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 3.2176833152770996\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.09112537652254105\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02186555601656437\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003629482234828174\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.03332522511482239\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.000527663272805512\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019861359149217606\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0005197270074859262\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02561429888010025\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0005868885782547295\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05780588462948799\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0010479472111910582\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.00859780516475439\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005619907169602811\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.02236427552998066\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004362021281849593\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.022063255310058594\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0004152652691118419\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04667702317237854\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007739941356703639\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.027849644422531128\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0007887069950811565\n",
      "epoch: 11 ( 1 ) recon_loss: 4.0980706214904785  perplexity:  54.069053649902344  commit_loss:  0.43729233741760254 \n",
      "\t codebook loss:  1.7491693496704102  total_loss:  5.9646782875061035 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(5.4187, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 3.6375460624694824\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.030254477635025978\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 5.743119716644287\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.040105175226926804\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 3.332632541656494\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.04139889404177666\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 3.874760389328003\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.04619917273521423\n",
      "After scaling - encoder.proj.weight: grad norm 7.4635725021362305\n",
      "After scaling - encoder.proj.bias: grad norm 0.08169814944267273\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.3701938390731812\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.04162576422095299\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.5021328926086426\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.03144417703151703\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.146747350692749\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.029495984315872192\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 5.862299919128418\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.05653875693678856\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 4.02433967590332\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.057006701827049255\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.025551369413733482\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00021251778525765985\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.040341634303331375\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00028171244775876403\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.02340954914689064\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00029079997329972684\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.027217641472816467\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003245187981519848\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05242668464779854\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005738756735809147\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.009624709375202656\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00029239358264021575\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.024600178003311157\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00022087464458309114\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.02210382930934429\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0002071898925350979\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04117879644036293\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0003971475234720856\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.02826833724975586\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00040043456829153\n",
      "Are there any dead codes on this epoch?  373\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(5.0195, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.722679615020752\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.029630426317453384\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 4.2268877029418945\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.04182194918394089\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.3229618072509766\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.03882988542318344\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.771770715713501\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.03834857419133186\n",
      "After scaling - encoder.proj.weight: grad norm 6.246715545654297\n",
      "After scaling - encoder.proj.bias: grad norm 0.06611701101064682\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.124493956565857\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03626292198896408\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.778958797454834\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.028158878907561302\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.3629181385040283\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.025942949578166008\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 4.4413652420043945\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04482313618063927\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 3.0121347904205322\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.04403049871325493\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02477562613785267\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00026962862466461957\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.03846350312232971\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00038056806079111993\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.021138304844498634\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00035334110725671053\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02522234059870243\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00034896130091510713\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05684336647391319\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006016463739797473\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.010232580825686455\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00032998246024362743\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.025287749245762825\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00025623792316764593\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.021501895040273666\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0002360735961701721\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04041518270969391\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.00040787801844999194\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.027409585192799568\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004006652452517301\n",
      "epoch: 12 ( 1 ) recon_loss: 3.8387980461120605  perplexity:  56.21413040161133  commit_loss:  0.27677005529403687 \n",
      "\t codebook loss:  1.1070802211761475  total_loss:  5.019495964050293 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(5.3151, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 4.864201068878174\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.042010899633169174\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 7.387232780456543\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.05736797675490379\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 4.297774314880371\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.05174519121646881\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 5.117259502410889\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.04910477623343468\n",
      "After scaling - encoder.proj.weight: grad norm 12.4649658203125\n",
      "After scaling - encoder.proj.bias: grad norm 0.08677089214324951\n",
      "After scaling - decoder.in_proj.weight: grad norm 2.0590434074401855\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.0490470752120018\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 4.78636360168457\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.038219012320041656\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.6552767753601074\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.03411339595913887\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 6.112898349761963\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.0509721115231514\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 4.259831428527832\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.050956521183252335\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02514156885445118\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00021714151080232114\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.03818235173821449\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.000296517537208274\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.022213881835341454\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0002674550923984498\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.026449551805853844\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0002538076078053564\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.06442759186029434\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.00044849221012555063\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.010642565786838531\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0002535093517508358\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.024739250540733337\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0001975423947442323\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.018893009051680565\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00017632172966841608\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03159570321440697\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.00026345925289206207\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.022017767652869225\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00026337869348935783\n",
      "Are there any dead codes on this epoch?  369\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(4.4149, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.45554256439209\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.031170843169093132\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 3.8044731616973877\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.04060333967208862\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.0666329860687256\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.03790293633937836\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.4348104000091553\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.042948223650455475\n",
      "After scaling - encoder.proj.weight: grad norm 4.927441596984863\n",
      "After scaling - encoder.proj.bias: grad norm 0.07770860940217972\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.8917780518531799\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.0408003143966198\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.2684412002563477\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.030829735100269318\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.1665027141571045\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.03076964244246483\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 4.5060858726501465\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.05660878121852875\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.8965437412261963\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.06123873218894005\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.025282010436058044\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003209318092558533\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.039170462638139725\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00041804779903031886\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.021277843043208122\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00039024476427584887\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.025068556889891624\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004421904741320759\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05073243007063866\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0008000798407010734\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.009181655012071133\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00042007584124803543\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.02335563860833645\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003174197918269783\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.022306086495518684\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00031680104439146817\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04639419540762901\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005828381981700659\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.029822515323758125\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0006305076531134546\n",
      "epoch: 13 ( 1 ) recon_loss: 3.589132785797119  perplexity:  58.95556640625  commit_loss:  0.19367071986198425 \n",
      "\t codebook loss:  0.774682879447937  total_loss:  4.414876461029053 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(4.5762, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.8262399435043335\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.0219253096729517\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.938405990600586\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.03244805708527565\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.651572585105896\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.03129611536860466\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.9432851076126099\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.03359387442469597\n",
      "After scaling - encoder.proj.weight: grad norm 4.547674655914307\n",
      "After scaling - encoder.proj.bias: grad norm 0.057849954813718796\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.8574632406234741\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.033424582332372665\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.169163227081299\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.026080451905727386\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 1.7800111770629883\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.025913063436746597\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.8559887409210205\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.05712536349892616\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.670987367630005\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.07145720720291138\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02186143584549427\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00026246209745295346\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.035174883902072906\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.000388427113648504\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019770538434386253\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00037463754415512085\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02326255291700363\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004021433705929667\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05443901941180229\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006925065536051989\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.010264466516673565\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004001168708782643\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.025966478511691093\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003122022026218474\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.021308045834302902\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00031019843299873173\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04615902528166771\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006838326808065176\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.03197368606925011\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0008553953375667334\n",
      "Are there any dead codes on this epoch?  365\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(4.0619, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.3281846046447754\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03314467519521713\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 3.539663314819336\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.04158220812678337\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.0488574504852295\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.04042857512831688\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.573683023452759\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.050232063978910446\n",
      "After scaling - encoder.proj.weight: grad norm 4.8503241539001465\n",
      "After scaling - encoder.proj.bias: grad norm 0.08464590460062027\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.8923153281211853\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03950444608926773\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.391782760620117\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.0303797647356987\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.307781934738159\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.029210468754172325\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 4.306174278259277\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.05904359370470047\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.7822067737579346\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.06987814605236053\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02448018454015255\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003485066117718816\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.03721852973103523\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00043722480768337846\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.021543141454458237\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0004250947094988078\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.027061527594923973\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0005281755584292114\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05099974945187569\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0008900270331650972\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.009382436983287334\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004153777554165572\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.025148900225758553\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003194343880750239\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.024265656247735023\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003071395622100681\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.045278169214725494\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006208262057043612\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.029254095628857613\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0007347483769990504\n",
      "epoch: 14 ( 1 ) recon_loss: 3.3963489532470703  perplexity:  63.78925704956055  commit_loss:  0.15617039799690247 \n",
      "\t codebook loss:  0.6246815919876099  total_loss:  4.061868667602539 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(4.3682, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.3588883876800537\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.016484133899211884\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.1466989517211914\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.024169981479644775\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.2735041379928589\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02157622016966343\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.6290218830108643\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.019268721342086792\n",
      "After scaling - encoder.proj.weight: grad norm 3.6913230419158936\n",
      "After scaling - encoder.proj.bias: grad norm 0.03370310366153717\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.7560168504714966\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.02380913868546486\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 1.957707166671753\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.019018137827515602\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 1.7610515356063843\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.017587579786777496\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.583543062210083\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.03685037046670914\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.3722567558288574\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.04755249246954918\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.019152792170643806\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0002323349326616153\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.03025655634701252\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003406627511139959\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01794935017824173\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003041051095351577\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.022960178554058075\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00027158219018019736\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05202719569206238\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0004750269581563771\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01065564714372158\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00033557688584551215\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.027592821046710014\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00026805035304278135\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.024821065366268158\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.000247887393925339\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.05050809308886528\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005193859688006341\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.03343566879630089\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0006702265818603337\n",
      "Are there any dead codes on this epoch?  359\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(3.8032, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.4839330911636353\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.02022305689752102\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.3360366821289062\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.026154713705182076\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.3395410776138306\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02581232599914074\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.7544190883636475\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.0327109768986702\n",
      "After scaling - encoder.proj.weight: grad norm 3.3886282444000244\n",
      "After scaling - encoder.proj.bias: grad norm 0.05216320604085922\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.6326457262039185\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.02601548284292221\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 1.648241400718689\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.019867368042469025\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 1.5547696352005005\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.01957584172487259\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.204651355743408\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.038937702775001526\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.1550276279449463\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.04629182815551758\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.022251814603805542\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003032480017282069\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.035029247403144836\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00039219410973601043\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.020086633041501045\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00038706001942045987\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02630779705941677\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004905063542537391\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05081302672624588\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007821956533007324\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.009486624039709568\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00039010634645819664\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.024715645238757133\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00029791437555104494\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.023314019665122032\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00029354289290495217\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04805426299571991\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005838771467097104\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.032314982265233994\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0006941533647477627\n",
      "epoch: 15 ( 1 ) recon_loss: 3.200908660888672  perplexity:  69.44795989990234  commit_loss:  0.14140325784683228 \n",
      "\t codebook loss:  0.5656130313873291  total_loss:  3.803217649459839 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(4.2146, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.546921730041504\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.016268113628029823\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 3.741786241531372\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.020693276077508926\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.2573368549346924\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.020693756639957428\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.806291103363037\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.026424532756209373\n",
      "After scaling - encoder.proj.weight: grad norm 5.973308086395264\n",
      "After scaling - encoder.proj.bias: grad norm 0.04598368704319\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.017189621925354\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.02497921511530876\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.5422818660736084\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.019264496862888336\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.15163254737854\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.0192860160022974\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.925034523010254\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.044294241815805435\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.428406000137329\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.05464817211031914\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02514692209661007\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00016062251233961433\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.03694436699151993\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00020431415759958327\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.02228771708905697\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00020431890152394772\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.027707792818546295\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.000260901462752372\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05897719785571098\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.000454017921583727\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.010043177753686905\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00024663121439516544\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.025101108476519585\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00019020716717932373\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.021244050934910774\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0001904196251416579\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.038753654807806015\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0004373372648842633\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.02397676184773445\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005395663320086896\n",
      "Are there any dead codes on this epoch?  344\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(3.6780, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.5535801649093628\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.01976466365158558\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.4044973850250244\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.02734414115548134\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.333428144454956\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.025290464982390404\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.6663010120391846\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.02906162291765213\n",
      "After scaling - encoder.proj.weight: grad norm 3.444422721862793\n",
      "After scaling - encoder.proj.bias: grad norm 0.04881449416279793\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.72640460729599\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.02808144874870777\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 1.9124746322631836\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.02242950163781643\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 1.7179185152053833\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.021864157170057297\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.2662222385406494\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.0501517578959465\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.1639347076416016\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.05994676798582077\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.022633850574493408\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00028794811805710196\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.03503072261810303\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00039837227086536586\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01942649483680725\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.000368452601833269\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.024276064708828926\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004233939980622381\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.050181224942207336\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.000711170316208154\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.010582868941128254\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004091139999218285\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.02786252833902836\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00032677172566764057\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.02502807043492794\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.000318535283440724\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.047585051506757736\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007306526531465352\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.03152600675821304\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0008733544964343309\n",
      "epoch: 16 ( 1 ) recon_loss: 3.0655603408813477  perplexity:  70.20895385742188  commit_loss:  0.14384935796260834 \n",
      "\t codebook loss:  0.5753974318504333  total_loss:  3.678023338317871 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(3.9066, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.1004031896591187\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.014628068543970585\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.6973932981491089\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.020430635660886765\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 0.9588731527328491\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.01940412074327469\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.247881293296814\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.022772209718823433\n",
      "After scaling - encoder.proj.weight: grad norm 2.6211819648742676\n",
      "After scaling - encoder.proj.bias: grad norm 0.03885233402252197\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.5716177225112915\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.02333446592092514\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 1.5546257495880127\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.018876411020755768\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 1.4518227577209473\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.018610650673508644\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.0339982509613037\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04069051146507263\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.0823798179626465\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.05150596424937248\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.019501158967614174\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0002592361415736377\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.03008091263473034\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003620682400651276\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01699298806488514\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00034387651248835027\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.022114740684628487\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004035652382299304\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04645213857293129\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006885344628244638\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.010130111128091812\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004135294584557414\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.02755081094801426\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00033452457864768803\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.02572895586490631\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003298147930763662\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.05376800149679184\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.000721110322047025\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.03690358251333237\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0009127799421548843\n",
      "Are there any dead codes on this epoch?  351\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(3.5709, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.0091590881347656\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.02324657142162323\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 3.0729877948760986\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.027730444446206093\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.8070886135101318\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02777293510735035\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.3151960372924805\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.03676990047097206\n",
      "After scaling - encoder.proj.weight: grad norm 4.511716842651367\n",
      "After scaling - encoder.proj.bias: grad norm 0.060563161969184875\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.8239634037017822\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.027992546558380127\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.0165138244628906\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.021425774320960045\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 1.8682763576507568\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.021094685420393944\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.192657232284546\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.039043184369802475\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.0927534103393555\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.05228148400783539\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.024866536259651184\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0002877132792491466\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.03803310543298721\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00034320828854106367\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.022365592420101166\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00034373419475741684\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.028654230758547783\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00045508594484999776\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.0558396615087986\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007495653117075562\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.010197855532169342\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00034645223058760166\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.02495756186544895\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0002651779795996845\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.023122888058423996\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00026108024758286774\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.0395142026245594\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.000483221432659775\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.025901148095726967\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.000647066452074796\n",
      "epoch: 17 ( 1 ) recon_loss: 2.945474863052368  perplexity:  72.38634490966797  commit_loss:  0.14696379005908966 \n",
      "\t codebook loss:  0.5878551602363586  total_loss:  3.5709452629089355 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(3.7931, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.7229795455932617\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.015576676465570927\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.684619188308716\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.022423239424824715\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.56850004196167\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.020557498559355736\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.9148015975952148\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.022756528109312057\n",
      "After scaling - encoder.proj.weight: grad norm 3.981623888015747\n",
      "After scaling - encoder.proj.bias: grad norm 0.03830756992101669\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.8811970353126526\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.023702414706349373\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.2343413829803467\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.018444040790200233\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 1.9461430311203003\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.016593148931860924\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.6413466930389404\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.03629766404628754\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.375558614730835\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.04362380877137184\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.022129105404019356\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00020005919213872403\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.034479930996894836\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0002879930834751576\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.02014504373073578\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0002640304446686059\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.024592773988842964\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.000292273674858734\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.0511380210518837\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0004920035717077553\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.011317661963403225\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00030442216666415334\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.028696784749627113\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00023688620422035456\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.024995308369398117\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00021311425371095538\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.046767666935920715\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.00046618940541520715\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.03051050938665867\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005602827877737582\n",
      "Are there any dead codes on this epoch?  330\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(3.5984, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.5848968029022217\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.019309572875499725\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.416050672531128\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.02267899364233017\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.3776779174804688\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.020830579102039337\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.6942461729049683\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.023207638412714005\n",
      "After scaling - encoder.proj.weight: grad norm 3.505176067352295\n",
      "After scaling - encoder.proj.bias: grad norm 0.040947962552309036\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.6846686601638794\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.023768208920955658\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 1.7991695404052734\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.019483545795083046\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 1.6987273693084717\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.018276046961545944\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.0703494548797607\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.03770370036363602\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.0284831523895264\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.04714126139879227\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.023484300822019577\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00028612068854272366\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.03579996898770332\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003360472619533539\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.020413821563124657\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00030865828739479184\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.025104589760303497\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003438805288169533\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.051938146352767944\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006067487993277609\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.010145116597414017\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.000352186820236966\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.026659298688173294\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0002886985312215984\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.025170989334583282\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0002708063693717122\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04549507796764374\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005586767219938338\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.030057165771722794\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0006985183572396636\n",
      "epoch: 18 ( 1 ) recon_loss: 2.8712992668151855  perplexity:  76.87727355957031  commit_loss:  0.17091268301010132 \n",
      "\t codebook loss:  0.6836507320404053  total_loss:  3.598419666290283 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(3.7175, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 0.9532808065414429\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.011104830540716648\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.5267055034637451\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.014366505667567253\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 0.951693594455719\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.013213946484029293\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.1760613918304443\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.013904818333685398\n",
      "After scaling - encoder.proj.weight: grad norm 2.449690341949463\n",
      "After scaling - encoder.proj.bias: grad norm 0.022963959723711014\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.670440137386322\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.015999969094991684\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 1.8585799932479858\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.013206098228693008\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 1.6870595216751099\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.012801006436347961\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.070495128631592\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.03017106093466282\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.064845085144043\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.04513649269938469\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.016858190298080444\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0001963821705430746\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.0269988551735878\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00025406290660612285\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.016830120235681534\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00023368059191852808\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.020797928795218468\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00024589826352894306\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04332128167152405\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.00040610364521853626\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.011856324970722198\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0002829497097991407\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.032867856323719025\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0002335417811991647\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.029834620654582977\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00022637800429947674\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.05429983511567116\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005335568566806614\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.036515526473522186\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0007982113747857511\n",
      "Are there any dead codes on this epoch?  328\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(3.4140, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.116091728210449\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.022053292021155357\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 3.1590466499328613\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.026168322190642357\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.9211925268173218\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.025599569082260132\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.29134464263916\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.03015386126935482\n",
      "After scaling - encoder.proj.weight: grad norm 4.470542907714844\n",
      "After scaling - encoder.proj.bias: grad norm 0.051791172474622726\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.8901078701019287\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.028094930574297905\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.2177133560180664\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.022475309669971466\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.087306499481201\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.021570570766925812\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.4043784141540527\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.03955565020442009\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.15079927444458\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.04702398180961609\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.0253504179418087\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0002641946484800428\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.03784484043717384\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00031349199707619846\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.02301555685698986\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003066784411203116\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.027449917048215866\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003612380533013493\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05355633795261383\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006204493693076074\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.010663339868187904\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00033657244057394564\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.02656782604753971\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00026925039128400385\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.025005575269460678\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00025841171736828983\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.040783870965242386\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.00047386990627273917\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.02576620876789093\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005633392138406634\n",
      "epoch: 19 ( 1 ) recon_loss: 2.682255744934082  perplexity:  79.88153076171875  commit_loss:  0.17206621170043945 \n",
      "\t codebook loss:  0.6882648468017578  total_loss:  3.4140231609344482 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(3.6954, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.8662629127502441\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.01758306846022606\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.7617528438568115\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.022785784676671028\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.7147270441055298\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.0218308474868536\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.2670199871063232\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.024707861244678497\n",
      "After scaling - encoder.proj.weight: grad norm 4.657456874847412\n",
      "After scaling - encoder.proj.bias: grad norm 0.042064450681209564\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.9810519814491272\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.02632184326648712\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.519986629486084\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.020891806110739708\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.252328395843506\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.018714604899287224\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 4.039670944213867\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.03414279222488403\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.4655542373657227\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.04164892062544823\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.021453628316521645\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00020212618983350694\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.031747736036777496\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.000261934008449316\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019711647182703018\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0002509565674699843\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02606053277850151\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0002840292581822723\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05353980511426926\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.00048355196486227214\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.011277684941887856\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0003025827754754573\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.028968511149287224\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00024016178213059902\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.02589164674282074\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00021513378305826336\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04643804207444191\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.00039248852408491075\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.028342783451080322\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004787752404808998\n",
      "Are there any dead codes on this epoch?  333\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(3.5348, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.8757398128509521\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.022167358547449112\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.7945785522460938\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.025120534002780914\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.7854759693145752\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02448621578514576\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.1305630207061768\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.028727512806653976\n",
      "After scaling - encoder.proj.weight: grad norm 4.214066028594971\n",
      "After scaling - encoder.proj.bias: grad norm 0.047514427453279495\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.9406757354736328\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.024851977825164795\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.5645570755004883\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.019977236166596413\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.4651875495910645\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.019087476655840874\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.620523452758789\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.036699194461107254\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.235750913619995\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.042729757726192474\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.022668685764074326\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00026789691764861345\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.0337730348110199\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003035865956917405\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.021577829495072365\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0002959207631647587\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.025748277083039284\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003471776726655662\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.050927821546792984\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005742213106714189\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.011368252336978912\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00030034102383069694\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.030993176624178886\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00024142884649336338\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.02979227714240551\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00023067592701409012\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04375474154949188\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.00044351699762046337\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.027019493281841278\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005163975874893367\n",
      "epoch: 20 ( 1 ) recon_loss: 2.569361925125122  perplexity:  84.71817016601562  commit_loss:  0.2270958125591278 \n",
      "\t codebook loss:  0.9083832502365112  total_loss:  3.5348362922668457 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(3.6459, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 0.9578844308853149\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.013914025388658047\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.6972806453704834\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.0185434240847826\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.1195920705795288\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.017378704622387886\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.4407296180725098\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.017977898940443993\n",
      "After scaling - encoder.proj.weight: grad norm 3.1632258892059326\n",
      "After scaling - encoder.proj.bias: grad norm 0.029684947803616524\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.8124857544898987\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.019803408533334732\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.183138608932495\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.016438104212284088\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 1.846838355064392\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.015011404640972614\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.2016279697418213\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.03276639059185982\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.007519245147705\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.042940884828567505\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.015097071416676044\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00021929685317445546\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.026750583201646805\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0002922600833699107\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01764572039246559\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00027390310424380004\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.022707121446728706\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0002833469188772142\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.049855124205350876\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.00046785996528342366\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01280546560883522\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0003121185000054538\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03440811485052109\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00025907845702022314\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.02910773828625679\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00023659247381146997\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.05046037212014198\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005164261092431843\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.03164020553231239\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0006767847226001322\n",
      "Are there any dead codes on this epoch?  323\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(3.4022, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.7528034448623657\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.016410047188401222\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.8108863830566406\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.021646631881594658\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.8187673091888428\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.020397376269102097\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.181727647781372\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.02302168309688568\n",
      "After scaling - encoder.proj.weight: grad norm 4.5853962898254395\n",
      "After scaling - encoder.proj.bias: grad norm 0.03873054310679436\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.068190574645996\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.021533632650971413\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.8129613399505615\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.016911737620830536\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.493873119354248\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.01640673540532589\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.769930601119995\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.03389507159590721\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.357250452041626\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.041887495666742325\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.020223939791321754\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0001893399894470349\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.032432157546281815\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00024975999258458614\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.020985033363103867\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00023534601496066898\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.025172889232635498\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00026562539278529584\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05290655046701431\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0004468750848900527\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01232483983039856\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00024845617008395493\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03245609626173973\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0001951285230461508\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.028774438425898552\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0001893017761176452\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.043497659265995026\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0003910831001121551\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.027198079973459244\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004833001585211605\n",
      "epoch: 21 ( 1 ) recon_loss: 2.4168636798858643  perplexity:  86.69532775878906  commit_loss:  0.2318480908870697 \n",
      "\t codebook loss:  0.9273923635482788  total_loss:  3.402228355407715 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(3.5965, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.7452853918075562\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.020028362050652504\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.579425811767578\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.02516656555235386\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.6902459859848022\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02425123192369938\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.10284686088562\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.027956647798419\n",
      "After scaling - encoder.proj.weight: grad norm 4.61480188369751\n",
      "After scaling - encoder.proj.bias: grad norm 0.04891753941774368\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.1781055927276611\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.02764412946999073\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.9837048053741455\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.022206777706742287\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.6918435096740723\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.020750273019075394\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 4.322412014007568\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.03843563422560692\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.6540634632110596\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03857294097542763\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.019326798617839813\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00022178844665177166\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.028563836589455605\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0002786874829325825\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.018717311322689056\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0002685513172764331\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.023286335170269012\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003095840511377901\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05110301449894905\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005416990607045591\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01304601039737463\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0003061233146581799\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03304070979356766\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0002459116221871227\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.029808716848492622\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00022978268680162728\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04786517471075058\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.00042562538874335587\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.029390349984169006\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004271459183655679\n",
      "Are there any dead codes on this epoch?  314\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(3.5045, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.9281831979751587\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.018808705732226372\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.826725721359253\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.021652929484844208\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.8787070512771606\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02117988094687462\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.343953847885132\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.024768957868218422\n",
      "After scaling - encoder.proj.weight: grad norm 4.808168888092041\n",
      "After scaling - encoder.proj.bias: grad norm 0.04209928959608078\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.0572923421859741\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.02374453656375408\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.88478684425354\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.019783806055784225\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.6922547817230225\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.017713254317641258\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.8395721912384033\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.031253159046173096\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.249829053878784\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03593879193067551\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.021510768681764603\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00020982949354220182\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.031534887850284576\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00024155959545169026\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.02095881476998329\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0002362822851864621\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.026149095967411995\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00027632195269688964\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05363982915878296\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0004696587275248021\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01179512869566679\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00026489351876080036\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.032182615250349045\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00022070770501159132\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.030034735798835754\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0001976086787180975\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04283418133854866\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.00034865952329710126\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.025099044665694237\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.000400932360207662\n",
      "epoch: 22 ( 1 ) recon_loss: 2.2961816787719727  perplexity:  90.14669036865234  commit_loss:  0.284381628036499 \n",
      "\t codebook loss:  1.137526512145996  total_loss:  3.5044519901275635 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(3.5969, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.6574887037277222\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.02065252885222435\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.455641984939575\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.027424657717347145\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.6421935558319092\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.025987127795815468\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.050898551940918\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.027750205248594284\n",
      "After scaling - encoder.proj.weight: grad norm 4.355935573577881\n",
      "After scaling - encoder.proj.bias: grad norm 0.04778779670596123\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.0601979494094849\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.027537938207387924\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.8611838817596436\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.022741256281733513\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.5146753787994385\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.021017594262957573\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.781479597091675\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.039620742201805115\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.2525548934936523\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.04364495724439621\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.019857313483953476\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00024742475943639874\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02941947802901268\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00032855733297765255\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019674070179462433\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00031133517040871084\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02457050420343876\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003324574790894985\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05218568444252014\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005725150695070624\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.012701554223895073\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00032991444459185004\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.0342780165374279\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00027244846569374204\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.030126718804240227\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00025179836666211486\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04530349001288414\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.00047467078547924757\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.026986420154571533\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005228823283687234\n",
      "Are there any dead codes on this epoch?  309\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(3.2979, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.249393939971924\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03039519488811493\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 3.3375606536865234\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.03575768694281578\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.273071527481079\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.034894343465566635\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.7058777809143066\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.039613690227270126\n",
      "After scaling - encoder.proj.weight: grad norm 5.716925621032715\n",
      "After scaling - encoder.proj.bias: grad norm 0.06746122986078262\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.4328910112380981\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03681108355522156\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.831876754760742\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.031597793102264404\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.38908052444458\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.028098968788981438\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 4.167825698852539\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.044985897839069366\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.537771701812744\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.04572439566254616\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.021121196448802948\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00028540260973386467\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.03133878484368324\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.000335754913976416\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.021343521773815155\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00032764836214482784\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02540745586156845\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003719617670867592\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05368037521839142\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006334425997920334\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013454455882310867\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00034564605448395014\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03598027676343918\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0002966946631204337\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03182254359126091\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0002638416481204331\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03913474455475807\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.00042240534094162285\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.023828985169529915\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00042933959048241377\n",
      "epoch: 23 ( 1 ) recon_loss: 2.1946821212768555  perplexity:  94.20278930664062  commit_loss:  0.259721040725708 \n",
      "\t codebook loss:  1.038884162902832  total_loss:  3.297858953475952 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(3.4978, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.917451024055481\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.013037371449172497\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 3.0122413635253906\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.019740207120776176\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.9977664947509766\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.018479924649000168\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.5766892433166504\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.01741671748459339\n",
      "After scaling - encoder.proj.weight: grad norm 5.84016227722168\n",
      "After scaling - encoder.proj.bias: grad norm 0.030376063659787178\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.5696607828140259\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.024741632863879204\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 4.184750556945801\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.020558690652251244\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.675447940826416\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.01801704242825508\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 5.050752639770508\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.02745390310883522\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.9360153675079346\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03000030294060707\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.017142770811915398\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00011655925482045859\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02693062834441662\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00017648524953983724\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01786082237958908\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00016521783254574984\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.023036619648337364\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00015571233234368265\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05221335589885712\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.000271574012003839\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014033387415111065\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00022119998175185174\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.037413325160741806\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00018380281107965857\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.032859958708286285\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00016107948613353074\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04515572637319565\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0002454487548675388\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.02624913491308689\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00026821455685421824\n",
      "Are there any dead codes on this epoch?  314\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(3.3505, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.86238694190979\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.030360275879502296\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.7123138904571533\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.034662552177906036\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.8756717443466187\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.03379605710506439\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.4050869941711426\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.03951895982027054\n",
      "After scaling - encoder.proj.weight: grad norm 5.130005836486816\n",
      "After scaling - encoder.proj.bias: grad norm 0.06800945103168488\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.147120475769043\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03636792674660683\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.1702301502227783\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.03072214126586914\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.9677581787109375\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.02865920588374138\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.9075350761413574\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.044011473655700684\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.190727472305298\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.04062601178884506\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02000107802450657\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00032605373417027295\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02912885695695877\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00037225798587314785\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.020143749192357063\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00036295229801908135\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02582939714193344\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00042441333062015474\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05509362369775772\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007303864695131779\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01231948472559452\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0003905728517565876\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03404664248228073\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00032993999775499105\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03187219426035881\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003077851433772594\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04196491464972496\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.00047266061301343143\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.02352728694677353\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004363024781923741\n",
      "epoch: 24 ( 1 ) recon_loss: 2.084416389465332  perplexity:  96.83223724365234  commit_loss:  0.29816317558288574 \n",
      "\t codebook loss:  1.192652702331543  total_loss:  3.350531816482544 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(3.4767, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.0554208755493164\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.023032981902360916\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 3.374459743499756\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.02870432659983635\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.266519546508789\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.027281802147626877\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.7017698287963867\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.030161188915371895\n",
      "After scaling - encoder.proj.weight: grad norm 6.277932167053223\n",
      "After scaling - encoder.proj.bias: grad norm 0.05001205578446388\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.705445647239685\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.0349414236843586\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 4.6498308181762695\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.02879541926085949\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.9665579795837402\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.02563025988638401\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 5.219720840454102\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04038795456290245\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.8737895488739014\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.04014330357313156\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.017173917964100838\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00019245037401560694\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02819504588842392\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00023983686696738005\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.0189377348870039\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0002279510663356632\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02257443591952324\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0002520095731597394\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05245479568839073\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0004178720118943602\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014249724335968494\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00029195044771768153\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03885131701827049\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00024059799034148455\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03314227983355522\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0002141517325071618\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04361299052834511\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.00033745853579603136\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.024011734873056412\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0003354143991600722\n",
      "Are there any dead codes on this epoch?  308\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(3.1622, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.4857896566390991\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.02617361769080162\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.1858553886413574\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.031094729900360107\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.4567315578460693\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.03002122975885868\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.8889687061309814\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.034594833850860596\n",
      "After scaling - encoder.proj.weight: grad norm 4.236017227172852\n",
      "After scaling - encoder.proj.bias: grad norm 0.061006128787994385\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.9897559881210327\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03797987848520279\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.6778509616851807\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.031560223549604416\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.4381399154663086\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.030228493735194206\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.5702388286590576\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.05290468782186508\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.9918789863586426\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.05037020146846771\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.018954571336507797\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.000333903037244454\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.027885472401976585\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00039668282261118293\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.018583867698907852\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00038298789877444506\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.024098020046949387\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004413345013745129\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05403987690806389\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.000778269546572119\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01262655109167099\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004845182702410966\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.034161973744630814\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004026212263852358\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03110392764210701\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00038563201087526977\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04554637894034386\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006749175954610109\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.02541087195277214\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0006425845785997808\n",
      "epoch: 25 ( 1 ) recon_loss: 1.9783302545547485  perplexity:  97.39170837402344  commit_loss:  0.2788662612438202 \n",
      "\t codebook loss:  1.1154650449752808  total_loss:  3.16219425201416 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(3.3236, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.422382116317749\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03975050523877144\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 3.845484733581543\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.04571348428726196\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.669581174850464\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.045324791222810745\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 3.3566322326660156\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.053425922989845276\n",
      "After scaling - encoder.proj.weight: grad norm 7.232885837554932\n",
      "After scaling - encoder.proj.bias: grad norm 0.08748992532491684\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.8644120693206787\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.052994709461927414\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 5.249636173248291\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.04464836046099663\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 4.5282487869262695\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.04019498825073242\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 5.767305850982666\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.05729416385293007\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 3.064823627471924\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.04517361894249916\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.017781535163521767\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0002917892416007817\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02822784334421158\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003355605585966259\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019596103578805923\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003327073936816305\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.024639414623379707\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00039217385347001255\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.053093113005161285\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006422212463803589\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013685747049748898\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0003890085208695382\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.038535039871931076\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003277420182712376\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.033239684998989105\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00029505195561796427\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04233500361442566\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0004205687437206507\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.0224973876029253\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0003315976937301457\n",
      "Are there any dead codes on this epoch?  307\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(3.1610, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.7041099071502686\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.04161179065704346\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.5006942749023438\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.048182163387537\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.7265057563781738\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.044137902557849884\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.2709250450134277\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.04847518354654312\n",
      "After scaling - encoder.proj.weight: grad norm 5.264504432678223\n",
      "After scaling - encoder.proj.bias: grad norm 0.08601059764623642\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.2871936559677124\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.05381575599312782\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.6305716037750244\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.04399591684341431\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.267341136932373\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.040425855666399\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 4.118035316467285\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.05548778176307678\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.1398959159851074\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03663739934563637\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.017758097499608994\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00043362591532059014\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.026059098541736603\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0005020941025577486\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.017991477623581886\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00045994986430741847\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02366473339498043\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0005051475600339472\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05486006289720535\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0008962946012616158\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013413517735898495\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005608003120869398\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.037833262234926224\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00045847028377465904\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03404812887310982\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00042126758489757776\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04291300103068352\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005782240186817944\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.02229931391775608\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0003817890246864408\n",
      "epoch: 26 ( 1 ) recon_loss: 1.9340466260910034  perplexity:  98.01688385009766  commit_loss:  0.2890794575214386 \n",
      "\t codebook loss:  1.1563178300857544  total_loss:  3.1609621047973633 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(3.1229, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.9861887693405151\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.038924604654312134\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.976649045944214\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.04400024563074112\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.179746150970459\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.04254351183772087\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.7928171157836914\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.0494685135781765\n",
      "After scaling - encoder.proj.weight: grad norm 5.701843738555908\n",
      "After scaling - encoder.proj.bias: grad norm 0.07912801951169968\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.4147706031799316\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.04969853535294533\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.998784065246582\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.042264364659786224\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.6985108852386475\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.04035940393805504\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 4.864135265350342\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.06568337976932526\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.606626510620117\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.055547915399074554\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.018129833042621613\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00035530186141841114\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.027170704677700996\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004016320453956723\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01989661529660225\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00038833505823276937\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.025492696091532707\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004515461332630366\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05204614996910095\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007222766289487481\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.012913956306874752\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004536457417998463\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03650069981813431\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00038578701787628233\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03375982493162155\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003683986433316022\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04439958557486534\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005995546816848218\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.023793157190084457\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.000507038610521704\n",
      "Are there any dead codes on this epoch?  298\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(3.1158, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.5758204460144043\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.038418401032686234\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 3.7311851978302\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.046941258013248444\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.560849189758301\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.043075017631053925\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 3.099748373031616\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.0435587540268898\n",
      "After scaling - encoder.proj.weight: grad norm 7.269345283508301\n",
      "After scaling - encoder.proj.bias: grad norm 0.07617177069187164\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.9313929080963135\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.059004686772823334\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 5.27224588394165\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.05017959699034691\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 4.657954692840576\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.045473843812942505\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 5.056553363800049\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.07301857322454453\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.6714985370635986\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.06195349618792534\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.019442180171608925\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0002899804094340652\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.028162825852632523\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00035431061405688524\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01932917907834053\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00032512834877707064\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.023396767675876617\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003287795989308506\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.054868705570697784\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005749412230215967\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014578070491552353\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004453648580238223\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.0397946871817112\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003787534369621426\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03515804186463356\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00034323459840379655\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.038166649639606476\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005511411000043154\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.020164355635643005\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00046762233250774443\n",
      "epoch: 27 ( 1 ) recon_loss: 1.9520130157470703  perplexity:  100.77393341064453  commit_loss:  0.2742709219455719 \n",
      "\t codebook loss:  1.0970836877822876  total_loss:  3.1157989501953125 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(2.9974, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.6404541730880737\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.022223858162760735\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.327782392501831\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.027373719960451126\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.5540568828582764\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.025559674948453903\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.9463611841201782\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.03005940094590187\n",
      "After scaling - encoder.proj.weight: grad norm 3.7812788486480713\n",
      "After scaling - encoder.proj.bias: grad norm 0.048156581819057465\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.9442172050476074\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.027532728388905525\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.5499510765075684\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.022753527387976646\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.472874879837036\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.021762389689683914\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.699854612350464\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04453668370842934\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.970151662826538\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03978152945637703\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.0212311539798975\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00028762652073055506\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.030126720666885376\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003542772901710123\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.020112980157136917\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00033079946297220886\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.025190278887748718\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00038903599488548934\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04893822222948074\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006232541636563838\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.012220287695527077\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0003563352220226079\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.033002082258462906\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0002944816369563341\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.032004546374082565\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00028165412368252873\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.047884415835142136\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005764045054093003\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.025498177856206894\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005148621276021004\n",
      "Are there any dead codes on this epoch?  303\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(3.0486, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.971327304840088\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.045492760837078094\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 4.197754859924316\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.0549626499414444\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.966895341873169\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.0513954721391201\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 3.6141223907470703\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.05211426690220833\n",
      "After scaling - encoder.proj.weight: grad norm 7.834333419799805\n",
      "After scaling - encoder.proj.bias: grad norm 0.08913302421569824\n",
      "After scaling - decoder.in_proj.weight: grad norm 2.038764238357544\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.06003524735569954\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 5.574387550354004\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.05035428702831268\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 5.02463960647583\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.04446962848305702\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 5.711713790893555\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.05868379771709442\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 3.090327262878418\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03618674352765083\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02038920670747757\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003121706540696323\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02880493365228176\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.000377152900910005\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.020358793437480927\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00035267500788904727\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.024800054728984833\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00035760735045187175\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05375908687710762\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006116295116953552\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01398997101932764\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00041196102392859757\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.0382513701915741\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003455303958617151\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03447900339961052\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003051499661523849\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03919369727373123\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.00040268737939186394\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.021205782890319824\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0002483129210304469\n",
      "epoch: 28 ( 1 ) recon_loss: 1.864975929260254  perplexity:  101.41095733642578  commit_loss:  0.2790078818798065 \n",
      "\t codebook loss:  1.116031527519226  total_loss:  3.0485899448394775 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(2.9986, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 3.4297637939453125\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.02507096529006958\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 4.764508247375488\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.03368087857961655\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.9289445877075195\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.029491091147065163\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 3.4459352493286133\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.030317207798361778\n",
      "After scaling - encoder.proj.weight: grad norm 7.2868499755859375\n",
      "After scaling - encoder.proj.bias: grad norm 0.05138840898871422\n",
      "After scaling - decoder.in_proj.weight: grad norm 2.078380823135376\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.038423385471105576\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 5.582954406738281\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.031202396377921104\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 5.1174774169921875\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.02732604742050171\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 6.509465217590332\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.047612424939870834\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 3.5733699798583984\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.036228541284799576\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02287282422184944\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00016719629638828337\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.0317741297185421\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00022461511252913624\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01953290030360222\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00019667376182042062\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.022980669513344765\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00020218307327013463\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.048595428466796875\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.000342705228831619\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013860558159649372\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0002562425215728581\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.037232283502817154\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00020808631961699575\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03412805125117302\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00018223524966742843\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04341110959649086\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0003175235178787261\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.02383052371442318\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00024160528846550733\n",
      "Are there any dead codes on this epoch?  302\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(2.9010, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.9755754470825195\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03907611593604088\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.8680901527404785\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.044122468680143356\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.2289891242980957\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.04277445003390312\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.8438851833343506\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.04741416871547699\n",
      "After scaling - encoder.proj.weight: grad norm 5.832569122314453\n",
      "After scaling - encoder.proj.bias: grad norm 0.0833645761013031\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.375690221786499\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.0469302274286747\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.905158042907715\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.04016737639904022\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.657663345336914\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.03692318871617317\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 4.478360176086426\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.06344763934612274\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.295105218887329\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.052594684064388275\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.018416350707411766\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00036426831502467394\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.026736387982964516\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004113104660063982\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.020778678357601166\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00039874418871477246\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.026510750874876976\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00044199576950632036\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.054371315985918045\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007771260570734739\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.012824208475649357\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.000437484442954883\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03640395402908325\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.000374441035091877\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03409680351614952\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003441986336838454\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.041747353971004486\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005914600333198905\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.021395014598965645\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004902885993942618\n",
      "epoch: 29 ( 1 ) recon_loss: 1.7371724843978882  perplexity:  106.74813842773438  commit_loss:  0.2743939757347107 \n",
      "\t codebook loss:  1.0975759029388428  total_loss:  2.900956153869629 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(2.9106, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 3.024749994277954\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.02240651100873947\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 4.282483100891113\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.0293840654194355\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.613797664642334\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.027054915204644203\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 3.0523014068603516\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.027998819947242737\n",
      "After scaling - encoder.proj.weight: grad norm 6.4731597900390625\n",
      "After scaling - encoder.proj.bias: grad norm 0.04743746295571327\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.7759592533111572\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03437342867255211\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 4.773248195648193\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.028215359896421432\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 4.329849720001221\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.02474188804626465\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 5.257434844970703\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.03985965624451637\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.973334789276123\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.036634329706430435\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.023426106199622154\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00017353410657960922\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.03316700458526611\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00022757392434868962\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.020243357867002487\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00020953510829713196\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.023639487102627754\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00021684546663891524\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05013337358832359\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0003673940082080662\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013754462823271751\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00026621558936312795\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.036967888474464417\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00021852253121323884\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03353385254740715\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0001916211622301489\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04071781784296036\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0003087053482886404\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.023027906194329262\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00028372585074976087\n",
      "Are there any dead codes on this epoch?  292\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(2.8658, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.1683125495910645\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.040967267006635666\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 3.1660947799682617\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.0467207171022892\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.24680495262146\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.04341014474630356\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.882173776626587\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.04703355208039284\n",
      "After scaling - encoder.proj.weight: grad norm 6.179930686950684\n",
      "After scaling - encoder.proj.bias: grad norm 0.08351542055606842\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.415124773979187\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.04799159988760948\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.9867329597473145\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.04092911630868912\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.66133713722229\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.036933306604623795\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 4.799558639526367\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.06176735460758209\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.5041301250457764\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.045548852533102036\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.019247718155384064\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00036365893902257085\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02810484915971756\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004147313011344522\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019944481551647186\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.000385343941161409\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02558453194797039\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004175083013251424\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05485812574625015\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007413512212224305\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.012561805546283722\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00042601273162290454\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.035389505326747894\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00036332031595520675\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03250102698802948\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003278502554167062\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04260480776429176\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005482975975610316\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.022228708490729332\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00040432889363728464\n",
      "epoch: 30 ( 1 ) recon_loss: 1.698480248451233  perplexity:  106.92826843261719  commit_loss:  0.2752775549888611 \n",
      "\t codebook loss:  1.1011102199554443  total_loss:  2.8657631874084473 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(2.7930, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 3.5732381343841553\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.028018727898597717\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 5.30709981918335\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.03290329501032829\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 3.5395493507385254\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.030597670003771782\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 4.171606063842773\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.03225051984190941\n",
      "After scaling - encoder.proj.weight: grad norm 8.932554244995117\n",
      "After scaling - encoder.proj.bias: grad norm 0.05663276091217995\n",
      "After scaling - decoder.in_proj.weight: grad norm 2.555427312850952\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.038956355303525925\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 7.033177852630615\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.03408027067780495\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 6.060796737670898\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.0288611501455307\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 5.465700149536133\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04193006828427315\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.979153633117676\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.034692730754613876\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.021272802725434303\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00016680581029504538\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.03159511834383011\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00019588542636483908\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.02107224054634571\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.000182159201358445\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02483510784804821\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00019199920643586665\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.053178783506155014\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0003371556813362986\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.015213399194180965\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00023192152730189264\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.041871096938848495\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0002028924209298566\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.036082156002521515\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00017182105511892587\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03253932669758797\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.00024962512543424964\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.017735999077558517\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00020653862156905234\n",
      "Are there any dead codes on this epoch?  288\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(2.8113, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.020404577255249\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.037664707750082016\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.8335413932800293\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.04369679465889931\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.022404193878174\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.040352657437324524\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.5812506675720215\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.04314503073692322\n",
      "After scaling - encoder.proj.weight: grad norm 5.89044189453125\n",
      "After scaling - encoder.proj.bias: grad norm 0.0717104971408844\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.590232014656067\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.04909246042370796\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 4.4574151039123535\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.04179506003856659\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.9719605445861816\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.03801380842924118\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 4.639470100402832\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.05932912230491638\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.442089796066284\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.045562636107206345\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.018199129030108452\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003392710641492158\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.025523591786623\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003936061111744493\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.018217138946056366\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.000363483268301934\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.023251041769981384\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.000388635991839692\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.053059130907058716\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006459441501647234\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014324276708066463\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00044220846029929817\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.040150903165340424\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003764759167097509\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.035778094083070755\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003424156457185745\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04179079085588455\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005344168748706579\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.021997526288032532\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004104129911866039\n",
      "epoch: 31 ( 1 ) recon_loss: 1.6504851579666138  perplexity:  109.72906494140625  commit_loss:  0.27380743622779846 \n",
      "\t codebook loss:  1.0952297449111938  total_loss:  2.811296224594116 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(2.7636, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.5440192222595215\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.021295655518770218\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.3151674270629883\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.025504151359200478\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.65169095993042\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.0227202121168375\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.0268735885620117\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.024022230878472328\n",
      "After scaling - encoder.proj.weight: grad norm 4.647152900695801\n",
      "After scaling - encoder.proj.bias: grad norm 0.04108499735593796\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.4597218036651611\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03151973709464073\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.9132821559906006\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.027263132855296135\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.5508477687835693\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.024182509630918503\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 4.08005952835083\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.040671929717063904\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.2645435333251953\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.031159797683358192\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01645805686712265\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0002269953110953793\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.024677904322743416\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00027185463113710284\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.017605755478143692\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00024217995814979076\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.021604912355542183\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00025605849805288017\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.049535077065229416\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.00043793447548523545\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.015559512190520763\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0003359761612955481\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.041712578386068344\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0002906040463130921\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.0378493070602417\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00025776695110835135\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.043490294367074966\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.00043353147339075804\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.02413829229772091\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0003321394615340978\n",
      "Are there any dead codes on this epoch?  288\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(2.8593, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 4.377851963043213\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.04691154882311821\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 6.36376428604126\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.0523642934858799\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 4.702853679656982\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.05086536705493927\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 5.814737319946289\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.05684712901711464\n",
      "After scaling - encoder.proj.weight: grad norm 12.518645286560059\n",
      "After scaling - encoder.proj.bias: grad norm 0.09552810341119766\n",
      "After scaling - decoder.in_proj.weight: grad norm 3.4685208797454834\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.062368836253881454\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 9.516837120056152\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.05007564648985863\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 8.450723648071289\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.04588083550333977\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 7.011331558227539\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.06112733483314514\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 3.8686599731445312\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.044835202395915985\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01937629096210003\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0002076296223094687\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.028165899217128754\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00023176337708719075\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.020814741030335426\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0002251291589345783\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.025735916569828987\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0002516043314244598\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05540728569030762\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0004228055477142334\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.015351607464253902\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00027604331262409687\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.042121339589357376\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00022163386165630072\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.037402745336294174\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0002030677133006975\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03103202022612095\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0002705484221223742\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01712261512875557\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00019843976770062\n",
      "epoch: 32 ( 1 ) recon_loss: 1.6681761741638184  perplexity:  114.36370086669922  commit_loss:  0.2810138165950775 \n",
      "\t codebook loss:  1.12405526638031  total_loss:  2.8593032360076904 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(2.9633, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 3.5608842372894287\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.030372316017746925\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 4.8959221839904785\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.037118736654520035\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 3.312446355819702\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.03150675818324089\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 3.8595643043518066\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.02892770990729332\n",
      "After scaling - encoder.proj.weight: grad norm 9.180769920349121\n",
      "After scaling - encoder.proj.bias: grad norm 0.051034193485975266\n",
      "After scaling - decoder.in_proj.weight: grad norm 2.786510944366455\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.04366219788789749\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 7.600269317626953\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.037712715566158295\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 6.814579010009766\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.03293292596936226\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 7.164573669433594\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.05479590967297554\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 3.9277658462524414\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.04209687560796738\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.019735638052225113\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0001683337613940239\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02713487483561039\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00020572471839841455\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.018358711153268814\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00017462123651057482\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02139102667570114\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.000160327268531546\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.050882965326309204\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0002828489523380995\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.015443798154592514\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0002419908414594829\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04212329164147377\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0002090167545247823\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03776872903108597\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00018252554582431912\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.039708517491817474\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.00030369768501259387\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.021769022569060326\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0002333152951905504\n",
      "Are there any dead codes on this epoch?  281\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(2.7785, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 3.8383641242980957\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03886653482913971\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 5.737268924713135\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.04630645364522934\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 3.9527885913848877\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.04569615423679352\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 4.896577835083008\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.05191856622695923\n",
      "After scaling - encoder.proj.weight: grad norm 10.315347671508789\n",
      "After scaling - encoder.proj.bias: grad norm 0.08973157405853271\n",
      "After scaling - decoder.in_proj.weight: grad norm 2.7669737339019775\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.05523008108139038\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 7.705293655395508\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.04424779862165451\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 6.780887603759766\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.042003221809864044\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 6.835701942443848\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.06696511805057526\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 3.580374002456665\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.054077137261629105\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.020056838169693947\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0002030916657531634\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.029979296028614044\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.000241967907641083\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.020654745399951935\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00023877888452261686\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.025586381554603577\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00027129321824759245\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05390141159296036\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0004688798217102885\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014458434656262398\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00028859704616479576\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04026293382048607\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00023121068079490215\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03543258458375931\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00021948198263999075\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03571900352835655\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.00034991686698049307\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018708745017647743\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0002825725241564214\n",
      "epoch: 33 ( 1 ) recon_loss: 1.6611404418945312  perplexity:  116.05360412597656  commit_loss:  0.26367083191871643 \n",
      "\t codebook loss:  1.0546833276748657  total_loss:  2.7785425186157227 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(2.8095, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 3.7575137615203857\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.031509045511484146\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 5.608022212982178\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.03881586343050003\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 4.00005578994751\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.0377698689699173\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 4.798750877380371\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.041012078523635864\n",
      "After scaling - encoder.proj.weight: grad norm 10.485920906066895\n",
      "After scaling - encoder.proj.bias: grad norm 0.07116245478391647\n",
      "After scaling - decoder.in_proj.weight: grad norm 2.999189615249634\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.04774880036711693\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 8.387401580810547\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.03881939500570297\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 7.291697978973389\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.03654401749372482\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 6.470355033874512\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.05107758566737175\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 3.485672950744629\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.04187175631523132\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01924780197441578\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00016140456136781722\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.028727000579237938\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00019883361528627574\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.020490217953920364\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00019347554189153016\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02458152174949646\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00021008368639741093\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.053713954985141754\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0003645284741651267\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01536329835653305\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00024459243286401033\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04296432435512543\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0001988517033169046\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03735160082578659\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00018719612853601575\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.033144284039735794\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0002616441051941365\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01785530149936676\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00021448735787998885\n",
      "Are there any dead codes on this epoch?  282\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(2.7195, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 3.2713828086853027\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03882506117224693\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 4.6600117683410645\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.04339133948087692\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 3.1960647106170654\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.043124690651893616\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 3.96567964553833\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.04909007251262665\n",
      "After scaling - encoder.proj.weight: grad norm 8.026642799377441\n",
      "After scaling - encoder.proj.bias: grad norm 0.08242247998714447\n",
      "After scaling - decoder.in_proj.weight: grad norm 2.096602439880371\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.05070506036281586\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 5.937084197998047\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.040274880826473236\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 5.541793346405029\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.038325581699609756\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 5.491742134094238\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.05445786938071251\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.867642879486084\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03878225386142731\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02149856649339199\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0002551468787714839\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.030624225735664368\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0002851551107596606\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.021003596484661102\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0002834027982316911\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.0260612815618515\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00032260551233775914\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05274873971939087\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005416563362814486\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013778254389762878\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0003332187479827553\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03901677206158638\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0002646746579557657\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.036419037729501724\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0002518644614610821\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03609011322259903\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0003578811010811478\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01884530484676361\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0002548655611462891\n",
      "epoch: 34 ( 1 ) recon_loss: 1.5131720304489136  perplexity:  116.15518951416016  commit_loss:  0.2847174406051636 \n",
      "\t codebook loss:  1.1388697624206543  total_loss:  2.719543933868408 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(2.8680, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 4.867451190948486\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03249561786651611\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 7.019436359405518\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.04169044271111488\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 4.548063278198242\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.03783686086535454\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 5.187526226043701\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.03837452083826065\n",
      "After scaling - encoder.proj.weight: grad norm 11.292407035827637\n",
      "After scaling - encoder.proj.bias: grad norm 0.06708163768053055\n",
      "After scaling - decoder.in_proj.weight: grad norm 3.236358404159546\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.04447796568274498\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 9.346257209777832\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.03632500395178795\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 8.0270414352417\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.032135821878910065\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 7.511072158813477\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04501689597964287\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 4.051878452301025\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.033257581293582916\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02214331552386284\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00014783111691940576\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.0319332629442215\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00018966078641824424\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.02069033496081829\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00017212984676007181\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.023599421605467796\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00017457579087931663\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05137213319540024\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0003051720268558711\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014723045751452446\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00020234196563251317\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.042518582195043564\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00016525201499462128\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.036517124623060226\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00014619431749451905\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.034169841557741165\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.00020479370141401887\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018433062359690666\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0001512974704382941\n",
      "Are there any dead codes on this epoch?  283\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(2.6164, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.7937079668045044\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.0324673168361187\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.5776028633117676\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.03558747470378876\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.9225881099700928\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.03423279523849487\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.4286258220672607\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.03754234313964844\n",
      "After scaling - encoder.proj.weight: grad norm 4.936792850494385\n",
      "After scaling - encoder.proj.bias: grad norm 0.06363348662853241\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.2870681285858154\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.04226464405655861\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.556943893432617\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.035427916795015335\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.6047184467315674\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.0341041162610054\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 4.0671281814575195\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.05331826210021973\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.0447158813476562\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03991897776722908\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.018713893368840218\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00033873404026962817\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02689232863485813\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00037128684925846756\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.020058508962392807\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003571533889044076\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02533803880214691\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00039168214425444603\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05150594189763069\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006638930644840002\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013428080826997757\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004409502726048231\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.0371098667383194\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00036962219746783376\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.037608303129673004\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003558108874130994\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04243266209959984\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005562735605053604\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.021332677453756332\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00041647773468866944\n",
      "epoch: 35 ( 1 ) recon_loss: 1.4334583282470703  perplexity:  116.34017181396484  commit_loss:  0.27924278378486633 \n",
      "\t codebook loss:  1.1169711351394653  total_loss:  2.616422176361084 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(2.7151, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.5375964641571045\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.029808981344103813\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 3.6089394092559814\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.03777185082435608\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.31748104095459\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.03354644775390625\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.641700506210327\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.03244248032569885\n",
      "After scaling - encoder.proj.weight: grad norm 6.011141300201416\n",
      "After scaling - encoder.proj.bias: grad norm 0.056521110236644745\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.8075827360153198\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.04356677457690239\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 5.289680004119873\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.03785918280482292\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 4.376388072967529\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.03436867892742157\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 4.083863258361816\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04103083536028862\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.2552521228790283\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03134249150753021\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.021436898037791252\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00025181786622852087\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.03048730082809925\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003190859279129654\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019577424973249435\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0002833908947650343\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.0223163403570652\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0002740649215411395\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05078042298555374\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0004774743865709752\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.015269949100911617\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00036803982220590115\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.0446857251226902\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003198237100150436\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03697049245238304\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0002903368731494993\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03449932485818863\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0003466169291641563\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01905173249542713\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0002647725341375917\n",
      "Are there any dead codes on this epoch?  281\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(2.6420, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.097757577896118\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.035013001412153244\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.931779384613037\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.039080530405044556\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.1546621322631836\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.03577060624957085\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.70737624168396\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.03751489147543907\n",
      "After scaling - encoder.proj.weight: grad norm 5.862445831298828\n",
      "After scaling - encoder.proj.bias: grad norm 0.0629497691988945\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.636794924736023\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.044846054166555405\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 4.6409382820129395\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.0380573533475399\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 4.496694087982178\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.035958267748355865\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 4.854422092437744\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.05808964744210243\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.443531036376953\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.0409444198012352\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01816735230386257\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00030322547536343336\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02539028972387314\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00033845179132185876\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.0186601672321558\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003097866429015994\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02344687655568123\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00032489278237335384\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05077093839645386\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005451681790873408\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01417524740099907\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0003883833414874971\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04019223526120186\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00032959069358184934\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03894302621483803\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003114118298981339\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04204108566045761\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005030777538195252\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.02116188034415245\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0003545937652233988\n",
      "epoch: 36 ( 1 ) recon_loss: 1.4097871780395508  perplexity:  119.76300811767578  commit_loss:  0.29091230034828186 \n",
      "\t codebook loss:  1.1636492013931274  total_loss:  2.6419730186462402 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(2.6923, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.2624683380126953\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03814659267663956\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 3.2647900581359863\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.04630082845687866\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.334200620651245\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.04365944117307663\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.831071376800537\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.04360487684607506\n",
      "After scaling - encoder.proj.weight: grad norm 6.349166393280029\n",
      "After scaling - encoder.proj.bias: grad norm 0.07311347126960754\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.9233860969543457\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.05378030613064766\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 5.463848114013672\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.04360327869653702\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 4.711337089538574\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.04053492471575737\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 4.49716329574585\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.05439751222729683\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.3758392333984375\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.04177495464682579\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.018399786204099655\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00031023158226162195\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02655128203332424\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003765468718484044\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.018983157351613045\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003550654510036111\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.023024016991257668\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003546217340044677\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05163533240556717\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.000594603770878166\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.015642160549759865\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004373745759949088\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04443537816405296\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.000354608753696084\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03831549361348152\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003296549548394978\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03657370060682297\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0004423940845299512\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.019321788102388382\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0003397396649233997\n",
      "Are there any dead codes on this epoch?  268\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(2.5868, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.9019728899002075\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.024384280666708946\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.702491521835327\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.028037162497639656\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.8587911128997803\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02497999370098114\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.221644163131714\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.025922195985913277\n",
      "After scaling - encoder.proj.weight: grad norm 4.926595687866211\n",
      "After scaling - encoder.proj.bias: grad norm 0.04342234507203102\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.3694220781326294\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03381185233592987\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.8973793983459473\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.029362214729189873\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.5681376457214355\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.027086306363344193\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.5250048637390137\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04124573618173599\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.9146509170532227\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03324555233120918\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02010658197104931\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0002577768173068762\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.028569210320711136\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00029639300191774964\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019650088623166084\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00026407436234876513\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02348596602678299\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0002740347699727863\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05208118259906769\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.00045903646969236434\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014476755633950233\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00035743979969993234\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04120088741183281\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003104007337242365\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03772033005952835\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00028634112095460296\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03726435825228691\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.00043602660298347473\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.020240604877471924\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00035145319998264313\n",
      "epoch: 37 ( 1 ) recon_loss: 1.3463411331176758  perplexity:  121.84429168701172  commit_loss:  0.2929157614707947 \n",
      "\t codebook loss:  1.1716630458831787  total_loss:  2.5868043899536133 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(2.7546, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.318657636642456\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.04606686159968376\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 3.4162938594818115\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.05459066480398178\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.580089569091797\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.051793843507766724\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 3.308605670928955\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.052864570170640945\n",
      "After scaling - encoder.proj.weight: grad norm 7.092085838317871\n",
      "After scaling - encoder.proj.bias: grad norm 0.08736351132392883\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.919980525970459\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.06221058592200279\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 5.383489608764648\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.05093313753604889\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 4.912412166595459\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.04783891513943672\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 4.982375144958496\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.06213071197271347\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.514747381210327\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.042350076138973236\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.017632024362683296\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003503113111946732\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.025978902354836464\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004151297907810658\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019620060920715332\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00039386164280585945\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.025159994140267372\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00040200387593358755\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05393113195896149\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006643480155616999\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014600319787859917\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00047307481872849166\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04093826562166214\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00038731651147827506\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.037356000393629074\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003637867630459368\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03788802772760391\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.00047246742178685963\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.019123172387480736\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0003220473590772599\n",
      "Are there any dead codes on this epoch?  279\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(2.5271, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.223173975944519\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.016782473772764206\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.8105388879776\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.01895671896636486\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.2616920471191406\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.01705438457429409\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.5204838514328003\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.017981700599193573\n",
      "After scaling - encoder.proj.weight: grad norm 3.305253028869629\n",
      "After scaling - encoder.proj.bias: grad norm 0.0295404139906168\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.996161162853241\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.024459820240736008\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.962014675140381\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.021003765985369682\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.6216182708740234\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.01843811571598053\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.811737298965454\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.03182552754878998\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.4190113544464111\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.024234497919678688\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01800226978957653\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00024699888308532536\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02664691023528576\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0002789987192954868\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.018569163978099823\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00025100077618844807\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.022377977147698402\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0002646487264428288\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04864561930298805\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.00043476599967107177\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014661170542240143\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0003599915362428874\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04359395056962967\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00030912645161151886\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.038584109395742416\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00027136606513522565\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04138221591711044\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.00046839757123962045\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.02088453806936741\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00035667530028149486\n",
      "epoch: 38 ( 1 ) recon_loss: 1.2647813558578491  perplexity:  124.63855743408203  commit_loss:  0.29812729358673096 \n",
      "\t codebook loss:  1.1925091743469238  total_loss:  2.527108907699585 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(2.6990, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.781691074371338\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.05832495167851448\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 3.983198404312134\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.06659428030252457\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 3.118173360824585\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.06367101520299911\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 4.007246971130371\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.06787953525781631\n",
      "After scaling - encoder.proj.weight: grad norm 8.036849975585938\n",
      "After scaling - encoder.proj.bias: grad norm 0.11189186573028564\n",
      "After scaling - decoder.in_proj.weight: grad norm 2.251847743988037\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.08233390748500824\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 6.561774730682373\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.06815938651561737\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 6.174506187438965\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.06476493924856186\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 6.263543128967285\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.09017667919397354\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.934781789779663\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.06657589226961136\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.017659125849604607\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003702667308971286\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.025286704301834106\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004227632307447493\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01979522965848446\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0004042053478769958\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.025439374148845673\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004309224314056337\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05102067068219185\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007103278185240924\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01429549977183342\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005226837820373476\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.041656386107206345\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00043269904563203454\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03919787332415581\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0004111499583814293\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.039763111621141434\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.000572472345083952\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018630998209118843\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004226465243846178\n",
      "Are there any dead codes on this epoch?  270\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(2.4977, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.5839494466781616\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.033658143132925034\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.3158090114593506\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.039726682007312775\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.75026273727417\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.03716961666941643\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.2848641872406006\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.03958914801478386\n",
      "After scaling - encoder.proj.weight: grad norm 4.692133903503418\n",
      "After scaling - encoder.proj.bias: grad norm 0.06550755351781845\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.3668198585510254\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.0470469631254673\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 4.013540744781494\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.039711304008960724\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.6852056980133057\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.03646707162261009\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 4.346529006958008\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.057999007403850555\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.137559413909912\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.04356269910931587\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.016406165435910225\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003486229106783867\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.023986589163541794\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004114793555345386\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01812879927456379\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00038499393849633634\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02366606891155243\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004100548103451729\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04859998822212219\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006785113946534693\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01415718998759985\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004873010329902172\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04157128557562828\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004113200702704489\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.0381704717874527\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003777171077672392\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.045020293444395065\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006007396732456982\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.022140325978398323\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.000451211875770241\n",
      "epoch: 39 ( 1 ) recon_loss: 1.289530634880066  perplexity:  125.27474212646484  commit_loss:  0.28539130091667175 \n",
      "\t codebook loss:  1.141565203666687  total_loss:  2.4977402687072754 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(2.7368, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 3.036080837249756\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.06535664945840836\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 4.354319095611572\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.07356275618076324\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 3.3743021488189697\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.06902226060628891\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 4.348448276519775\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.0737362653017044\n",
      "After scaling - encoder.proj.weight: grad norm 8.572283744812012\n",
      "After scaling - encoder.proj.bias: grad norm 0.12113865464925766\n",
      "After scaling - decoder.in_proj.weight: grad norm 2.3270509243011475\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.0857272744178772\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 6.755804538726807\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.07123590260744095\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 6.884945392608643\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.0695401057600975\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 6.779439926147461\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.09687849134206772\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 3.309006690979004\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.06749796867370605\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.017909204587340355\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.000385525228921324\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02568521723151207\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004339313309174031\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019904302433133125\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00040714789065532386\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.025650588795542717\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00043495482532307506\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.050566110759973526\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007145716226659715\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013726786710321903\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005056872614659369\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.039851076900959015\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004202057025395334\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.040612854063510895\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00041020254138857126\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03999049589037895\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.000571465992834419\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.0195191390812397\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00039815640775486827\n",
      "Are there any dead codes on this epoch?  265\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(2.5804, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.3750195503234863\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.06131572276353836\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 3.3765249252319336\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.07031175494194031\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.752708673477173\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.06543353945016861\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 3.6289706230163574\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.06850035488605499\n",
      "After scaling - encoder.proj.weight: grad norm 7.46815824508667\n",
      "After scaling - encoder.proj.bias: grad norm 0.11308643221855164\n",
      "After scaling - decoder.in_proj.weight: grad norm 2.067920446395874\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.0792476013302803\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 5.976844310760498\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.06646046787500381\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 5.790032863616943\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.0631442591547966\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 6.036159515380859\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.0907636359333992\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.86909556388855\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.06454052031040192\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.016319023445248604\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00042130713700316846\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02320047840476036\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004831198893953115\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01891416683793068\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0004496011242736131\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.024935059249401093\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004706735780928284\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.051314543932676315\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007770294323563576\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01420891098678112\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005445190472528338\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.041067562997341156\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00045665723155252635\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.039783962070941925\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00043387123150750995\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04147512465715408\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006236470653675497\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.0197138749063015\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.000443465105490759\n",
      "epoch: 40 ( 1 ) recon_loss: 1.339656949043274  perplexity:  130.58876037597656  commit_loss:  0.29311105608940125 \n",
      "\t codebook loss:  1.172444224357605  total_loss:  2.580357074737549 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(2.5671, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.162179946899414\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.033640626817941666\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 3.124145746231079\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.03732173144817352\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.2948007583618164\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.034318696707487106\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 3.0291385650634766\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.0374574288725853\n",
      "After scaling - encoder.proj.weight: grad norm 6.1557087898254395\n",
      "After scaling - encoder.proj.bias: grad norm 0.06212402507662773\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.6898694038391113\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.04731779173016548\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 4.733170986175537\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.03894834220409393\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 4.844791412353516\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.039722740650177\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 5.395202159881592\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.06369014084339142\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.6085474491119385\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.04665372520685196\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01757035404443741\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00027337123174220324\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02538750320672989\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003032847016584128\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.018648061901330948\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00027888137265108526\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02461545169353485\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.000304387416690588\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.050022661685943604\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005048336461186409\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013732255436480045\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0003845149476546794\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.0384627990424633\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00031650287564843893\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.0393698513507843\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003227958222851157\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.043842609971761703\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005175602273084223\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.021197635680437088\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00037911851541139185\n",
      "Are there any dead codes on this epoch?  259\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(2.4605, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.0204591751098633\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.05645628273487091\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.703831911087036\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.060356028378009796\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.2318451404571533\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.055398304015398026\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.9429025650024414\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.058660198003053665\n",
      "After scaling - encoder.proj.weight: grad norm 6.061501502990723\n",
      "After scaling - encoder.proj.bias: grad norm 0.09778707474470139\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.742334246635437\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.07132250815629959\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 5.048465251922607\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.06025679409503937\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 4.982377052307129\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.05862116813659668\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 5.093273162841797\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.08409763872623444\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.3624653816223145\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.06104819476604462\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.016681954264640808\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00046613221638835967\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.022324232384562492\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004983305116184056\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01842726580798626\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0004573969927150756\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02429812401533127\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00048432889161631465\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.0500468872487545\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.000807380594778806\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014385610818862915\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005888754967600107\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.041682735085487366\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004975112387910485\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.041137076914310455\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0004840066540054977\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04205269366502762\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.000694353599101305\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.019505733624100685\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005040453979745507\n",
      "epoch: 41 ( 1 ) recon_loss: 1.2746840715408325  perplexity:  135.54519653320312  commit_loss:  0.28019288182258606 \n",
      "\t codebook loss:  1.1207715272903442  total_loss:  2.4605255126953125 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(2.4468, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.3711209297180176\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.022932643070816994\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 3.3925981521606445\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.025335822254419327\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.3248207569122314\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.023272233083844185\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.808364152908325\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.0253460593521595\n",
      "After scaling - encoder.proj.weight: grad norm 5.828959941864014\n",
      "After scaling - encoder.proj.bias: grad norm 0.04138683155179024\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.8593717813491821\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.02813578024506569\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 5.184631824493408\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.02414753846824169\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 4.7795305252075195\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.021704701706767082\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 4.670072078704834\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.03348537161946297\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.4807400703430176\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.025821581482887268\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.019653620198369026\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0001900828501675278\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.028120385482907295\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0002100022102240473\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019269850105047226\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00019289765623398125\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02327781915664673\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00021008707699365914\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04831477254629135\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0003430449578445405\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.015411860309541225\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0002332103467779234\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04297409951686859\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00020015281916130334\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03961631655693054\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0001799047749955207\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.038709044456481934\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0002775517641566694\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.020562227815389633\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00021402853599283844\n",
      "Are there any dead codes on this epoch?  253\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(2.4436, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.254700183868408\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.044168613851070404\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 3.2094948291778564\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.04753971844911575\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.2081716060638428\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.04247298836708069\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.8641390800476074\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.04385320097208023\n",
      "After scaling - encoder.proj.weight: grad norm 6.232830047607422\n",
      "After scaling - encoder.proj.bias: grad norm 0.07387452572584152\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.6661319732666016\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.05717305466532707\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 4.7644219398498535\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.04854758828878403\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 4.496772289276123\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.046181194484233856\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 4.856163024902344\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.0700685977935791\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.4393351078033447\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.051697831600904465\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.018883151933550835\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003699129447340965\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.026879576966166496\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00039814598858356476\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.018493475392460823\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003557120217010379\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.023987216874957085\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003672713355626911\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.052200064063072205\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006187004619278014\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013953885063529015\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004788253572769463\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03990212082862854\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004065869434271008\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03766055032610893\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.000386768311727792\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.040670450776815414\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005868257721886039\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.020429475232958794\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00043297026422806084\n",
      "epoch: 42 ( 1 ) recon_loss: 1.2974752187728882  perplexity:  134.79823303222656  commit_loss:  0.27085885405540466 \n",
      "\t codebook loss:  1.0834354162216187  total_loss:  2.443645477294922 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(2.3962, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.3368754386901855\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.02614826336503029\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 3.3703978061676025\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.031389884650707245\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.2747087478637695\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02837640792131424\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.662369728088379\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.02890409156680107\n",
      "After scaling - encoder.proj.weight: grad norm 5.699592113494873\n",
      "After scaling - encoder.proj.bias: grad norm 0.04919333755970001\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.719396948814392\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03578782081604004\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 4.776065349578857\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.03043140098452568\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 4.351192951202393\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.02846130169928074\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 4.101287841796875\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.043323203921318054\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.2263951301574707\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.030871201306581497\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.020650627091526985\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0002310683485120535\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.029783710837364197\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00027738779317587614\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.020101267844438553\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00025075816665776074\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02352697029709816\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00025542121147736907\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05036645755171776\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.00043471428216435015\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.0151940593495965\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0003162517095915973\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.042205389589071274\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00026891782181337476\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.038450852036476135\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0002515083469916135\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03624247759580612\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0003828408080153167\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.019674327224493027\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00027280428912490606\n",
      "Are there any dead codes on this epoch?  254\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(2.2594, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.7041536569595337\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.025551125407218933\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.4810574054718018\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.02728120982646942\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.6425116062164307\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02498025633394718\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.0286319255828857\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.02653823047876358\n",
      "After scaling - encoder.proj.weight: grad norm 4.370940685272217\n",
      "After scaling - encoder.proj.bias: grad norm 0.04556958004832268\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.1447490453720093\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03151555731892586\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.3280558586120605\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.02742788754403591\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.075939893722534\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.025876151397824287\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.3110523223876953\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.040240101516246796\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.7326310873031616\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.029893992468714714\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.020253023132681847\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00030366252758540213\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.029486142098903656\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00032422368531115353\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019520439207553864\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0002968780172523111\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.024109287187457085\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003153937868773937\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05194646865129471\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005415719351731241\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013604776933789253\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00037454679841175675\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03955230116844177\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00032596688834019005\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.036556024104356766\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003075252752751112\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.039350222796201706\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0004782337346114218\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.020591463893651962\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00035527534782886505\n",
      "epoch: 43 ( 1 ) recon_loss: 1.1605688333511353  perplexity:  141.28134155273438  commit_loss:  0.2597062885761261 \n",
      "\t codebook loss:  1.0388251543045044  total_loss:  2.259389877319336 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(2.4022, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.032203435897827\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.02533702738583088\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.865785837173462\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.0299986582249403\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.9052150249481201\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.026285843923687935\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.2424309253692627\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.02565082721412182\n",
      "After scaling - encoder.proj.weight: grad norm 4.896048069000244\n",
      "After scaling - encoder.proj.bias: grad norm 0.04397541284561157\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.3920313119888306\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.0332365408539772\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.9972569942474365\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.027667727321386337\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.591973066329956\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.02591085433959961\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.586977481842041\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.038406915962696075\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.9002844095230103\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.029700295999646187\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.021155301481485367\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0002637592551764101\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02983291819691658\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003122869529761374\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01983334869146347\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00027363645494915545\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.023343777284026146\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0002670259273145348\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05096801370382309\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0004577854124363512\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014491090551018715\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00034599340870045125\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04161156713962555\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0002880218962673098\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03739255294203758\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00026973281637765467\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.037340547889471054\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0003998171887360513\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.019782021641731262\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00030918102129362524\n",
      "Are there any dead codes on this epoch?  246\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(2.2274, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.7308082580566406\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.01953318528831005\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.4601902961730957\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.021616443991661072\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.6373836994171143\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.020190007984638214\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.909744381904602\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.021419189870357513\n",
      "After scaling - encoder.proj.weight: grad norm 4.079715728759766\n",
      "After scaling - encoder.proj.bias: grad norm 0.03731783106923103\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.1243531703948975\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.025804707780480385\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.243497610092163\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.023514559492468834\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.152402400970459\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.022241460159420967\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.297238349914551\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.037232156842947006\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.7048054933547974\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.02571505308151245\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.021062426269054413\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00023770183906890452\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02993837185204029\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0002630532835610211\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01992553286254406\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00024569479865022004\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.023239925503730774\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00026065288693644106\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.049646586179733276\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0004541254893410951\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013682397082448006\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0003140208136755973\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03947053849697113\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00028615171322599053\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.038361988961696625\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0002706591913010925\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04012451320886612\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0004530829028226435\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.02074599824845791\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0003129297692794353\n",
      "epoch: 44 ( 1 ) recon_loss: 1.1195939779281616  perplexity:  143.31602478027344  commit_loss:  0.26186513900756836 \n",
      "\t codebook loss:  1.0474605560302734  total_loss:  2.227395534515381 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(2.3736, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.5972521305084229\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03345967084169388\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.184657573699951\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.03654840216040611\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.5527417659759521\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.03228287026286125\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.9167909622192383\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.03231731429696083\n",
      "After scaling - encoder.proj.weight: grad norm 4.195847511291504\n",
      "After scaling - encoder.proj.bias: grad norm 0.055972978472709656\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.2917684316635132\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.04475510120391846\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.864381790161133\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.038722842931747437\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.5828428268432617\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.036866337060928345\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.5381531715393066\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.049797847867012024\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.744179368019104\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03505711629986763\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.018318122252821922\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003837329859379679\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.025054797530174255\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004191561893094331\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.017807655036449432\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00037023687036708\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.021982762962579727\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00037063186755403876\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.048120174556970596\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006419274141080678\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014814676716923714\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005132749211043119\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.044318750500679016\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00044409383554011583\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04108991473913193\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00042280243360437453\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04057738929986954\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005711077828891575\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.02000316046178341\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.000402053352445364\n",
      "Are there any dead codes on this epoch?  253\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(2.3493, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.0846197605133057\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.013343332335352898\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.969973087310791\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.019473280757665634\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.9036619663238525\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.016613882035017014\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.3181746006011963\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.016238931566476822\n",
      "After scaling - encoder.proj.weight: grad norm 5.051535129547119\n",
      "After scaling - encoder.proj.bias: grad norm 0.027500083670020103\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.3311045169830322\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.025829007849097252\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.953355312347412\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.022404713556170464\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.5558295249938965\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.02050171233713627\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.4438607692718506\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.027960626408457756\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.861997365951538\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.01787884533405304\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02160966768860817\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00013832018885295838\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.030787454918026924\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00020186472102068365\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019733818247914314\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00017222348833456635\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02403075620532036\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00016833667177706957\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05236542969942093\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.00028507248498499393\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013798549771308899\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0002677497104741633\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04098143428564072\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00023225265613291413\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03686058148741722\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00021252567239571363\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.035699889063835144\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0002898465609177947\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.019301913678646088\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00018533642287366092\n",
      "epoch: 45 ( 1 ) recon_loss: 1.1419552564620972  perplexity:  143.3134307861328  commit_loss:  0.2854326367378235 \n",
      "\t codebook loss:  1.141730546951294  total_loss:  2.349294424057007 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(2.3021, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.5388485193252563\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.032172780483961105\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.1324477195739746\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.03494381532073021\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.5858643054962158\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.03175366669893265\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.002964496612549\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.032649796456098557\n",
      "After scaling - encoder.proj.weight: grad norm 4.248091220855713\n",
      "After scaling - encoder.proj.bias: grad norm 0.05411696806550026\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.2885984182357788\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.041445452719926834\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.721456527709961\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.033958688378334045\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.5027217864990234\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.03269162029027939\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.781700611114502\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.05122729763388634\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.856136441230774\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03677563741803169\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01753350906074047\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003665739204734564\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.024296928197145462\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00039814686169847846\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.018069203943014145\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.000361798593075946\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.022821607068181038\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003720090026035905\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04840238764882088\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006166041130200028\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014682180248200893\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00047222591820172966\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.042401958256959915\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00038692238740622997\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.039909716695547104\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00037248554872348905\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04308837279677391\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005836794734932482\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.02114866115152836\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00041901846998371184\n",
      "Are there any dead codes on this epoch?  241\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(2.2611, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.4306271076202393\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.018064554780721664\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.0004947185516357\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.02264547534286976\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.3115172386169434\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.01930411532521248\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.4888813495635986\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.017524737864732742\n",
      "After scaling - encoder.proj.weight: grad norm 3.5522983074188232\n",
      "After scaling - encoder.proj.bias: grad norm 0.03023485466837883\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.2159273624420166\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03337642550468445\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.7685279846191406\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.031253617256879807\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.177001476287842\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.027434147894382477\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.7851932048797607\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.03456605225801468\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.4612308740615845\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.025914473459124565\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.018740257248282433\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0002366335829719901\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.026205142959952354\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0002966405590996146\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.017179997637867928\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00025287101743742824\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.01950334757566452\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0002295623125974089\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.046532731503248215\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.00039605636266060174\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.015927834436297417\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00043720880057662725\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04936519265174866\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00040940145845524967\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.041616592556238174\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003593689762055874\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.036484166979789734\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.00045279209734871984\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.019141145050525665\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00033946227631531656\n",
      "epoch: 46 ( 1 ) recon_loss: 1.0731688737869263  perplexity:  145.00531005859375  commit_loss:  0.28087663650512695 \n",
      "\t codebook loss:  1.1235065460205078  total_loss:  2.261080265045166 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(2.4217, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.1923766136169434\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.038831986486911774\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 3.0280423164367676\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.04439764469861984\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.258863687515259\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.03909579664468765\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.7387847900390625\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.036758940666913986\n",
      "After scaling - encoder.proj.weight: grad norm 6.1189069747924805\n",
      "After scaling - encoder.proj.bias: grad norm 0.061657968908548355\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.7797504663467407\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.05679783225059509\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 5.269715785980225\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.05146778002381325\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 4.957728385925293\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.046445392072200775\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 4.876570701599121\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.06594160944223404\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.3372035026550293\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.04131847620010376\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.017972461879253387\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003183332737535238\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.024823002517223358\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003639589122030884\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.018517501652240753\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003204958629794419\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02245175465941429\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00030133905238471925\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.050161007791757584\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005054539651609957\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014589873142540455\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.000465612014522776\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04319958761334419\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00042191779357381165\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04064200446009636\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00038074571057222784\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03997670114040375\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005405700067058206\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01915971003472805\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0003387167816981673\n",
      "Are there any dead codes on this epoch?  243\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(2.2328, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.9617705345153809\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.017508409917354584\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.812516450881958\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.018931284546852112\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.7990366220474243\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.015855232253670692\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.219271183013916\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.01556536927819252\n",
      "After scaling - encoder.proj.weight: grad norm 4.8995161056518555\n",
      "After scaling - encoder.proj.bias: grad norm 0.026799147948622704\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.3496510982513428\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.0276033915579319\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.806837320327759\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.023620111867785454\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.4292194843292236\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.02201792038977146\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.13128662109375\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.0323166698217392\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.7339060306549072\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.023831592872738838\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02130151353776455\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0001901117357192561\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.030539175495505333\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00020556175149977207\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01953449845314026\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00017216104606632143\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.024097533896565437\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00016901361232157797\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05320046469569206\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0002909934555646032\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014654930680990219\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00029972620541229844\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.0413358211517334\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0002564745082054287\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03723553195595741\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00023907740251161158\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03400048241019249\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0003509044472593814\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018827291205525398\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00025877082953229547\n",
      "epoch: 47 ( 1 ) recon_loss: 1.0670051574707031  perplexity:  144.5667724609375  commit_loss:  0.27568352222442627 \n",
      "\t codebook loss:  1.102734088897705  total_loss:  2.2328033447265625 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(2.2625, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.2608418464660645\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.025944536551833153\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.7228124141693115\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.029738115146756172\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.306361198425293\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02655135467648506\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.6258177757263184\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.027279580011963844\n",
      "After scaling - encoder.proj.weight: grad norm 3.291226625442505\n",
      "After scaling - encoder.proj.bias: grad norm 0.04610332101583481\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.089877724647522\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03831106796860695\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.206792116165161\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.0325789600610733\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.242396354675293\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.03182598203420639\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.5675277709960938\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04990747570991516\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.6495659351348877\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.035830866545438766\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01667320728302002\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00034308715839870274\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02278224565088749\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00039325293619185686\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.017275147140026093\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00035111160832457244\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.021499602124094963\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003607416001614183\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04352274909615517\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006096642464399338\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014412399381399155\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005066205631010234\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.042406193912029266\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00043081992771476507\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04287702590227127\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00042086257599294186\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04717651754617691\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006599699263460934\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.021813644096255302\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00047382269985973835\n",
      "Are there any dead codes on this epoch?  243\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(2.1911, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.8447705507278442\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03390805423259735\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.602808713912964\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.03723437339067459\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.87288236618042\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.0335906557738781\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.3590328693389893\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.03426692634820938\n",
      "After scaling - encoder.proj.weight: grad norm 5.0173869132995605\n",
      "After scaling - encoder.proj.bias: grad norm 0.05799465626478195\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.543421745300293\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.051242485642433167\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 4.39115047454834\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.04530274122953415\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 4.265819072723389\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.04185042902827263\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.912381649017334\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.05566183477640152\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.8007527589797974\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03436785563826561\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01818990521132946\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00033434201031923294\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.025664355605840683\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003671404265332967\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.018467094749212265\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00033121241722255945\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02326066419482231\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003378805995453149\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.049472708255052567\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005718420725315809\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.015218529850244522\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005052639171481133\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04329786077141762\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00044669650378637016\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04206205904483795\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0004126558778807521\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03857707604765892\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005488398019224405\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01775587908923626\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0003388758050277829\n",
      "epoch: 48 ( 1 ) recon_loss: 1.039313793182373  perplexity:  143.02381896972656  commit_loss:  0.2724076807498932 \n",
      "\t codebook loss:  1.0896307229995728  total_loss:  2.1911144256591797 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(2.2448, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.6368108987808228\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.028506385162472725\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.149635076522827\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.032426703721284866\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.5257577896118164\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.028902702033519745\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.8645421266555786\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.028669314458966255\n",
      "After scaling - encoder.proj.weight: grad norm 4.139771938323975\n",
      "After scaling - encoder.proj.bias: grad norm 0.049197740852832794\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.3255597352981567\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03816690668463707\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.838914155960083\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.033290427178144455\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.6589150428771973\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.032450802624225616\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.778998613357544\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04664432629942894\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.891337275505066\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03227521851658821\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.018533824011683464\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.000322781503200531\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.024340597912669182\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00036717180046252906\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.017276354134082794\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00032726905192248523\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.021112453192472458\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003246263659093529\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04687517508864403\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005570724024437368\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.015009485185146332\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00043216883204877377\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.043468523770570755\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00037695179344154894\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04143036901950836\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003674446779768914\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.042790088802576065\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005281597259454429\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.02141585759818554\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0003654564789030701\n",
      "Are there any dead codes on this epoch?  234\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(2.2369, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.8797199726104736\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.020418046042323112\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.616177797317505\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.023853840306401253\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.7278114557266235\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.022549251094460487\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.1956613063812256\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.023092176765203476\n",
      "After scaling - encoder.proj.weight: grad norm 4.866494655609131\n",
      "After scaling - encoder.proj.bias: grad norm 0.03844020143151283\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.1818095445632935\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.0298225786536932\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.242821216583252\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.02373962104320526\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.8606936931610107\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.022076422348618507\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.7930874824523926\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.0322735458612442\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.5429458618164062\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.023596184328198433\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.022129373624920845\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0002403754333499819\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.030799465253949165\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0002808240242302418\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.020340997725725174\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0002654654672369361\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.025848854333162308\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00027185719227418303\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05729176104068756\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.00045254480210132897\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01391308382153511\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0003510921378619969\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.038176752626895905\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0002794793399516493\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.0336780771613121\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0002598990104161203\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.032882168889045715\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.00037994663580320776\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018164632841944695\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00027779070660471916\n",
      "epoch: 49 ( 1 ) recon_loss: 0.9940335750579834  perplexity:  147.26614379882812  commit_loss:  0.29397591948509216 \n",
      "\t codebook loss:  1.1759036779403687  total_loss:  2.236876964569092 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(2.2514, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.0304431915283203\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03284706175327301\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.6365699768066406\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.036140237003564835\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.8894881010055542\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.030798401683568954\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.2561538219451904\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.02925017662346363\n",
      "After scaling - encoder.proj.weight: grad norm 5.137270450592041\n",
      "After scaling - encoder.proj.bias: grad norm 0.04975282400846481\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.6261606216430664\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.045977700501680374\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 4.755030632019043\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.04178900644183159\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 4.546088695526123\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.04009909927845001\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 4.567060947418213\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.05675176531076431\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.161961078643799\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03664521872997284\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.018723776564002037\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00030289991991594434\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02431318908929825\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003332679916638881\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.017423955723643303\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0002840081579051912\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.020805170759558678\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00026973115745931864\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.0473734550178051\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0004587967705447227\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014995675534009933\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00042398436926305294\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04384861886501312\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00038535826024599373\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.041921861469745636\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003697747306432575\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04211525246500969\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005233376869000494\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01993657276034355\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0003379246627446264\n",
      "Are there any dead codes on this epoch?  233\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(2.1196, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.2131280899047852\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.02172737382352352\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.6969976425170898\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.024812903255224228\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.2136917114257812\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.022568196058273315\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.4273570775985718\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.02152499556541443\n",
      "After scaling - encoder.proj.weight: grad norm 3.2715940475463867\n",
      "After scaling - encoder.proj.bias: grad norm 0.03754575178027153\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.066908359527588\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03391515091061592\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.239840507507324\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.02978372387588024\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.862076997756958\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.026820478960871696\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.714428186416626\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.037720367312431335\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.3634023666381836\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.028398867696523666\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.017578428611159325\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003148332762066275\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02458977699279785\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00035954310442321\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.017586594447493553\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00032701692543923855\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.020682640373706818\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.000311900774249807\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.047405946999788284\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005440442473627627\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.015459679998457432\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004914362216368318\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04694582521915436\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004315711266826838\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.041471969336271286\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003886332269757986\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.039332516491413116\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005465744761750102\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.019755927845835686\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004115043266210705\n",
      "epoch: 50 ( 1 ) recon_loss: 0.9645387530326843  perplexity:  147.01173400878906  commit_loss:  0.2732559144496918 \n",
      "\t codebook loss:  1.093023657798767  total_loss:  2.119645357131958 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(2.1206, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.402327299118042\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.028562694787979126\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.8951066732406616\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.031419865787029266\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.4250041246414185\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.028126243501901627\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.809727430343628\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.028963584452867508\n",
      "After scaling - encoder.proj.weight: grad norm 3.800675392150879\n",
      "After scaling - encoder.proj.bias: grad norm 0.04906989261507988\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.1021192073822021\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03664170205593109\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.1786458492279053\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.030470099300146103\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.9929277896881104\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.02770201489329338\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.475381851196289\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04730280861258507\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.727584719657898\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.033162351697683334\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.018005214631557465\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00036673140130005777\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.024332264438271523\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00040341608109883964\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.0182963740080595\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00036112754605710506\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02323603630065918\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003718786174431443\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04879885911941528\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006300339591689408\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01415068469941616\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004704620223492384\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04081229493021965\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00039122154703363776\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03842776641249657\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.000355680676875636\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04462224617600441\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006073454860597849\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.02218136563897133\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00042578877764754\n",
      "Are there any dead codes on this epoch?  238\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(2.0703, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.1110804080963135\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.0172685869038105\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.5769431591033936\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.018206210806965828\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.0685267448425293\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.01677008345723152\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.335727572441101\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.017222240567207336\n",
      "After scaling - encoder.proj.weight: grad norm 2.900280475616455\n",
      "After scaling - encoder.proj.bias: grad norm 0.029125990346074104\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.7684840559959412\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.021209150552749634\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.1753487586975098\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.017766602337360382\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.0215110778808594\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.017156856134533882\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.209029197692871\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.029612164944410324\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.1428828239440918\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.021100955083966255\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02006644383072853\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003118758031632751\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.028480064123868942\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003288096049800515\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01929791271686554\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003028727078344673\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02412363886833191\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003110388061031699\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05237992852926254\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005260241450741887\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013879052363336086\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0003830435744021088\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03928745165467262\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003208701382391155\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03650909289717674\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003098579472862184\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03989572823047638\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005348045378923416\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.02064080536365509\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0003810895432252437\n",
      "epoch: 51 ( 1 ) recon_loss: 0.9281378388404846  perplexity:  149.3590850830078  commit_loss:  0.27021482586860657 \n",
      "\t codebook loss:  1.0808593034744263  total_loss:  2.0702550411224365 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(2.1165, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.533583164215088\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03272554278373718\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.113969326019287\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.03476680815219879\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.6910418272018433\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.031977344304323196\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.1731834411621094\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.03358985111117363\n",
      "After scaling - encoder.proj.weight: grad norm 4.429912090301514\n",
      "After scaling - encoder.proj.bias: grad norm 0.05818122252821922\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.1917970180511475\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.039299510419368744\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.422214984893799\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.03134425729513168\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.308490753173828\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.029994940385222435\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.7388381958007812\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.05290720984339714\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.7358460426330566\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03757726773619652\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.017694992944598198\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003775982477236539\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02439168095588684\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004011510463897139\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01951180398464203\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003689652367029339\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02507491409778595\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003875708789564669\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.051113806664943695\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006713143084198236\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013751351274549961\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.000453450862551108\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.039486657828092575\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00036166052450425923\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03817446902394295\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00034609163412824273\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04313996061682701\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006104610511101782\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.020028769969940186\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00043357902904972434\n",
      "Are there any dead codes on this epoch?  231\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(2.0355, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 0.9871787428855896\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.01739887148141861\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.3393946886062622\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.020279280841350555\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 0.939015805721283\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.017828676849603653\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.1265114545822144\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.01762799546122551\n",
      "After scaling - encoder.proj.weight: grad norm 2.5439071655273438\n",
      "After scaling - encoder.proj.bias: grad norm 0.030124465003609657\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.7886061668395996\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.022637709975242615\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.22755765914917\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.018744656816124916\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.0930256843566895\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.01770767755806446\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.4877350330352783\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.0322909876704216\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.2711337804794312\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.023687491193413734\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.018279502168297768\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.000322173407766968\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.024801453575491905\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00037550966953858733\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.017387673258781433\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00033013205393217504\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02085951343178749\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003264160477556288\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04710530489683151\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005578121053986251\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01460255216807127\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00041918052011169493\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.041247494518756866\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003470931842457503\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03875637799501419\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00032789152464829385\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04606517404317856\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005979294655844569\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.023537473753094673\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.000438619201304391\n",
      "epoch: 52 ( 1 ) recon_loss: 0.8869248628616333  perplexity:  149.20668029785156  commit_loss:  0.2717621624469757 \n",
      "\t codebook loss:  1.0870486497879028  total_loss:  2.0354502201080322 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(2.0565, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.5735706090927124\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.041997261345386505\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.0780670642852783\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.04382183402776718\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.7594032287597656\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.04033423587679863\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.3042590618133545\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.04267403110861778\n",
      "After scaling - encoder.proj.weight: grad norm 4.610239028930664\n",
      "After scaling - encoder.proj.bias: grad norm 0.07204356789588928\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.2751953601837158\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.049406424164772034\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.580925226211548\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.03982852026820183\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.60284686088562\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.038216207176446915\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 4.236561298370361\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.06653434783220291\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.8708170652389526\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.04570470005273819\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.017032109200954437\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004545725241769105\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.022492706775665283\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00047432142309844494\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019043534994125366\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00043657212518155575\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02494097873568535\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004618977545760572\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04990058392286301\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007797895232215524\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013802537694573402\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005347682745195925\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03875943645834923\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00043109836406074464\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03899671137332916\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0004136468924116343\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04585595056414604\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007201585685834289\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.020249461755156517\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004947013221681118\n",
      "Are there any dead codes on this epoch?  228\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(2.0647, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.1311991214752197\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.021713288500905037\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.5901706218719482\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.025682944804430008\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.194430947303772\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.022557804360985756\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.4910249710083008\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.022620119154453278\n",
      "After scaling - encoder.proj.weight: grad norm 3.102573871612549\n",
      "After scaling - encoder.proj.bias: grad norm 0.038504309952259064\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.9030890464782715\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.026048114523291588\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.6157968044281006\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.021765967831015587\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.4569497108459473\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.020784839987754822\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.0031912326812744\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.03989483416080475\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.460443377494812\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.028654638677835464\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.017467617988586426\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00033528971835039556\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.024554912000894547\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00039658788591623306\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01844402402639389\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00034833047538995743\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.0230239350348711\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00034929270623251796\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.0479089617729187\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005945713492110372\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013945215381681919\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00040222672396339476\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.040392305701971054\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00033610317041166127\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.037939440459012985\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00032095290953293443\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04637432470917702\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006160433404147625\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.022551706060767174\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004424758371897042\n",
      "epoch: 53 ( 1 ) recon_loss: 0.8816285133361816  perplexity:  151.55960083007812  commit_loss:  0.2799619436264038 \n",
      "\t codebook loss:  1.1198477745056152  total_loss:  2.0646748542785645 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.9959, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.2721582651138306\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03163006901741028\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.711909294128418\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.032790426164865494\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.4385000467300415\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.030301567167043686\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.917830228805542\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.032406244426965714\n",
      "After scaling - encoder.proj.weight: grad norm 3.8147006034851074\n",
      "After scaling - encoder.proj.bias: grad norm 0.055388785898685455\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.0400043725967407\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03793109208345413\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.9389700889587402\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.03034025803208351\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.9887423515319824\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.030191607773303986\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.742065668106079\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.056518618017435074\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.6888567209243774\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03946863114833832\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01637301966547966\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004070875293109566\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.022032734006643295\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00042202166514471173\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.018513884395360947\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00038998934905976057\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.024682993069291115\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004170771280769259\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04909622669219971\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007128685829229653\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013385136611759663\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00048818340292200446\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.037825338542461395\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003904873155988753\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.038465920835733414\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00038857414619997144\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04816139489412308\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007274099043570459\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.021736042574048042\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005079719703644514\n",
      "Are there any dead codes on this epoch?  225\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.9852, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.1596698760986328\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.023537740111351013\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.5773112773895264\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.026780806481838226\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.193637490272522\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02393726073205471\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.4901043176651\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.02438863180577755\n",
      "After scaling - encoder.proj.weight: grad norm 3.1010055541992188\n",
      "After scaling - encoder.proj.bias: grad norm 0.041592370718717575\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.8894698619842529\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.027906274423003197\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.541109085083008\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.02300105430185795\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.516089916229248\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.02254696562886238\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.075782537460327\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04384693130850792\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.496766448020935\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.032556794583797455\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01781306229531765\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003615505120251328\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.024228226393461227\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004113655013497919\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.018334820866584778\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00036768734571523964\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.022888686507940292\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00037462060572579503\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04763286933302879\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006388779147528112\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013662665151059628\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00042865320574492216\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03903260454535484\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003533067647367716\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.038648299872875214\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00034633176983334124\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.047245435416698456\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006735090282745659\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.02299102023243904\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005000873352400959\n",
      "epoch: 54 ( 1 ) recon_loss: 0.8585494756698608  perplexity:  150.98829650878906  commit_loss:  0.26663413643836975 \n",
      "\t codebook loss:  1.066536545753479  total_loss:  1.985151767730713 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.9928, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.5665464401245117\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.026956237852573395\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.192263126373291\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.02788987010717392\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.7492564916610718\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.025498710572719574\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.4245119094848633\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.026806794106960297\n",
      "After scaling - encoder.proj.weight: grad norm 5.049802303314209\n",
      "After scaling - encoder.proj.bias: grad norm 0.04543933644890785\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.187118411064148\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03317396342754364\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.2673439979553223\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.02670825645327568\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.08251690864563\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.025739191100001335\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.5714898109436035\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.048491526395082474\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.6943175792694092\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.033272635191679\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.017633667215704918\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003034300752915442\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.024676982313394547\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003139394102618098\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01969032548367977\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00028702354757115245\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02729126438498497\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00030174790299497545\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05684257671236992\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005114831728860736\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013362674973905087\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0003734192287083715\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.036778517067432404\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003006386978086084\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03469803184270859\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0002897305239457637\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.040202103555202484\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005458397790789604\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.0190719086676836\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0003745299472939223\n",
      "Are there any dead codes on this epoch?  228\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(2.0181, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.4671353101730347\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.036423489451408386\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.043973445892334\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.039017241448163986\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.6889288425445557\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.03542553260922432\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.267645835876465\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.037429481744766235\n",
      "After scaling - encoder.proj.weight: grad norm 4.442081451416016\n",
      "After scaling - encoder.proj.bias: grad norm 0.06376189738512039\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.0733811855316162\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.039179567247629166\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.05666184425354\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.0317041277885437\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.253690004348755\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.03189193084836006\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 4.0303120613098145\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.06161243095993996\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.8559130430221558\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.04338116571307182\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.016958486288785934\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00042101589497178793\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.023626109585165977\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004509968275669962\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019522178918123245\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.000409480620874092\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.026211518794298172\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00043264406849630177\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05134562402963638\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.000737018184736371\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.012407117523252964\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004528731806203723\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03533168509602547\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003664652176667005\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03760911524295807\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00036863601417280734\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04658602178096771\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.000712172593921423\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.02145233564078808\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005014390917494893\n",
      "epoch: 55 ( 1 ) recon_loss: 0.8663261532783508  perplexity:  154.79672241210938  commit_loss:  0.2726272642612457 \n",
      "\t codebook loss:  1.090509057044983  total_loss:  2.0181267261505127 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.9003, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.5401897430419922\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.019196240231394768\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.1024415493011475\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.02197195030748844\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.5329264402389526\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.019077565521001816\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.9456019401550293\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.01798270083963871\n",
      "After scaling - encoder.proj.weight: grad norm 4.265685081481934\n",
      "After scaling - encoder.proj.bias: grad norm 0.031204143539071083\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.1309869289398193\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.025199830532073975\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.132535457611084\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.02096107229590416\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.8185367584228516\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.018949510529637337\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.015875816345215\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.031656596809625626\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.613564372062683\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.023182867094874382\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.019581854343414307\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00024405954172834754\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.026730284094810486\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.000279349711490795\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019489509984850883\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0002425507118459791\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.024736234918236732\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00022863069898448884\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05423359200358391\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0003967270895373076\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014379281550645828\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0003203887608833611\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03982681781053543\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00026649748906493187\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.0358346626162529\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0002409226435702294\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03834361582994461\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0004024796071462333\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.02051473595201969\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0002947452012449503\n",
      "Are there any dead codes on this epoch?  225\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.9770, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.3651845455169678\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.02731994166970253\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.8398518562316895\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.029618438333272934\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.3532917499542236\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.026314714923501015\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.6981784105300903\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.02648896351456642\n",
      "After scaling - encoder.proj.weight: grad norm 3.6215827465057373\n",
      "After scaling - encoder.proj.bias: grad norm 0.04618077352643013\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.1131635904312134\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03346915543079376\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.2999095916748047\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.02777411974966526\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.136483907699585\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.027177052572369576\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.5320208072662354\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04738067090511322\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.6574333906173706\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03384210169315338\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.017583569511771202\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003518807061482221\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02369728311896324\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003814853262156248\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.017430391162633896\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003389334015082568\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02187252789735794\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00034117771429009736\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04664596542716026\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005948081961832941\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014337540604174137\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004310825315769762\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04250281676650047\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00035773051786236465\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.040397897362709045\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003500402963254601\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04549240693449974\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.000610262795817107\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.021347733214497566\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00043588606058619916\n",
      "epoch: 56 ( 1 ) recon_loss: 0.824743390083313  perplexity:  155.6570281982422  commit_loss:  0.2727726399898529 \n",
      "\t codebook loss:  1.0910905599594116  total_loss:  1.9770362377166748 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.9463, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.9844876527786255\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.018134066835045815\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.871387481689453\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.02040025033056736\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.1097545623779297\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.019015220925211906\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.887748956680298\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.021054349839687347\n",
      "After scaling - encoder.proj.weight: grad norm 6.039249897003174\n",
      "After scaling - encoder.proj.bias: grad norm 0.03645757958292961\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.2651431560516357\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.02297365292906761\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.438633918762207\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.019105862826108932\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.065716028213501\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.01768168993294239\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.2722654342651367\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.031350817531347275\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.6670522689819336\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.02337440848350525\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02007078379392624\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00018340499082114547\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02904074266552925\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00020632479572668672\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.021337712183594704\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00019231683108955622\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02920622006058693\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00021294024190865457\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.06107998266816139\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.00036872600321657956\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.012795450165867805\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0002323517546756193\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03477777913212776\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00019323357264511287\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03100614994764328\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00017882972315419465\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03309515491127968\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.00031707706511951983\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.016860293224453926\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00023640492872800678\n",
      "Are there any dead codes on this epoch?  225\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(2.0117, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.4665813446044922\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.027285518124699593\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.9658602476119995\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.029711198061704636\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.4293756484985352\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02621966227889061\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.7828477621078491\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.02605389431118965\n",
      "After scaling - encoder.proj.weight: grad norm 3.760226249694824\n",
      "After scaling - encoder.proj.bias: grad norm 0.04431600123643875\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.1631094217300415\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.033860865980386734\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.3330905437469482\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.028529148548841476\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.275212287902832\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.027842748910188675\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.559725522994995\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04517325386404991\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.7261861562728882\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03232722729444504\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.018299710005521774\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.000340463244356215\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02452961355447769\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00037073041312396526\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01783546432852745\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003271637251600623\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.022246019914746284\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00032509531592950225\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04691935330629349\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005529662012122571\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014513048343360424\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00042250912520103157\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.0415896438062191\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00035598105750977993\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04086744785308838\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00034741629497148097\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.044417548924684525\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005636628484353423\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.021539008244872093\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004033727163914591\n",
      "epoch: 57 ( 1 ) recon_loss: 0.8211191892623901  perplexity:  156.1021270751953  commit_loss:  0.281875878572464 \n",
      "\t codebook loss:  1.127503514289856  total_loss:  2.0117440223693848 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.8333, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.490998387336731\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.02410126104950905\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.1219964027404785\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.026721864938735962\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.585357427597046\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.023927947506308556\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.099339723587036\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.023909414187073708\n",
      "After scaling - encoder.proj.weight: grad norm 4.416953086853027\n",
      "After scaling - encoder.proj.bias: grad norm 0.0401032492518425\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.1587936878204346\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.032184068113565445\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.186561346054077\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.0262394230812788\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.103076457977295\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.0255217757076025\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.362866163253784\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04223160818219185\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.6623188257217407\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.030072730034589767\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.018032899126410484\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0002914929937105626\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02566451020538807\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00032318790908902884\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.0191741231828928\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00028939684852957726\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.025390489026904106\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0002891726908273995\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.053420890122652054\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0004850292461924255\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014015043154358864\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00038925057742744684\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03853990137577057\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003173529985360801\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03753019496798515\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003086733922827989\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04067222401499748\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005107706529088318\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.02010493353009224\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0003637149930000305\n",
      "Are there any dead codes on this epoch?  224\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.9119, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.3222899436950684\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.01592176780104637\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.870686650276184\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.018961157649755478\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.2256429195404053\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.01654469221830368\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.4073175191879272\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.0147557333111763\n",
      "After scaling - encoder.proj.weight: grad norm 3.1884238719940186\n",
      "After scaling - encoder.proj.bias: grad norm 0.02518359199166298\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.0329688787460327\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.02229919470846653\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.98093843460083\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.019238712266087532\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.690800428390503\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.018453655764460564\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.726802349090576\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.028190715238451958\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.4213907718658447\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.020277950912714005\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.019592147320508957\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00023591013450641185\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.027717646211385727\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.000280944281257689\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.018160143867135048\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00024513990501873195\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02085198648273945\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0002186332130804658\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.047242335975170135\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0003731410251930356\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.015305324457585812\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00033040338894352317\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04416806250810623\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00028505673981271684\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03986913710832596\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00027342469547875226\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04040256887674332\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.00041769706876948476\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.021060505881905556\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00030045496532693505\n",
      "epoch: 58 ( 1 ) recon_loss: 0.779050886631012  perplexity:  156.97305297851562  commit_loss:  0.26822322607040405 \n",
      "\t codebook loss:  1.0728929042816162  total_loss:  1.9118924140930176 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.8196, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.389053225517273\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.02240859344601631\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.9966100454330444\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.024740083143115044\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.410684585571289\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02156461402773857\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.809739589691162\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.02040991373360157\n",
      "After scaling - encoder.proj.weight: grad norm 4.008556365966797\n",
      "After scaling - encoder.proj.bias: grad norm 0.03432217612862587\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.144877314567566\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03034159727394581\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.224079132080078\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.025439633056521416\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.008183717727661\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.024190036579966545\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.0471889972686768\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.03655455261468887\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.5099605321884155\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.026872416958212852\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.017996152862906456\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00029031894518993795\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02586747705936432\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00032052502501755953\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.018276402726769447\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0002793846360873431\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02344643883407116\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0002644246560521424\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.051933642476797104\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.00044466773397289217\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014832683838903904\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0003930965322069824\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04177019000053406\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00032958819065243006\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.038973119109869\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00031339877750724554\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03947845846414566\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0004735897236969322\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01956259272992611\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0003481509047560394\n",
      "Are there any dead codes on this epoch?  223\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.9060, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.0939340591430664\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.014517039991915226\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.548218846321106\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.016700103878974915\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.068727731704712\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.014568690210580826\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.3226479291915894\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.014309098944067955\n",
      "After scaling - encoder.proj.weight: grad norm 2.7497775554656982\n",
      "After scaling - encoder.proj.bias: grad norm 0.023378638550639153\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.8547842502593994\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.019382735714316368\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.55265736579895\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.016969753429293633\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.350680351257324\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.01616724021732807\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.610659599304199\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.02845955453813076\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.3048440217971802\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.021005267277359962\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.018401503562927246\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0002441969409119338\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.026043212041258812\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00028091916465200484\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.017977498471736908\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00024506577756255865\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02224879153072834\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00024069911160040647\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04625511169433594\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0003932614636141807\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014378668740391731\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00032604477019049227\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04293927177786827\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0002854549966286868\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03954173997044563\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0002719556214287877\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.043914955109357834\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0004787295765709132\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.02194930426776409\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00035333799314685166\n",
      "epoch: 59 ( 1 ) recon_loss: 0.7553921937942505  perplexity:  157.4200897216797  commit_loss:  0.2724548876285553 \n",
      "\t codebook loss:  1.0898195505142212  total_loss:  1.9059913158416748 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.7691, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.7603929042816162\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.02558445930480957\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.459467649459839\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.029178299009799957\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.6700184345245361\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.025359483435750008\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.9330703020095825\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.023300237953662872\n",
      "After scaling - encoder.proj.weight: grad norm 4.3452887535095215\n",
      "After scaling - encoder.proj.bias: grad norm 0.038551215082407\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.4419416189193726\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03552054241299629\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 4.134328365325928\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.03096742369234562\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.7169172763824463\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.02842915803194046\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.4044859409332275\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.040434062480926514\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.7676596641540527\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.027728337794542313\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.019408676773309708\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.000282073684502393\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.027116110548377037\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00032169645419344306\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.018412280827760696\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0002795932814478874\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.021312478929758072\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.000256889674346894\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04790766164660454\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.00042503472650423646\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.015897689387202263\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.000391620968002826\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04558178037405014\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00034142189542762935\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.040979743003845215\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00031343704904429615\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03753512352705002\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0004457934701349586\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01948879286646843\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0003057103604078293\n",
      "Are there any dead codes on this epoch?  222\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.9282, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 0.9657179713249207\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.02250657230615616\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.2897814512252808\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.023512190207839012\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.0103375911712646\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.020654773339629173\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.3290129899978638\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.021922782063484192\n",
      "After scaling - encoder.proj.weight: grad norm 2.7311391830444336\n",
      "After scaling - encoder.proj.bias: grad norm 0.03805740550160408\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.7873766422271729\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.02648685686290264\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.4805846214294434\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.02397909387946129\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.350355863571167\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.022198688238859177\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.634115219116211\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.03749440982937813\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.2636431455612183\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.027399837970733643\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01663981005549431\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00038779963506385684\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.022223588079214096\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00040512694977223873\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01740862801671028\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.000355892232619226\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.0228995680809021\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003777406527660787\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04705891013145447\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006557484157383442\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013566898182034492\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00045638196752406657\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04274172708392143\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00041317197610624135\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04049782082438469\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00038249464705586433\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.045387137681245804\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006460477015934885\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.021773211658000946\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004721130826510489\n",
      "epoch: 60 ( 1 ) recon_loss: 0.738911509513855  perplexity:  160.45767211914062  commit_loss:  0.2816479802131653 \n",
      "\t codebook loss:  1.1265919208526611  total_loss:  1.9282172918319702 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.7473, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.6205377578735352\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.02746252901852131\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.247011423110962\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.030165867879986763\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.5072987079620361\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.024908555671572685\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.7110768556594849\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.02257862500846386\n",
      "After scaling - encoder.proj.weight: grad norm 3.9416956901550293\n",
      "After scaling - encoder.proj.bias: grad norm 0.03836847096681595\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.4503791332244873\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.04285066947340965\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 4.447325229644775\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.0401817224919796\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 4.147245407104492\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.038155894726514816\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.6091597080230713\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.05130660906434059\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.737485647201538\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03684578835964203\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.017728867009282112\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00030044320737943053\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02458256110548973\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00033001802512444556\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.016490017995238304\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0002725024241954088\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.018719375133514404\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0002470127074047923\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04312259703874588\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.00041975537897087634\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.015867311507463455\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00046879114233888686\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04865423962473869\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004395925789140165\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04537133499979973\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00041742980829440057\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03948461636900902\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005613000830635428\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.019008290022611618\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00040309710311703384\n",
      "Are there any dead codes on this epoch?  215\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.8843, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.3662233352661133\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03301159664988518\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.8504812717437744\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.03647080808877945\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.49556303024292\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.03275123983621597\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.9097321033477783\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.03387533873319626\n",
      "After scaling - encoder.proj.weight: grad norm 3.9013073444366455\n",
      "After scaling - encoder.proj.bias: grad norm 0.0570390410721302\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.0641013383865356\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03853371739387512\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.0820677280426025\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.03216627612709999\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.0686323642730713\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.03056807816028595\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.6021676063537598\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.054306354373693466\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.6955711841583252\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03998027369379997\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.017307940870523453\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00041820589103735983\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.023442739620804787\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00046202880912460387\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01894647255539894\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0004149076121393591\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.024193355813622475\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004291482036933303\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04942353442311287\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007225965382531285\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013480519875884056\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004881627100985497\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03904503583908081\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004074970493093133\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.038874827325344086\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00038725032936781645\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04563389718532562\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006879776483401656\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.021480267867445946\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005064882570877671\n",
      "epoch: 61 ( 1 ) recon_loss: 0.7530837655067444  perplexity:  158.9220428466797  commit_loss:  0.2679252624511719 \n",
      "\t codebook loss:  1.0717010498046875  total_loss:  1.8843342065811157 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.6832, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.053102731704712\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.021848978474736214\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.46517813205719\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.024204034358263016\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.0490323305130005\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.020739078521728516\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.2597296237945557\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.020166343078017235\n",
      "After scaling - encoder.proj.weight: grad norm 2.7544302940368652\n",
      "After scaling - encoder.proj.bias: grad norm 0.034348610788583755\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.873629093170166\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.027121657505631447\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.501283645629883\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.022619176656007767\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.4803550243377686\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.023068144917488098\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.5115885734558105\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.03713446483016014\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.1733384132385254\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.024582121521234512\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.017956946045160294\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003725571441464126\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.024983439594507217\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004127143183723092\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01788754016160965\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00035363173810765147\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.021480238065123558\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00034386577317491174\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04696707800030708\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005856942152604461\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014896659180521965\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004624640860129148\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.042650558054447174\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003856901603285223\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04229369014501572\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003933457483071834\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.042826272547245026\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006331971380859613\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.020007140934467316\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00041916128247976303\n",
      "Are there any dead codes on this epoch?  208\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.8498, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.2515203952789307\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.026340121403336525\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.7216240167617798\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.029326915740966797\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.3464629650115967\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.026797259226441383\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.7005094289779663\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.028598306700587273\n",
      "After scaling - encoder.proj.weight: grad norm 3.4064784049987793\n",
      "After scaling - encoder.proj.bias: grad norm 0.04829807206988335\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.9081776738166809\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.029585111886262894\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.6490230560302734\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.024306396022439003\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.4854366779327393\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.022282855585217476\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.9831879138946533\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04439399391412735\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.4596370458602905\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.033627819269895554\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01847294718027115\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003887908242177218\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02541186474263668\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00043287716107442975\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019874336197972298\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003955383726861328\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.025100205093622208\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00042212256812490523\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05028099939227104\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.000712899025529623\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013405068777501583\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00043668822036124766\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03910064697265625\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003587722312659025\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03668605163693428\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00032890395959839225\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04403305798768997\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006552732666023076\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.02154483087360859\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004963601822964847\n",
      "epoch: 62 ( 1 ) recon_loss: 0.7181553840637207  perplexity:  164.84776306152344  commit_loss:  0.26803699135780334 \n",
      "\t codebook loss:  1.0721479654312134  total_loss:  1.8497706651687622 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.6655, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.2024877071380615\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.022209729999303818\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.6583483219146729\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.025026358664035797\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.2215064764022827\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.021473409608006477\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.570769190788269\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.02100318670272827\n",
      "After scaling - encoder.proj.weight: grad norm 3.4106295108795166\n",
      "After scaling - encoder.proj.bias: grad norm 0.03668690100312233\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.9732839465141296\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03029709681868553\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.847322940826416\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.0268531646579504\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.8068618774414062\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.02576727606356144\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.928133964538574\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.03957587108016014\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.4308735132217407\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.02650669775903225\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.017475100234150887\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003227619163226336\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02409987524151802\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.000363694503903389\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01775149069726467\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003120613982900977\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.022827135398983955\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.000305227906210348\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04956482723355293\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.00053315085824579\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014144206419587135\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004402912745717913\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04137859493494034\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003902424650732428\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04079060256481171\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003744618152268231\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04255298152565956\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005751346470788121\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.020794108510017395\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00038520744419656694\n",
      "Are there any dead codes on this epoch?  205\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.8163, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.2751911878585815\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.026109179481863976\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.6987042427062988\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.026940425857901573\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.3202406167984009\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02384183742105961\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.6861929893493652\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.025393495336174965\n",
      "After scaling - encoder.proj.weight: grad norm 3.413240909576416\n",
      "After scaling - encoder.proj.bias: grad norm 0.0444457083940506\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.9497301578521729\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.030325261875987053\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.83884859085083\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.027497736737132072\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.012831687927246\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.026219112798571587\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.3668363094329834\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.046503473073244095\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.5322773456573486\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03257537633180618\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.017602654173970222\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003604093217290938\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.023448798805475235\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003718838270287961\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01822451315820217\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003291110915597528\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02327609434723854\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00035053008468821645\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04711614549160004\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006135255098342896\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013110009953379631\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00041860787314362824\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.039187271147966385\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00037957687163725495\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.0415889210999012\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003619268536567688\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04647558182477951\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006419307901524007\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.021151453256607056\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004496682377066463\n",
      "epoch: 63 ( 1 ) recon_loss: 0.7044259309768677  perplexity:  163.26451110839844  commit_loss:  0.2633925974369049 \n",
      "\t codebook loss:  1.0535703897476196  total_loss:  1.816330075263977 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.6561, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.2585281133651733\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.027399564161896706\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.6923381090164185\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.027359092608094215\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.376623272895813\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.025587916374206543\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.794281005859375\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.027238603681325912\n",
      "After scaling - encoder.proj.weight: grad norm 3.493473529815674\n",
      "After scaling - encoder.proj.bias: grad norm 0.04710059612989426\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.9641830325126648\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03296064957976341\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.8680272102355957\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.029259975999593735\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.8878066539764404\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.02769378572702408\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.1512796878814697\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04504572972655296\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.4155042171478271\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03150709345936775\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.017587345093488693\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00038289616350084543\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.023649638518691063\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00038233055965974927\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019237669184803963\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00035757923615165055\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.025074241682887077\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00038064681575633585\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04881966486573219\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006582089699804783\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01347400899976492\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00046060970635153353\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04007934778928757\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004088945279363543\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.040355756878852844\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00038700777804479003\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04403766617178917\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006294930353760719\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01978101022541523\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00044029694981873035\n",
      "Are there any dead codes on this epoch?  210\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.8181, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.0004360675811768\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.02152116410434246\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.3411787748336792\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.0225664135068655\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.0303285121917725\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.020108947530388832\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.3455238342285156\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.02053619921207428\n",
      "After scaling - encoder.proj.weight: grad norm 2.774125337600708\n",
      "After scaling - encoder.proj.bias: grad norm 0.03300466388463974\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.8316002488136292\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.026754507794976234\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.4337642192840576\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.02295895479619503\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.344055652618408\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.02100258693099022\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.717881679534912\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.03728795796632767\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.2489347457885742\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.027172381058335304\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01704980805516243\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003667717392090708\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02285687066614628\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003845853207167238\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.017559247091412544\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00034270426840521395\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.022930923849344254\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003499856102280319\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04727768898010254\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005624779150821269\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014172444120049477\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004559603985399008\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04147712513804436\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00039127515628933907\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03994827717542648\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00035793401184491813\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04631916061043739\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006354753859341145\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.021284814924001694\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004630819312296808\n",
      "epoch: 64 ( 1 ) recon_loss: 0.6768925786018372  perplexity:  165.59107971191406  commit_loss:  0.27035853266716003 \n",
      "\t codebook loss:  1.0814341306686401  total_loss:  1.8180991411209106 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.6164, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.1268296241760254\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.02143002673983574\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.525719165802002\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.02214687503874302\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.1801867485046387\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.020097114145755768\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.517217993736267\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.020786484703421593\n",
      "After scaling - encoder.proj.weight: grad norm 3.0760788917541504\n",
      "After scaling - encoder.proj.bias: grad norm 0.03576892241835594\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.801500678062439\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.02330569364130497\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.2739920616149902\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.01903047040104866\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.228524923324585\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.018727948889136314\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.7105536460876465\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.03576929494738579\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.3289679288864136\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.02625553123652935\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.0186122115701437\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.000353966694092378\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.025200797244906425\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003658071509562433\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019493525847792625\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00033195054857060313\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02506038174033165\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00034333710209466517\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05080859363079071\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005908068851567805\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013238647021353245\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0003849476925097406\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03756026178598404\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.000314332457492128\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03680926933884621\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003093356208410114\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.044771090149879456\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005908129969611764\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.021950993686914444\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00043367111356928945\n",
      "Are there any dead codes on this epoch?  207\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.7529, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.3883744478225708\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.028224587440490723\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.8622573614120483\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.029962796717882156\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.4059562683105469\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.026806101202964783\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.7546398639678955\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.02732004038989544\n",
      "After scaling - encoder.proj.weight: grad norm 3.6314361095428467\n",
      "After scaling - encoder.proj.bias: grad norm 0.044877152889966965\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.1908255815505981\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.039000947028398514\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.5522661209106445\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.03673933818936348\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.3195788860321045\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.03205389901995659\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.0005054473876953\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.043215688318014145\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.4190958738327026\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.0292506143450737\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.017965305596590042\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003652209124993533\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02409726195037365\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00038771299296058714\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01819281093776226\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003468659706413746\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.0227047111839056\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003535162250045687\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.046990104019641876\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005807020352222025\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.015409060753881931\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005046649603173137\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.045965664088726044\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004754002147819847\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04295473173260689\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0004147714644204825\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03882598131895065\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005592029192484915\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01836283504962921\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00037849749787710607\n",
      "epoch: 65 ( 1 ) recon_loss: 0.6749815940856934  perplexity:  164.74986267089844  commit_loss:  0.25539731979370117 \n",
      "\t codebook loss:  1.0215892791748047  total_loss:  1.752938985824585 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.6290, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.4485448598861694\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.02872874215245247\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.034559965133667\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.03081832453608513\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.5710784196853638\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02695411816239357\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.0859148502349854\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.026180382817983627\n",
      "After scaling - encoder.proj.weight: grad norm 4.358602523803711\n",
      "After scaling - encoder.proj.bias: grad norm 0.044039785861968994\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.319283366203308\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.04453398659825325\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 4.023008823394775\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.04236975684762001\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.898902654647827\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.03823155537247658\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.6416141986846924\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.05258481949567795\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.652753472328186\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03602821007370949\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.016076257452368736\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.000318837643135339\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.0225799772888422\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00034202830283902586\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.017436161637306213\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0002991424989886582\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.023149924352765083\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00029055544291622937\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04837269335985184\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.000488762860186398\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014641685411334038\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004942476516589522\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04464820399880409\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.000470228522317484\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04327084869146347\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0004243018920533359\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.040415406227111816\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005835974588990211\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018342606723308563\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00039984870818443596\n",
      "Are there any dead codes on this epoch?  204\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.7847, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.0093122720718384\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.024980811402201653\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.3389828205108643\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.025875762104988098\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.0669310092926025\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.022202571853995323\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.464778184890747\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.024032892659306526\n",
      "After scaling - encoder.proj.weight: grad norm 2.8758461475372314\n",
      "After scaling - encoder.proj.bias: grad norm 0.041596442461013794\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.6915072798728943\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.02520430088043213\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.0100347995758057\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.020418085157871246\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.0303843021392822\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.01945633254945278\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.4077861309051514\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.03693898394703865\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.1259541511535645\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.028107119724154472\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.018402986228466034\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.000455479952506721\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.024413932114839554\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00047179777175188065\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019453559070825577\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0004048237460665405\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.026707584038376808\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004381963808555156\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05243585631251335\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.000758435926400125\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.012608385644853115\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00045955489622429013\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03664935380220413\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003722868859767914\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03702039271593094\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00035475107142701745\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04390162602066994\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006735156639479101\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.020529739558696747\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005124825402162969\n",
      "epoch: 66 ( 1 ) recon_loss: 0.641498863697052  perplexity:  165.01780700683594  commit_loss:  0.2708885669708252 \n",
      "\t codebook loss:  1.0835542678833008  total_loss:  1.7847394943237305 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.6267, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.6561545133590698\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.039345744997262955\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.166015625\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.038982950150966644\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.7212893962860107\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.03372557833790779\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.199390172958374\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.0353090837597847\n",
      "After scaling - encoder.proj.weight: grad norm 4.357952117919922\n",
      "After scaling - encoder.proj.bias: grad norm 0.058343950659036636\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.3330687284469604\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.04742337018251419\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.8846144676208496\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.04206883907318115\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 4.049944877624512\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.03996637091040611\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.9363317489624023\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.06026829406619072\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.693411946296692\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.04070888087153435\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.017871469259262085\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00042457770905457437\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.023373350501060486\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00042066280730068684\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.018574336543679237\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003639307979028672\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02373349480330944\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00038101832615211606\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.047026414424180984\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006295862258411944\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014385069720447063\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005117428954690695\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04191865399479866\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00045396244968287647\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04370272904634476\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0004312748496886343\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04247673228383064\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.000650351750664413\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01827351190149784\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004392872506286949\n",
      "Are there any dead codes on this epoch?  199\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.7995, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.3969228267669678\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.02972094900906086\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.8786709308624268\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.03157057240605354\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.4389209747314453\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02825373224914074\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.7653437852859497\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.028909478336572647\n",
      "After scaling - encoder.proj.weight: grad norm 3.737739086151123\n",
      "After scaling - encoder.proj.bias: grad norm 0.0512080043554306\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.0711185932159424\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03448151424527168\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.0198545455932617\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.028617169708013535\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.8715717792510986\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.02629733458161354\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.2894539833068848\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04969620704650879\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.5812078714370728\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03644336760044098\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01858692802488804\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003954556887038052\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.024996886029839516\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004200660914648324\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01914573647081852\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003759334795176983\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.023488998413085938\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003846585750579834\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04973294585943222\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.000681354315020144\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014251899905502796\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004587980220094323\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04018104076385498\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003807692264672369\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03820804879069328\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00034990240237675607\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04376823082566261\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006612389697693288\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.021038955077528954\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00048490165499970317\n",
      "epoch: 67 ( 1 ) recon_loss: 0.6600363254547119  perplexity:  168.98086547851562  commit_loss:  0.27002036571502686 \n",
      "\t codebook loss:  1.0800814628601074  total_loss:  1.7995140552520752 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.5383, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 0.7768150568008423\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.01853022165596485\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.0079485177993774\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.018884936347603798\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 0.7898966073989868\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.016591984778642654\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.0282648801803589\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.017495639622211456\n",
      "After scaling - encoder.proj.weight: grad norm 2.032228946685791\n",
      "After scaling - encoder.proj.bias: grad norm 0.028059503063559532\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.6938653588294983\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.024880215525627136\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 1.97898530960083\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.020928766578435898\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.037235975265503\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.02007509209215641\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.2002384662628174\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.0320117212831974\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.0427770614624023\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.022420968860387802\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.016594868153333664\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003958556044381112\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02153250388801098\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00040343331056647\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.016874326393008232\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00035444964305497706\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.021966518834233284\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003737541555892676\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.043413907289505005\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.000599426799453795\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014822839759290218\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005315086455084383\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.042276475578546524\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00044709505164064467\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04352086782455444\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00042885824223048985\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.047003041952848434\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006838569534011185\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.022276539355516434\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00047897256445139647\n",
      "Are there any dead codes on this epoch?  200\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.7870, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.2918970584869385\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.05421962961554527\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 3.035736083984375\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.05647663399577141\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.407339334487915\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.04959362372756004\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 3.1477413177490234\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.052908603101968765\n",
      "After scaling - encoder.proj.weight: grad norm 5.9336347579956055\n",
      "After scaling - encoder.proj.bias: grad norm 0.08426698297262192\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.9419656991958618\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.07117356359958649\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 5.58480167388916\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.0603710301220417\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 5.632874488830566\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.05592792108654976\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 5.491823673248291\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.08516146242618561\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.385915517807007\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.05730648711323738\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01768425665795803\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00041835816227830946\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.023423708975315094\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004357731668278575\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01857500709593296\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00038266394403763115\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.024287940934300423\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00040824225288815796\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.0457838699221611\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006502032047137618\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014984188601374626\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.000549174495972693\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.0430922731757164\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004658222896978259\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04346320405602455\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00043153928709216416\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.042374856770038605\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006571050616912544\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01840970106422901\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00044217632967047393\n",
      "epoch: 68 ( 1 ) recon_loss: 0.717357337474823  perplexity:  164.95864868164062  commit_loss:  0.25349321961402893 \n",
      "\t codebook loss:  1.0139728784561157  total_loss:  1.7869999408721924 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.6139, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.789633870124817\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03826116770505905\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.3747925758361816\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.04010278731584549\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.792112112045288\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.035061080008745193\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.2675445079803467\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.03676772862672806\n",
      "After scaling - encoder.proj.weight: grad norm 4.627630233764648\n",
      "After scaling - encoder.proj.bias: grad norm 0.06201570853590965\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.345706582069397\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.04443025961518288\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.742823362350464\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.03532427176833153\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.8002407550811768\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.03483259305357933\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 4.53496789932251\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.06653638184070587\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.1546974182128906\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.04806508123874664\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.018448691815137863\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003944206109736115\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02448088489472866\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00041340524330735207\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.018474239856004715\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003614320885390043\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02337530255317688\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003790253249462694\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.047704581171274185\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006392977084033191\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01387240644544363\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00045801562373526394\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.0385834239423275\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00036414526402950287\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03917531669139862\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003590766864363104\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.046749357134103775\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006858996348455548\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.02221200242638588\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004954856703989208\n",
      "Are there any dead codes on this epoch?  203\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.7533, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.0826234817504883\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.043032415211200714\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.7957122325897217\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.043203204870224\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.108647346496582\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.037593673914670944\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.744297504425049\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.041690975427627563\n",
      "After scaling - encoder.proj.weight: grad norm 5.150005340576172\n",
      "After scaling - encoder.proj.bias: grad norm 0.0668741911649704\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.6177302598953247\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.058272846043109894\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 4.753216743469238\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.05195216089487076\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 4.994136333465576\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.04932359978556633\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 4.703146934509277\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.07259661704301834\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.0247490406036377\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.048655424267053604\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.018476534634828568\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00038177319220267236\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.024802885949611664\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003832884249277413\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.018707411363720894\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003335220389999449\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.024346746504306793\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00036987228668294847\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04568960890173912\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005932917119935155\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01435211393982172\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.000516982632689178\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.042169392108917236\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004609070310834795\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04430677741765976\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00043758712126873434\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04172518849372864\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006440596771426499\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01796308532357216\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00043165922397747636\n",
      "epoch: 69 ( 1 ) recon_loss: 0.6750733852386475  perplexity:  171.70285034179688  commit_loss:  0.2555462121963501 \n",
      "\t codebook loss:  1.0221848487854004  total_loss:  1.7532883882522583 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.5435, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.835777759552002\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03158051148056984\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.509711742401123\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.03300772234797478\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.8546677827835083\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.029155869036912918\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.3444602489471436\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.031381379812955856\n",
      "After scaling - encoder.proj.weight: grad norm 4.478927135467529\n",
      "After scaling - encoder.proj.bias: grad norm 0.05129774287343025\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.2009729146957397\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03419385850429535\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.323434829711914\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.027542665600776672\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.486619234085083\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.02798665128648281\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.8028757572174072\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.051986876875162125\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.8000619411468506\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03680068254470825\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.020394347608089447\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00035083977854810655\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.027881333604454994\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00036669522523880005\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.020604202523827553\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.000323903514072299\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02604549005627632\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00034862756729125977\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04975809156894684\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005698859458789229\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013342060148715973\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.000379872479243204\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.036921288818120956\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003059818409383297\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03873416408896446\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00031091421260498464\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04224758222699165\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005775417084805667\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01999756507575512\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00040883265319280326\n",
      "Are there any dead codes on this epoch?  196\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.7951, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.583381414413452\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.05651704967021942\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 3.3793563842773438\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.055909790098667145\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.5624563694000244\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.048169028013944626\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 3.3479342460632324\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.05249662697315216\n",
      "After scaling - encoder.proj.weight: grad norm 6.3890380859375\n",
      "After scaling - encoder.proj.bias: grad norm 0.0878266990184784\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.9409750699996948\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.06996805220842361\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 5.696943759918213\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.06147006154060364\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 6.011676788330078\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.0583636648952961\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 5.570059299468994\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.08355749398469925\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.483259439468384\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.05649062991142273\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.018927782773971558\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00041408612742088735\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.0247596874833107\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004096368793398142\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.018774470314383507\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003529222740326077\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.024529466405510902\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003846295003313571\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04681086540222168\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006434839451685548\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01422103215008974\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005126381875015795\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.041740063577890396\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004503755772020668\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.044046033173799515\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0004276157997082919\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04081041365861893\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006122046615928411\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01819421350955963\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004138925578445196\n",
      "epoch: 70 ( 1 ) recon_loss: 0.7158584594726562  perplexity:  171.38070678710938  commit_loss:  0.25582069158554077 \n",
      "\t codebook loss:  1.023282766342163  total_loss:  1.795142412185669 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.5742, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.783618450164795\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03800788149237633\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 3.9145894050598145\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.039644088596105576\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.7747809886932373\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.03600114583969116\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 3.4355666637420654\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.04123059660196304\n",
      "After scaling - encoder.proj.weight: grad norm 6.320774078369141\n",
      "After scaling - encoder.proj.bias: grad norm 0.0663013905286789\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.7353605031967163\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.04158668965101242\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 4.940645217895508\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.03526512160897255\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 5.213879585266113\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.03730254992842674\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 5.2549614906311035\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.06575879454612732\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.49395751953125\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.04523660987615585\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.021287554875016212\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00029066301067359746\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.029936587437987328\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003031757951248437\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.021219970658421516\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00027531658997759223\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.026273289695382118\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003153085126541555\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04833774268627167\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.000507035874761641\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013271065428853035\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00031803169986233115\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03778328374028206\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0002696878800634295\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.039872828871011734\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0002852689940482378\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04018700122833252\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005028863670304418\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.019072391092777252\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00034594425233080983\n",
      "Are there any dead codes on this epoch?  195\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.6341, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.3601009845733643\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.033027809113264084\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.7420265674591064\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.03372997045516968\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.3012968301773071\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.0287513118237257\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.6634589433670044\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.030859757214784622\n",
      "After scaling - encoder.proj.weight: grad norm 3.4888651371002197\n",
      "After scaling - encoder.proj.bias: grad norm 0.05106670409440994\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.2684489488601685\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.050420235842466354\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.7386538982391357\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.04344700649380684\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.64103627204895\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.04014158248901367\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.3470888137817383\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.05810040980577469\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.4581568241119385\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03955280780792236\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.017040226608514786\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004137938085477799\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.021825237199664116\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.000422590906964615\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.016303488984704018\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003602150536607951\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02084089256823063\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00038663099985569715\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04371076449751854\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006397967808879912\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.015891950577497482\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0006316973594948649\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.046840276569128036\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0005443322588689625\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04561726376414299\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0005029197200201452\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04193449765443802\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007279196288436651\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018268732354044914\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004955432377755642\n",
      "epoch: 71 ( 1 ) recon_loss: 0.624014139175415  perplexity:  172.8238983154297  commit_loss:  0.23944810032844543 \n",
      "\t codebook loss:  0.9577924013137817  total_loss:  1.6341416835784912 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.5300, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.9458649158477783\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.031679779291152954\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 4.105733394622803\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.03429427742958069\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.8053088188171387\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.031345248222351074\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 3.284219741821289\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.034392014145851135\n",
      "After scaling - encoder.proj.weight: grad norm 6.44297456741333\n",
      "After scaling - encoder.proj.bias: grad norm 0.056670695543289185\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.8835184574127197\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03547883406281471\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 5.311642169952393\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.028798213228583336\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 5.214657783508301\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.02915087342262268\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 4.6694488525390625\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.046813029795885086\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.4847488403320312\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03359971567988396\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.022419793531298637\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00024110206868499517\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.031247088685631752\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0002609999501146376\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.0213500764220953\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00023855609470047057\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.024994876235723495\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00026174381491728127\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04903488606214523\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.000431297899922356\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01433470193296671\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00027001515263691545\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04042477533221245\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00021917161939200014\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.0396866612136364\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00022185558918863535\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.035537298768758774\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0003562751808203757\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018910424783825874\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00025571390870027244\n",
      "Are there any dead codes on this epoch?  196\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.6350, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.553755283355713\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.0384930856525898\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.026195764541626\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.04072440415620804\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.6118834018707275\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.03615378215909004\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.107820510864258\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.038637056946754456\n",
      "After scaling - encoder.proj.weight: grad norm 4.216352939605713\n",
      "After scaling - encoder.proj.bias: grad norm 0.06277312338352203\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.5703492164611816\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.06019609421491623\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 4.554810523986816\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.05053223669528961\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 4.394157886505127\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.04622238129377365\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 4.144250392913818\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.06697933375835419\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.815860390663147\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.04417245090007782\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01599549688398838\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003962760674767196\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.020859146490693092\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004192468768451363\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01659391075372696\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00037219354999251664\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02169944904744625\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00039775820914655924\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.043406229466199875\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006462325691245496\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01616632752120495\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0006197027978487313\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.046890560537576675\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0005202159518375993\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04523668438196182\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0004758470749948174\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04266395792365074\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006895344122312963\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018693799152970314\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004547436546999961\n",
      "epoch: 72 ( 1 ) recon_loss: 0.6327818632125854  perplexity:  172.66896057128906  commit_loss:  0.2375991940498352 \n",
      "\t codebook loss:  0.9503967761993408  total_loss:  1.6350294351577759 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.4985, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.223684787750244\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.02687542513012886\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 3.1429643630981445\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.030354894697666168\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.167064905166626\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.027596522122621536\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.5192172527313232\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.028899677097797394\n",
      "After scaling - encoder.proj.weight: grad norm 5.158598899841309\n",
      "After scaling - encoder.proj.bias: grad norm 0.05033842846751213\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.5903083086013794\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.039909783750772476\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 4.578937530517578\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.03299388289451599\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 4.494141578674316\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.03318176418542862\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 4.134274959564209\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.052699122577905655\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.025317430496216\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03539466857910156\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.020524881780147552\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0002480634720996022\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.029009943827986717\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0002801794034894556\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.020002273842692375\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0002547192561905831\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.023252682760357857\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00026674752007238567\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04761449620127678\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.00046462981845252216\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01467873901128769\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00036837218794971704\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.042264148592948914\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003045375633519143\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04148147627711296\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00030627171508967876\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03815985843539238\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0004864193615503609\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01869392767548561\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00032669713255017996\n",
      "Are there any dead codes on this epoch?  198\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.6034, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.3029112815856934\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.030170727521181107\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.7303097248077393\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.03287210315465927\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.3597002029418945\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.029432736337184906\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.7517377138137817\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.031048517674207687\n",
      "After scaling - encoder.proj.weight: grad norm 3.5051770210266113\n",
      "After scaling - encoder.proj.bias: grad norm 0.05146300420165062\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.2518373727798462\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.05003275349736214\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.680562973022461\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.042629312723875046\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.614506721496582\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.039173029363155365\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.472670555114746\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.05695124343037605\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.5186481475830078\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03908226639032364\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.016228636726737022\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003757967206183821\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.021552173420786858\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004094442119821906\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.016935981810092926\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003666045668069273\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02181907184422016\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00038673021481372416\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04365934431552887\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.000641006336081773\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.015592479147017002\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0006231916486285627\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04584389179944992\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.000530976802110672\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.045021116733551025\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00048792650341056287\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04325445368885994\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.000709366169758141\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01891578547656536\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00048679596511647105\n",
      "epoch: 73 ( 1 ) recon_loss: 0.6090933680534363  perplexity:  176.57044982910156  commit_loss:  0.23572733998298645 \n",
      "\t codebook loss:  0.9429093599319458  total_loss:  1.603366732597351 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.5084, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.275740385055542\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03567447140812874\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 3.1086459159851074\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.03721410408616066\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.3116135597229004\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.03364058956503868\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.8391263484954834\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.03619934991002083\n",
      "After scaling - encoder.proj.weight: grad norm 5.568951606750488\n",
      "After scaling - encoder.proj.bias: grad norm 0.06320689618587494\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.628465175628662\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.04672446846961975\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 4.745007514953613\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.038806892931461334\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 4.891266822814941\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.03910202160477638\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 4.699587345123291\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.06241745129227638\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.1783530712127686\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.0402534119784832\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.019571762531995773\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00030680664349347353\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02673489600419998\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00032004775130189955\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019880278035998344\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0002893148921430111\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.024416977539658546\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00031132070580497384\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.0478939525783062\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005435902858152986\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014005082659423351\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00040183853707276285\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04080789163708687\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00033374602207913995\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04206574335694313\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00033628419623710215\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04041726887226105\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005368008860386908\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018734212964773178\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0003461863088887185\n",
      "Are there any dead codes on this epoch?  189\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.6170, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.4923509359359741\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.037109579890966415\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.9489929676055908\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.038884297013282776\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.5926026105880737\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.0343402661383152\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.141489267349243\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.03665352240204811\n",
      "After scaling - encoder.proj.weight: grad norm 4.228044509887695\n",
      "After scaling - encoder.proj.bias: grad norm 0.06281454861164093\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.4435837268829346\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.05630655214190483\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 4.231351852416992\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.04810385778546333\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 4.407445430755615\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.04540488123893738\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 4.313457012176514\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.06686879694461823\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.7705293893814087\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.04231368377804756\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.015541770495474339\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00038646982284262776\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.020297370851039886\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004049522103741765\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.0165858194231987\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003576293820515275\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.022302081808447838\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.000381720281438902\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04403206706047058\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006541687180288136\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.015033895149827003\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005863925907760859\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04406651109457016\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0005009673768654466\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04590039700269699\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00047285945038311183\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.044921573251485825\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006963908090256155\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018438801169395447\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.000440666830400005\n",
      "epoch: 74 ( 1 ) recon_loss: 0.6046052575111389  perplexity:  175.1029052734375  commit_loss:  0.24003788828849792 \n",
      "\t codebook loss:  0.9601515531539917  total_loss:  1.6169815063476562 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.5089, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.266157627105713\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.039968423545360565\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 3.0440962314605713\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.04196878895163536\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.347843647003174\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.037450361996889114\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.9550564289093018\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.04004047065973282\n",
      "After scaling - encoder.proj.weight: grad norm 5.512199878692627\n",
      "After scaling - encoder.proj.bias: grad norm 0.06810589134693146\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.5846084356307983\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.053164608776569366\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 4.61944580078125\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.04418834298849106\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 4.936892032623291\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.04446765035390854\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 4.904032230377197\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.07208023965358734\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.1300582885742188\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.046714745461940765\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.019441716372966766\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00034289530594833195\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.026115771383047104\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00036005672882311046\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.020142514258623123\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00032129246392287314\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.025351885706186295\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003435133839957416\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04729001596570015\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005842909449711442\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013594601303339005\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00045610740198753774\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.0396309420466423\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00037909860839135945\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04235436022281647\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00038149484316818416\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04207245260477066\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006183875375427306\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018274100497364998\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004007730458397418\n",
      "Are there any dead codes on this epoch?  190\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.5437, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.4695802927017212\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03799200803041458\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.8547050952911377\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.039451565593481064\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.5486912727355957\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.034263648092746735\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.084625005722046\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.03719939664006233\n",
      "After scaling - encoder.proj.weight: grad norm 4.115631580352783\n",
      "After scaling - encoder.proj.bias: grad norm 0.06467034667730331\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.319697618484497\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.05575865879654884\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.8226051330566406\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.04786030948162079\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 4.164123058319092\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.04647033289074898\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 4.125739097595215\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.0708860456943512\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.643423318862915\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.046251364052295685\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01615772396326065\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004177140654064715\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02039209008216858\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004337616264820099\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.017027532681822777\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003767215821426362\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02292001061141491\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004089995054528117\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04525049775838852\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007110367296263576\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014509795233607292\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0006130546098574996\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04202873259782791\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0005262139602564275\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04578365013003349\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0005109314806759357\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.045361630618572235\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007793770055286586\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01806909218430519\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005085239536128938\n",
      "epoch: 75 ( 1 ) recon_loss: 0.5925074219703674  perplexity:  176.03953552246094  commit_loss:  0.22553911805152893 \n",
      "\t codebook loss:  0.9021564722061157  total_loss:  1.54366135597229 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.5133, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.2644221782684326\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.039559029042720795\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 3.015113353729248\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.04268292710185051\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.2430436611175537\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.03752118721604347\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.587836980819702\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.03770389035344124\n",
      "After scaling - encoder.proj.weight: grad norm 5.054403305053711\n",
      "After scaling - encoder.proj.bias: grad norm 0.06285155564546585\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.6225171089172363\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.053042933344841\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 4.82594108581543\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.045157887041568756\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 5.0870680809021\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.0443478599190712\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 5.01407527923584\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.07094571739435196\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.2064602375030518\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.044528860598802567\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.019610891118645668\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00034259859239682555\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.026112206280231476\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00036965293111279607\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01942574419081211\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003249500005040318\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.022411804646253586\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00032653225935064256\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.043773353099823\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005443220725283027\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014051711186766624\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004593751218635589\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04179477319121361\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.000391087174648419\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.0440562479197979\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003840719582512975\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.0434240959584713\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006144210346974432\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.019108915701508522\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00038563949055969715\n",
      "Are there any dead codes on this epoch?  189\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.5780, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.6326406002044678\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.04412371665239334\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.0646841526031494\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.045444950461387634\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.7880867719650269\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.03960534557700157\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.4197192192077637\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.04278502240777016\n",
      "After scaling - encoder.proj.weight: grad norm 4.567508220672607\n",
      "After scaling - encoder.proj.bias: grad norm 0.07255993038415909\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.4928057193756104\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.06122032552957535\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 4.367118835449219\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.05211472138762474\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 4.810062885284424\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.05067912116646767\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 4.812061309814453\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.07859811931848526\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.9061006307601929\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.04907424747943878\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.015702879056334496\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00042438574018888175\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.019858311861753464\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004370934620965272\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.017197974026203156\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003809276386164129\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02327306941151619\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004115100600756705\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04393068701028824\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006978877354413271\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014357935637235641\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005888224113732576\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.0420033298432827\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0005012439214624465\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04626360163092613\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0004874361911788583\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.046282824128866196\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007559635560028255\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018333042040467262\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00047200036351568997\n",
      "epoch: 76 ( 1 ) recon_loss: 0.5906388163566589  perplexity:  176.5930938720703  commit_loss:  0.23414558172225952 \n",
      "\t codebook loss:  0.9365823268890381  total_loss:  1.5780140161514282 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.4559, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.8356109857559204\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.034235648810863495\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.470951795578003\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.037623174488544464\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.8477305173873901\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.0329296849668026\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.0843026638031006\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.032314643263816833\n",
      "After scaling - encoder.proj.weight: grad norm 4.251472473144531\n",
      "After scaling - encoder.proj.bias: grad norm 0.05331406742334366\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.4015889167785645\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.04724079370498657\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 4.138094902038574\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.04066040366888046\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 4.197749137878418\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.04019167274236679\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.9601988792419434\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.06195301562547684\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.7567336559295654\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03994838520884514\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.0192630086094141\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003592708962969482\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.025930311530828476\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003948197700083256\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01939019188284874\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003455660480540246\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02187279239296913\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003391117206774652\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04461519792675972\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005594809772446752\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014708355069160461\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004957476048730314\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04342540726065636\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00042669265531003475\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04405142366886139\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0004217737587168813\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.041558556258678436\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006501385942101479\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018435264006257057\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004192206833977252\n",
      "Are there any dead codes on this epoch?  185\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.5207, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.2960866689682007\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03495923429727554\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.640581727027893\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.0365779735147953\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.380216360092163\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.03190147504210472\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.8447325229644775\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.034352127462625504\n",
      "After scaling - encoder.proj.weight: grad norm 3.467742919921875\n",
      "After scaling - encoder.proj.bias: grad norm 0.05701320618391037\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.2410203218460083\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.0533946268260479\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.659982681274414\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.04607493430376053\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.9980599880218506\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.04454069957137108\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 4.035419940948486\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.06953448057174683\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.540680170059204\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.04412461444735527\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.015348810702562332\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004140021337661892\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.019428467378020287\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00043317192466929555\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.016345109790563583\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003777908568736166\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02184610813856125\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004068125272169709\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.0410664901137352\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006751746404916048\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014696691185235977\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0006323218694888055\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04334307461977005\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0005456390208564699\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.047346729785203934\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0005274699069559574\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.047789160162210464\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0008234568522311747\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.0182453915476799\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005225424538366497\n",
      "epoch: 77 ( 1 ) recon_loss: 0.5676047205924988  perplexity:  180.41661071777344  commit_loss:  0.22602999210357666 \n",
      "\t codebook loss:  0.9041199684143066  total_loss:  1.520686388015747 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.4679, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.7685164213180542\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03648928552865982\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.3550429344177246\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.039221957325935364\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.821839690208435\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.03393623232841492\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.0881943702697754\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.03285149112343788\n",
      "After scaling - encoder.proj.weight: grad norm 4.331902503967285\n",
      "After scaling - encoder.proj.bias: grad norm 0.05351593345403671\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.4232971668243408\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.047234907746315\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 4.2118353843688965\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.04105433449149132\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 4.271899223327637\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.04032307118177414\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 4.047142028808594\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.062126170843839645\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.7589025497436523\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03963880240917206\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.018375705927610397\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003791406052187085\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02446999028325081\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004075343313161284\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01892976090312004\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00035261319135315716\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.021697307005524635\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00034134226734749973\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04501048102974892\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005560553981922567\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014788717962801456\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004907925613224506\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.043762922286987305\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00042657359153963625\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04438701272010803\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00041897539631463587\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.042051684111356735\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006455197581090033\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01827581413090229\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004118655633646995\n",
      "Are there any dead codes on this epoch?  191\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.5239, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.4591000080108643\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03885053098201752\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.8735040426254272\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.04041489586234093\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.5852500200271606\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.03521614894270897\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.134981393814087\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.03738444298505783\n",
      "After scaling - encoder.proj.weight: grad norm 4.005570411682129\n",
      "After scaling - encoder.proj.bias: grad norm 0.061542708426713943\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.3649940490722656\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.05421194061636925\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.9983110427856445\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.046921517699956894\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 4.430009365081787\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.04536871239542961\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 4.4289469718933105\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.06885411590337753\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.7420177459716797\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.042972221970558167\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.015474865213036537\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00041203940054401755\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.019869934767484665\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00042863067938014865\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.016812780871987343\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00037349399644881487\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02264309860765934\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00039649041718803346\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04248211905360222\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006527071818709373\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014476798474788666\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005749588017351925\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04240512475371361\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004976383643224835\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.046983618289232254\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0004811696708202362\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04697234556078911\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007302502053789794\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018475420773029327\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00045575303374789655\n",
      "epoch: 78 ( 1 ) recon_loss: 0.5692935585975647  perplexity:  178.73963928222656  commit_loss:  0.2264135479927063 \n",
      "\t codebook loss:  0.9056541919708252  total_loss:  1.5239226818084717 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.4117, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.6359819173812866\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03546641394495964\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.1331865787506104\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.037313658744096756\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.6615846157073975\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.03234082832932472\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.9272618293762207\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.03206048160791397\n",
      "After scaling - encoder.proj.weight: grad norm 3.8310909271240234\n",
      "After scaling - encoder.proj.bias: grad norm 0.05192805454134941\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.244722604751587\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.04574805870652199\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.6760988235473633\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.03936763480305672\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.8269131183624268\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.039237022399902344\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.636509656906128\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.06057224050164223\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.5785138607025146\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.0402047373354435\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01904088631272316\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004127869033254683\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.024827755987644196\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00043428665958344936\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01933887042105198\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00037640880327671766\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.022431038320064545\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003731458855327219\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.044589344412088394\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006043808534741402\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014487091451883316\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005324530648067594\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04278542101383209\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004581924877129495\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04454072192311287\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00045667230733670294\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.042324651032686234\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007049889536574483\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018372025340795517\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00046793537330813706\n",
      "Are there any dead codes on this epoch?  187\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.4661, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.1959542036056519\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03133470565080643\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.5326231718063354\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.03285649046301842\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.255415916442871\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02848382666707039\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.6559089422225952\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.029851650819182396\n",
      "After scaling - encoder.proj.weight: grad norm 3.3326408863067627\n",
      "After scaling - encoder.proj.bias: grad norm 0.04938477277755737\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.1196357011795044\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.04525715857744217\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.2412378787994385\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.0397719070315361\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.478546142578125\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.0387483648955822\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.4787278175354004\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.06083190068602562\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.385179877281189\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.039082057774066925\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01582689769566059\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00041467405389994383\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02028227411210537\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00043481288594193757\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01661379635334015\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.000376946380129084\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.0219138003885746\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00039504776941612363\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.044103167951107025\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006535432185046375\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014816922135651112\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005989196361042559\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04289356619119644\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0005263294442556798\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04603403061628342\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0005127841723151505\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04603643715381622\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0008050310425460339\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018331052735447884\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005172002129256725\n",
      "epoch: 79 ( 1 ) recon_loss: 0.545789897441864  perplexity:  182.83975219726562  commit_loss:  0.2182897925376892 \n",
      "\t codebook loss:  0.8731591701507568  total_loss:  1.4661002159118652 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.4373, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.8642762899398804\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.037413377314805984\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.404109239578247\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.03853725641965866\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.828554391860962\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.03328004479408264\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.1158437728881836\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.03309105709195137\n",
      "After scaling - encoder.proj.weight: grad norm 4.212218761444092\n",
      "After scaling - encoder.proj.bias: grad norm 0.05322355031967163\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.3909976482391357\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.04835343360900879\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 4.114512920379639\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.04191340133547783\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 4.324796199798584\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.04157385230064392\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 4.100122928619385\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.06298782676458359\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.779178261756897\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.0404970720410347\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.019396169111132622\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00038925354601815343\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.025012658908963203\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004009465337730944\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01902451366186142\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003462498134467751\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.022013509646058083\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003442835877649486\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.0438244603574276\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.000553744612261653\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014472117647528648\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005030753090977669\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.0428079217672348\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00043607241241261363\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.044995732605457306\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00043253967305645347\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04265820235013962\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006553333951160312\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01851079799234867\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00042133667739108205\n",
      "Are there any dead codes on this epoch?  176\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.5176, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.6294490098953247\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03711141273379326\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.1219348907470703\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.03768708556890488\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.6732382774353027\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.032888319343328476\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.3172454833984375\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.03462504968047142\n",
      "After scaling - encoder.proj.weight: grad norm 4.795895576477051\n",
      "After scaling - encoder.proj.bias: grad norm 0.057365454733371735\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.4581321477890015\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.04913587495684624\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 4.09113883972168\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.043121859431266785\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 4.17995548248291\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.04237686097621918\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.8574814796447754\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.06399215757846832\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.6140166521072388\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.04030285030603409\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.016972629353404045\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003865590551868081\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.022102447226643562\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003925553464796394\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.017428746446967125\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003425705654080957\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02413683943450451\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00036066066240891814\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04995489865541458\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005975287058390677\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.015188162215054035\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005118079716339707\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04261402785778046\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00044916491606272757\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04353915899991989\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0004414048744365573\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04018021002411842\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006665536784566939\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.016811883077025414\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004198016249574721\n",
      "epoch: 80 ( 1 ) recon_loss: 0.5540756583213806  perplexity:  181.31993103027344  commit_loss:  0.22856232523918152 \n",
      "\t codebook loss:  0.9142493009567261  total_loss:  1.5176265239715576 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.3768, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.6404571533203125\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.028750117868185043\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.0667009353637695\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.029860720038414\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.4966074228286743\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02574467658996582\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.762204647064209\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.026392946019768715\n",
      "After scaling - encoder.proj.weight: grad norm 3.353025436401367\n",
      "After scaling - encoder.proj.bias: grad norm 0.04292665421962738\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.130802869796753\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03688029199838638\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.3119351863861084\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.0317247211933136\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.442852020263672\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.03172620013356209\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.4286327362060547\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.05049712955951691\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.5790574550628662\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.034037720412015915\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.020848603919148445\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00036538581480272114\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.026265744119882584\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00037950053228996694\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01902041584253311\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003271896275691688\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.022395895794034004\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00033542848541401327\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.042613670229911804\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005455557256937027\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014371396042406559\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004687123582698405\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.042091451585292816\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00040319011895917356\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04375527799129486\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0004032089200336486\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04357456415891647\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006417690310627222\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.020068272948265076\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00043258609366603196\n",
      "Are there any dead codes on this epoch?  185\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.4376, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.4462047815322876\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.028368981555104256\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.9227204322814941\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.02887589856982231\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.4493322372436523\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.025050904601812363\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.084862232208252\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.026412615552544594\n",
      "After scaling - encoder.proj.weight: grad norm 4.4873270988464355\n",
      "After scaling - encoder.proj.bias: grad norm 0.04448963329195976\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.3109735250473022\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.039486005902290344\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.526082754135132\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.03463380038738251\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.454437255859375\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.033178139477968216\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.3641421794891357\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.051046185195446014\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.473866581916809\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.033425867557525635\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.017048541456460953\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003344268770888448\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.022665930911898613\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003404026501812041\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01708540879189968\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0002953118528239429\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02457733452320099\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003113643324468285\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.052898719906806946\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.000524464703630656\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.015454371459782124\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00046547959209419787\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.0415671207010746\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004082795057911426\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.040722526609897614\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00039111950900405645\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.039658088237047195\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006017563864588737\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.017374632880091667\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00039403981645591557\n",
      "epoch: 81 ( 1 ) recon_loss: 0.5450843572616577  perplexity:  183.5301513671875  commit_loss:  0.2117239385843277 \n",
      "\t codebook loss:  0.8468957543373108  total_loss:  1.437587022781372 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.3874, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.7410310506820679\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.023816555738449097\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.1550357341766357\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.025332694873213768\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.4481528997421265\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02143017016351223\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.6493260860443115\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.021510884165763855\n",
      "After scaling - encoder.proj.weight: grad norm 3.2969350814819336\n",
      "After scaling - encoder.proj.bias: grad norm 0.03540196642279625\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.1183383464813232\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.030430207028985023\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.3008015155792236\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.02678515762090683\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.3499300479888916\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.026795431971549988\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.2746310234069824\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.040443722158670425\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.596500039100647\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.02661174349486828\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.022474611178040504\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00030744299874641\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.027818914502859116\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003270145389251411\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01869390718638897\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0002766376710496843\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.021290810778737068\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0002776795590762049\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04255945608019829\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0004569966986309737\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014436399564146996\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00039281728095375\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04260936751961708\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003457641287241131\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04324355721473694\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.000345896725775674\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.042271535843610764\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005220797029323876\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.020608890801668167\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00034352552029304206\n",
      "Are there any dead codes on this epoch?  179\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.4549, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.323180079460144\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.02312750183045864\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.7356237173080444\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.022989705204963684\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.3217109441757202\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02031175047159195\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.9496089220046997\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.022883327677845955\n",
      "After scaling - encoder.proj.weight: grad norm 3.9351866245269775\n",
      "After scaling - encoder.proj.bias: grad norm 0.039019156247377396\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.1076350212097168\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.033526498824357986\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.0883824825286865\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.029035046696662903\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.174072742462158\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.02776358090341091\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.2073514461517334\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.044379234313964844\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.3609544038772583\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.028308531269431114\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01725783385336399\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00030164496274665\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02263721264898777\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00029984774300828576\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.017238672822713852\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00026491997414268553\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.025428155437111855\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00029846030520275235\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.051325440406799316\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005089149926789105\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014446546323597431\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004372759140096605\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04028083011507988\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003786952584050596\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.041398465633392334\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003621119540184736\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04183251038193703\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005788248381577432\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.017750514671206474\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00036921954597346485\n",
      "epoch: 82 ( 1 ) recon_loss: 0.51645827293396  perplexity:  181.75604248046875  commit_loss:  0.22264228761196136 \n",
      "\t codebook loss:  0.8905691504478455  total_loss:  1.4549212455749512 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.3856, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.719340443611145\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.02942764200270176\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.1237704753875732\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.02966109663248062\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.5042909383773804\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02550874650478363\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.876121997833252\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.02680872566998005\n",
      "After scaling - encoder.proj.weight: grad norm 3.565518856048584\n",
      "After scaling - encoder.proj.bias: grad norm 0.045796722173690796\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.1653372049331665\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03729837015271187\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.518843650817871\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.03383111208677292\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.715341329574585\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.03367071971297264\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.551478624343872\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04877077043056488\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.6588579416275024\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03254839777946472\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.020715942606329918\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00035456696059554815\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.025588827207684517\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.000357379816705361\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.018124857917428017\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003073490515816957\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.022604964673519135\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.000323012238368392\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04296012595295906\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005517942481674254\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014040883630514145\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004493995802477002\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04239774867892265\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004076233599334955\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.044765301048755646\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00040569083648733795\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.042790960520505905\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005876279319636524\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.019987203180789948\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00039216826553456485\n",
      "Are there any dead codes on this epoch?  189\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.3944, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.0318272113800049\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.02113410085439682\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.3490442037582397\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.02148757129907608\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.0502772331237793\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.018759476020932198\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.5063680410385132\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.02060774341225624\n",
      "After scaling - encoder.proj.weight: grad norm 3.0192811489105225\n",
      "After scaling - encoder.proj.bias: grad norm 0.035200152546167374\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.9204367399215698\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.029637452214956284\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.629607677459717\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.0254910159856081\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.6642520427703857\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.024477317929267883\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.7308883666992188\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.03777927905321121\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.1837531328201294\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.025711331516504288\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01650145836174488\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003379863337613642\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.021574540063738823\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00034363919985480607\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.016796518117189407\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00030001020058989525\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.024090532213449478\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003295685746707022\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04828574135899544\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005629371153190732\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014720050618052483\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004739758442156017\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04205390065908432\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00040766410529613495\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.042607951909303665\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00039145260234363377\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04367363080382347\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006041837041266263\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018931128084659576\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00041118753142654896\n",
      "epoch: 83 ( 1 ) recon_loss: 0.502556324005127  perplexity:  183.0111541748047  commit_loss:  0.2115899622440338 \n",
      "\t codebook loss:  0.8463598489761353  total_loss:  1.394371509552002 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.4181, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.9738073348999023\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.04385591670870781\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.385688066482544\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.04250765219330788\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.8395748138427734\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.03673683851957321\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.3475723266601562\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.038927413523197174\n",
      "After scaling - encoder.proj.weight: grad norm 4.429265975952148\n",
      "After scaling - encoder.proj.bias: grad norm 0.06649255007505417\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.3636265993118286\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.0510481633245945\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 4.140022277832031\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.045773252844810486\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 4.600006580352783\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.04579000547528267\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 4.431796550750732\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.06845362484455109\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.9067552089691162\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.0449751578271389\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.019585764035582542\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00043517499580048025\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.023672791197896004\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004217964014969766\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01825379766523838\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00036453359643928707\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02329457364976406\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003862702869810164\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.043950874358415604\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.000659794604871422\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013531041331589222\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005065425066277385\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04108076170086861\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004542004317045212\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.045645102858543396\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00045436667278409004\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.043975986540317535\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006792540661990643\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.0189204178750515\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00044628107571043074\n",
      "Are there any dead codes on this epoch?  176\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.4291, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.539084553718567\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03274928405880928\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.958833932876587\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.03297567367553711\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.5370500087738037\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.028691360726952553\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.1161723136901855\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.030935823917388916\n",
      "After scaling - encoder.proj.weight: grad norm 4.209725379943848\n",
      "After scaling - encoder.proj.bias: grad norm 0.052777670323848724\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.267532467842102\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.04114017263054848\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.657715320587158\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.0355263315141201\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.769749641418457\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.03485848754644394\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.6899573802948\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.05203581973910332\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.6064151525497437\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03498247638344765\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01763409748673439\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.000375225703464821\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.022443387657403946\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00037781961145810783\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.017610788345336914\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003287319850642234\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.0242460984736681\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00035444795503281057\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.0482330285012722\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006047014612704515\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01452278345823288\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00047136456123553216\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04190836474299431\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004070437862537801\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.043191999197006226\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00039939192356541753\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04227777570486069\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005962016875855625\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01840553991496563\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00040081259794533253\n",
      "epoch: 84 ( 1 ) recon_loss: 0.5248785614967346  perplexity:  181.259765625  commit_loss:  0.21455414593219757 \n",
      "\t codebook loss:  0.8582165837287903  total_loss:  1.4291263818740845 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.3736, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.5550698041915894\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03632202371954918\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.922641634941101\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.03468579426407814\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.51414954662323\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.030227629467844963\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.987404704093933\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.03308630734682083\n",
      "After scaling - encoder.proj.weight: grad norm 3.6919355392456055\n",
      "After scaling - encoder.proj.bias: grad norm 0.056711819022893906\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.1425961256027222\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.045153193175792694\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.447197914123535\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.040813617408275604\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.8994925022125244\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.04081105813384056\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.7871413230895996\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.06296125799417496\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.6129231452941895\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.0413828007876873\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.018410591408610344\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00043001925223506987\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.022762304171919823\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004106478299945593\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.017926134169101715\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003578672476578504\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.023529039695858955\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00039171139360405505\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04370911419391632\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.000671415647957474\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013527284376323223\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.000534572172909975\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04081164300441742\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00048319558845832944\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04616639390587807\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.000483165291370824\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04483625665307045\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007454032311215997\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01909552328288555\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004899342311546206\n",
      "Are there any dead codes on this epoch?  191\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.3706, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.4041993618011475\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03003022074699402\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.836757779121399\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.03112589567899704\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.387580156326294\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02702532522380352\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.7341761589050293\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.02831374853849411\n",
      "After scaling - encoder.proj.weight: grad norm 3.5540146827697754\n",
      "After scaling - encoder.proj.bias: grad norm 0.04795942083001137\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.1062923669815063\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03915373980998993\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.26434588432312\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.034220803529024124\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.2903926372528076\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.03319784998893738\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.958508253097534\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04839643836021423\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.337842583656311\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.033792607486248016\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.018735762685537338\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004006832023151219\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.024507245048880577\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004153024055995047\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.018514016643166542\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00036058988189324737\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.023138534277677536\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003777808742597699\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04742002859711647\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006399065023288131\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014760890044271946\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005224152700975537\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04355507716536522\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004565967246890068\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04390260577201843\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00044294781400822103\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03947438672184944\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006457374547608197\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01785038597881794\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00045088346814736724\n",
      "epoch: 85 ( 1 ) recon_loss: 0.5054084658622742  perplexity:  181.89198303222656  commit_loss:  0.20529234409332275 \n",
      "\t codebook loss:  0.821169376373291  total_loss:  1.3705644607543945 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.3599, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.2333866357803345\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03195679932832718\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.5378221273422241\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.03062446042895317\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.258474349975586\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02691126987338066\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.67438805103302\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.029870372265577316\n",
      "After scaling - encoder.proj.weight: grad norm 3.1739957332611084\n",
      "After scaling - encoder.proj.bias: grad norm 0.04970211163163185\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.9858859777450562\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03950060158967972\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.9831326007843018\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.035157859325408936\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.243349552154541\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.03423900902271271\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.186516523361206\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.05406378209590912\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.3370449542999268\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03587798774242401\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.017322931438684464\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00044883365626446903\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.021598730236291885\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00043012091191485524\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.017675286158919334\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003779690887313336\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.023516802117228508\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004195297369733453\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.044578809291124344\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006980667822062969\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013846781104803085\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005547864129766822\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04189813509583473\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004937925259582698\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04555288329720497\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.000480887305457145\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04475466534495354\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007593264454044402\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018778810277581215\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005039068055339158\n",
      "Are there any dead codes on this epoch?  176\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.3574, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.3076313734054565\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.020327815786004066\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.786529779434204\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.02130962908267975\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.2609987258911133\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.0187042448669672\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.548642635345459\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.01921389065682888\n",
      "After scaling - encoder.proj.weight: grad norm 3.2214667797088623\n",
      "After scaling - encoder.proj.bias: grad norm 0.033160500228405\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.022476315498352\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.0272506270557642\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.922654390335083\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.02346975915133953\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.923412322998047\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.022869210690259933\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.6219327449798584\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.03398215025663376\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.28302001953125\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.023569069802761078\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01929498463869095\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00029995068325661123\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.026361454278230667\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003144379588775337\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.018606888130307198\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00027599374880082905\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02285126782953739\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0002835139457602054\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.047534916549921036\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0004893055884167552\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.015087328851222992\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00040210140286944807\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04312574118375778\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003463121538516134\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04313692823052406\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00033745067776180804\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03868839144706726\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005014296039007604\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018931828439235687\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0003477775608189404\n",
      "epoch: 86 ( 1 ) recon_loss: 0.48498931527137756  perplexity:  184.30796813964844  commit_loss:  0.20703083276748657 \n",
      "\t codebook loss:  0.8281233310699463  total_loss:  1.3574144840240479 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.3417, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.208860993385315\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03084433637559414\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.5444140434265137\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.030210187658667564\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.3042856454849243\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.026928160339593887\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.7812342643737793\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.030369753018021584\n",
      "After scaling - encoder.proj.weight: grad norm 3.2671279907226562\n",
      "After scaling - encoder.proj.bias: grad norm 0.04954853653907776\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.9879947304725647\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.04042373225092888\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.9870002269744873\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.03583379462361336\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.207418918609619\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.03438262268900871\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.124750852584839\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.05422051250934601\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.2867684364318848\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.036253441125154495\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.016925008967518806\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00043184508103877306\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.021623017266392708\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00042296649189665914\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.018261030316352844\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003770155308302492\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.024938687682151794\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00042520056013017893\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.045742377638816833\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006937186117284\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013832706958055496\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005659642047248781\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04182036593556404\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0005017014336772263\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04490639641880989\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0004813838459085673\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04374898225069046\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007591300527565181\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018015773966908455\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005075767985545099\n",
      "Are there any dead codes on this epoch?  175\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.3725, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.7712135314941406\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.02095462568104267\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.4529104232788086\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.022412873804569244\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.625067114830017\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.018995152786374092\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.093870162963867\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.017064349725842476\n",
      "After scaling - encoder.proj.weight: grad norm 4.647443771362305\n",
      "After scaling - encoder.proj.bias: grad norm 0.02978408895432949\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.4648972749710083\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03297427296638489\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 4.081384658813477\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.030140912160277367\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.776545763015747\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.028269363567233086\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.3932456970214844\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.03942692652344704\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.6476815938949585\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.026528948917984962\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.019202442839741707\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0002271775738336146\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02659299597144127\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00024298705102410167\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.017618009820580482\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0002059341495623812\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.022700494155287743\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0001850015250965953\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05038481578230858\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0003229013818781823\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01588154397904873\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00035748744267039\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04424794018268585\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003267698339186609\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04094306379556656\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00030647963285446167\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.036787550896406174\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.00042744330130517483\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.017863182350993156\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00028761106659658253\n",
      "epoch: 87 ( 1 ) recon_loss: 0.5255209803581238  perplexity:  185.2014617919922  commit_loss:  0.2010117918252945 \n",
      "\t codebook loss:  0.804047167301178  total_loss:  1.3725271224975586 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.3274, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.0865432024002075\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.025400377810001373\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.4135491847991943\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.02517523616552353\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.1351423263549805\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.022034402936697006\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.5428858995437622\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.02440064400434494\n",
      "After scaling - encoder.proj.weight: grad norm 2.841832160949707\n",
      "After scaling - encoder.proj.bias: grad norm 0.03990718349814415\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.8281001448631287\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.030623652040958405\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.417757034301758\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.026495542377233505\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.6815614700317383\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.026672203093767166\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.7849743366241455\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04495919123291969\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.1763265132904053\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.029512472450733185\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.017673827707767487\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00041316525312140584\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.022992942482233047\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004095030890312046\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.018464345484972\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00035841393400914967\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.025096746161580086\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003969034878537059\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.046225547790527344\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006491345702670515\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013469965197145939\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004981275997124612\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03932749852538109\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004309793293941766\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.043618567287921906\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0004338529543019831\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.045300692319869995\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007313110399991274\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01913425326347351\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004800530441571027\n",
      "Are there any dead codes on this epoch?  174\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.3544, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.515785813331604\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.02594434656202793\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.032200336456299\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.026149872690439224\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.4152929782867432\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.023292338475584984\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.7971874475479126\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.023579223081469536\n",
      "After scaling - encoder.proj.weight: grad norm 3.847439765930176\n",
      "After scaling - encoder.proj.bias: grad norm 0.03876602649688721\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.2188363075256348\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.0351097509264946\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.4451305866241455\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.03275828808546066\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.3833072185516357\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.031005118042230606\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.1253716945648193\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.044415973126888275\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.4173749685287476\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.028276391327381134\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01908932998776436\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00032673493842594326\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02559289149940014\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00032932328758761287\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.017823755741119385\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0002933363721240312\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02263321354985237\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0002969493216369301\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.048453446477651596\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.00048820715164765716\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01534964144229889\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00044216119567863643\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04338689148426056\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00041254761163145304\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04260830581188202\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00039046871825121343\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03935994952917099\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005593608366325498\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01784997433423996\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0003561039920896292\n",
      "epoch: 88 ( 1 ) recon_loss: 0.49188074469566345  perplexity:  185.54928588867188  commit_loss:  0.20471304655075073 \n",
      "\t codebook loss:  0.8188521862030029  total_loss:  1.354427695274353 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.3349, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.6871883869171143\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.030137840658426285\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.352505922317505\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.03226745128631592\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.7877558469772339\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.028189977630972862\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.3320469856262207\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.029265528544783592\n",
      "After scaling - encoder.proj.weight: grad norm 4.443446636199951\n",
      "After scaling - encoder.proj.bias: grad norm 0.0499771423637867\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.1781615018844604\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03703737258911133\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.3898463249206543\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.03352510556578636\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.5268657207489014\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.03248792141675949\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.4649887084960938\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.05163155868649483\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.5104572772979736\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.032932355999946594\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.019305355846881866\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003448468924034387\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.026918133720755577\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003692145983222872\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.02045607939362526\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00032255882979370654\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02668403647840023\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00033486561733298004\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05084335431456566\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005718546453863382\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013480903580784798\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00042379359365440905\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03878771513700485\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00038360507460311055\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.040355537086725235\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003717373183462769\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03964751958847046\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.000590784999076277\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.017283141613006592\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00037682268884964287\n",
      "Are there any dead codes on this epoch?  179\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.3195, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.1401915550231934\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.016707170754671097\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.5649216175079346\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.017573431134223938\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.1186076402664185\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.016017436981201172\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.423410415649414\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.016762424260377884\n",
      "After scaling - encoder.proj.weight: grad norm 2.9081645011901855\n",
      "After scaling - encoder.proj.bias: grad norm 0.02768796682357788\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.9327943921089172\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.021201906725764275\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.7219934463500977\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.018904291093349457\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.732469320297241\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.01816907338798046\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.590714454650879\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.02905157208442688\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.2784943580627441\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.020103655755519867\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.018123364076018333\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0002655607822816819\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.024874458089470863\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0002793300081975758\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01778029091656208\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0002545974566601217\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.022625135257840157\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00026643904857337475\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.046225327998399734\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.00044010073179379106\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014826783910393715\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00033700469066388905\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04326613247394562\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00030048404005356133\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04343264922499657\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00028879777528345585\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04117945581674576\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.00046177528565749526\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.020321689546108246\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0003195479803252965\n",
      "epoch: 89 ( 1 ) recon_loss: 0.47380900382995605  perplexity:  187.82763671875  commit_loss:  0.20072120428085327 \n",
      "\t codebook loss:  0.8028848171234131  total_loss:  1.3194830417633057 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.3596, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.7430026531219482\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.04248971864581108\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.386756420135498\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.04622730240225792\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.9568008184432983\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.041253186762332916\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.4190642833709717\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.04056227207183838\n",
      "After scaling - encoder.proj.weight: grad norm 4.681703567504883\n",
      "After scaling - encoder.proj.bias: grad norm 0.06865771859884262\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.5311235189437866\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.058753665536642075\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 4.879610061645508\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.056981172412633896\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 4.893522262573242\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.05042099580168724\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.9867546558380127\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.06577956676483154\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.6229699850082397\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.04058912768959999\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01669563353061676\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00040699465898796916\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.022861933335661888\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004427956882864237\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01874353364109993\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00039515033131465316\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.023171400651335716\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00038853229489177465\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.044844456017017365\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006576490704901516\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014666115865111351\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005627814680337906\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04674014076590538\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0005458033410832286\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04687339439988136\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00048296566819772124\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03818777576088905\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.000630080234259367\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.015545880421996117\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0003887895436491817\n",
      "Are there any dead codes on this epoch?  169\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.3437, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.1832473278045654\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.01638050191104412\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.6441740989685059\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.018145279958844185\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.1728543043136597\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.016129646450281143\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.3540211915969849\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.01617150753736496\n",
      "After scaling - encoder.proj.weight: grad norm 2.958254814147949\n",
      "After scaling - encoder.proj.bias: grad norm 0.026375632733106613\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.0906049013137817\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.018565015867352486\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.148069143295288\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.016783416271209717\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.942680597305298\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.015407354570925236\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.830502986907959\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.025926437228918076\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.5475976467132568\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.018732288852334023\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.017401963472366333\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00024090729129966348\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.024180792272090912\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00026686181081458926\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01724911481142044\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00023721798788756132\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.019913529977202415\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0002378336212132126\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.043506916612386703\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.00038790522376075387\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.016039477661252022\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0002730348496697843\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04629850760102272\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00024683293304406106\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04327787086367607\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00022659526439383626\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.041628073900938034\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0003812989452853799\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.02276044897735119\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0002754949382506311\n",
      "epoch: 90 ( 1 ) recon_loss: 0.5005871057510376  perplexity:  186.18768310546875  commit_loss:  0.20012150704860687 \n",
      "\t codebook loss:  0.8004860281944275  total_loss:  1.343682050704956 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.2892, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.126550555229187\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.026448557153344154\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.531565546989441\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.029623527079820633\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.198340892791748\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.026001911610364914\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.4607502222061157\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.02574147656559944\n",
      "After scaling - encoder.proj.weight: grad norm 2.7977287769317627\n",
      "After scaling - encoder.proj.bias: grad norm 0.04296589270234108\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.0073492527008057\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.0401497557759285\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.262371063232422\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.03896257281303406\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.345020055770874\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.035394489765167236\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.9073574542999268\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04853421077132225\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.1801702976226807\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.030000800266861916\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.016308533027768135\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003828831249848008\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.0221717432141304\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00042884561116807163\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.017347808927297592\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00037641727249138057\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.021146582439541817\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00037264704587869346\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04050138220191002\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006219967035576701\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014582913368940353\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005812288145534694\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04722778499126434\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0005640425370074809\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04842425510287285\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0005123891169205308\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04208842292428017\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.000702606572303921\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.017084762454032898\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004343072359915823\n",
      "Are there any dead codes on this epoch?  176\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.3694, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.9497418403625488\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.045258261263370514\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.6404364109039307\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.049249593168497086\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.131547212600708\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.04417635500431061\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.5948216915130615\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.04419974237680435\n",
      "After scaling - encoder.proj.weight: grad norm 5.1068620681762695\n",
      "After scaling - encoder.proj.bias: grad norm 0.07299594581127167\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.6771597862243652\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.06007355824112892\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 5.238631725311279\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.05904810503125191\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 5.153703689575195\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.05193759500980377\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 4.210518836975098\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.06938348710536957\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.842046856880188\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.044507283717393875\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01736287958920002\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00040303479181602597\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.023513667285442352\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004385784559417516\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.018981894478201866\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003934001724701375\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.023107457906007767\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003936084103770554\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.0454777292907238\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006500449380837381\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014935475774109364\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005349682760424912\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.046651165932416916\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0005258363671600819\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04589486122131348\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.000462515716208145\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03749559447169304\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006178751937113702\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01640383154153824\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0003963471681345254\n",
      "epoch: 91 ( 1 ) recon_loss: 0.5147948861122131  perplexity:  184.66134643554688  commit_loss:  0.2028643935918808 \n",
      "\t codebook loss:  0.8114575743675232  total_loss:  1.3693926334381104 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.3216, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.2581398487091064\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.023782169446349144\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.7263522148132324\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.02746526710689068\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.2775462865829468\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.023935984820127487\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.4953285455703735\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.023475410416722298\n",
      "After scaling - encoder.proj.weight: grad norm 2.9088222980499268\n",
      "After scaling - encoder.proj.bias: grad norm 0.03910528123378754\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.0005998611450195\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.030597366392612457\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.1789209842681885\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.029456553980708122\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.113401174545288\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.026756230741739273\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.0484395027160645\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04078150540590286\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.4122343063354492\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.027173014357686043\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.0179754626005888\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003397837863303721\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.024664968252182007\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003924054326489568\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.018252728506922722\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003419813874643296\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.021364256739616394\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003354010113980621\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.041559312492609024\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.000558710191398859\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014295903965830803\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00043715478386729956\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.045418307185173035\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004208556201774627\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04448219761252403\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003822751750703901\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.043554071336984634\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005826589767821133\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.02017706073820591\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0003882299060933292\n",
      "Are there any dead codes on this epoch?  171\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.4292, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.184492588043213\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.05484657734632492\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.8991782665252686\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.059090737253427505\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.544696092605591\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.054794520139694214\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 3.218172550201416\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.05643666535615921\n",
      "After scaling - encoder.proj.weight: grad norm 5.9891252517700195\n",
      "After scaling - encoder.proj.bias: grad norm 0.0939975157380104\n",
      "After scaling - decoder.in_proj.weight: grad norm 2.0921311378479004\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.08597465604543686\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 6.701610088348389\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.085141621530056\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 7.216088771820068\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.0758606567978859\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 6.274388313293457\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.10309982299804688\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.491739273071289\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.06546951830387115\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.014972358010709286\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00037591459113173187\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.019870763644576073\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004050037241540849\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.017441166564822197\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003755577781703323\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.022057127207517624\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00038681289879605174\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.0410490408539772\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006442523445002735\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014339318498969078\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005892642075195909\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.045932359993457794\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0005835546180605888\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04945856332778931\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0005199435981921852\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04300421103835106\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007066389080137014\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.017078204080462456\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004487234400585294\n",
      "epoch: 92 ( 1 ) recon_loss: 0.6066548824310303  perplexity:  188.88267517089844  commit_loss:  0.19527016580104828 \n",
      "\t codebook loss:  0.7810806632041931  total_loss:  1.4292106628417969 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.3786, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.350313186645508\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.045066069811582565\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 3.283435821533203\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.05173341557383537\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.4898056983947754\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.0465114563703537\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.8763771057128906\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.04635343328118324\n",
      "After scaling - encoder.proj.weight: grad norm 5.5433855056762695\n",
      "After scaling - encoder.proj.bias: grad norm 0.07805725187063217\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.905076503753662\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.06772048026323318\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 6.052906513214111\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.0672859475016594\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 6.12240743637085\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.06089970842003822\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 5.434300899505615\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.08662779629230499\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 2.3277130126953125\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.05544654279947281\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01780509576201439\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00034140373463742435\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.024874087423086166\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003919130831491202\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.018861841410398483\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00035235346877016127\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.021790361031889915\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00035115634091198444\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.041994620114564896\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005913326749578118\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01443214900791645\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005130251520313323\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.045854564756155014\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0005097332177683711\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04638107493519783\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0004613535129465163\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04116823524236679\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006562599446624517\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.017633887007832527\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00042004234273917973\n",
      "Are there any dead codes on this epoch?  166\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.3425, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.863423228263855\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.04586842656135559\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.3555567264556885\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.04584996774792671\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.0272912979125977\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.042403411120176315\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.6280863285064697\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.043520741164684296\n",
      "After scaling - encoder.proj.weight: grad norm 5.122421741485596\n",
      "After scaling - encoder.proj.bias: grad norm 0.0726981982588768\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.7359293699264526\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.06473127007484436\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 5.293239593505859\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.05979207903146744\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 5.214575290679932\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.05391182005405426\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 4.462782382965088\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.0745595246553421\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.8528027534484863\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.047882791608572006\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01647811383008957\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00040561106288805604\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02083001285791397\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004054478486068547\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01792718656361103\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00037497023004107177\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.023239972069859505\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003848506894428283\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04529719427227974\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006428647902794182\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.015350693836808205\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.000572413788177073\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04680772125720978\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0005287369131110609\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04611210152506828\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0004767382051795721\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03946405276656151\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006593242869712412\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.016384197399020195\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00042342397500760853\n",
      "epoch: 93 ( 1 ) recon_loss: 0.5279848575592041  perplexity:  185.08236694335938  commit_loss:  0.19337569177150726 \n",
      "\t codebook loss:  0.773502767086029  total_loss:  1.3425114154815674 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.3029, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.8276973962783813\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.04255528002977371\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.442687749862671\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.04526729881763458\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.0227010250091553\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.0413910448551178\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.3843724727630615\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.04151654243469238\n",
      "After scaling - encoder.proj.weight: grad norm 4.538241863250732\n",
      "After scaling - encoder.proj.bias: grad norm 0.07010629773139954\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.4714573621749878\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.05695962905883789\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 4.6758012771606445\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.056017033755779266\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 4.85293436050415\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.05032762140035629\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 4.113669395446777\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.06925029307603836\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.683837890625\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.04330169036984444\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.017665380612015724\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004113127361051738\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02360949106514454\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00043752536294050515\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019550161436200142\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00040005994378589094\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.023045852780342102\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00040127296233549714\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04386380687355995\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006776035879738629\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014222186990082264\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005505361477844417\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.045193370431661606\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0005414256593212485\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.046905431896448135\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00048643528134562075\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03976015746593475\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006693300092592835\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01627492345869541\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00041852708091028035\n",
      "Are there any dead codes on this epoch?  170\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.3436, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.908711552619934\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.05091489851474762\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.3620681762695312\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.0507853627204895\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.036454200744629\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.04641718044877052\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.574674606323242\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.04762755334377289\n",
      "After scaling - encoder.proj.weight: grad norm 5.048375129699707\n",
      "After scaling - encoder.proj.bias: grad norm 0.07893750816583633\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.6772643327713013\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.06685014814138412\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 5.155686378479004\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.061453454196453094\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 4.9923996925354\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.05523955076932907\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 4.330972671508789\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.07783155888319016\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.8983783721923828\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.0516052208840847\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01726650819182396\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00046058426960371435\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.021367643028497696\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004594124620780349\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.018422087654471397\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0004198971437290311\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02329091541469097\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004308464122004807\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04566840082406998\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007140812231227756\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.015172799117863178\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0006047370843589306\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04663915932178497\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0005559177370741963\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04516204074025154\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0004997057840228081\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03917866572737694\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007040766649879515\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.017173031345009804\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004668290202971548\n",
      "epoch: 94 ( 1 ) recon_loss: 0.5241566300392151  perplexity:  190.94839477539062  commit_loss:  0.19456620514392853 \n",
      "\t codebook loss:  0.7782648205757141  total_loss:  1.343648910522461 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.2776, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.6453912258148193\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.040490299463272095\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.1144509315490723\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.04116314649581909\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.8298256397247314\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.03823795169591904\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.2432873249053955\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.03944765403866768\n",
      "After scaling - encoder.proj.weight: grad norm 4.152088642120361\n",
      "After scaling - encoder.proj.bias: grad norm 0.0666612759232521\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.3421103954315186\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.05485134199261665\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 4.132879734039307\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.05211717635393143\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 4.375881195068359\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.04820415377616882\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 4.02667760848999\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.07061503827571869\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.629772663116455\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.04595032334327698\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01737048476934433\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00042745828977786005\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02232237160205841\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00043456157436594367\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019317569211125374\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0004036801401525736\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.023682504892349243\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00041645101737231016\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04383382573723793\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007037466857582331\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014168732799589634\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005790686118416488\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04363103583455086\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0005502038402482867\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04619641602039337\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0005088938632979989\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.042509857565164566\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007454868173226714\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.017205599695444107\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004851000849157572\n",
      "Are there any dead codes on this epoch?  170\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.2866, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.4914629459381104\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.04062463715672493\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.8756890296936035\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.04001465439796448\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.6355328559875488\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.03655625507235527\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.165283679962158\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.039194803684949875\n",
      "After scaling - encoder.proj.weight: grad norm 4.127740383148193\n",
      "After scaling - encoder.proj.bias: grad norm 0.06419383734464645\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.2794126272201538\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.051812272518873215\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.9496870040893555\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.04903597757220268\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 4.28122615814209\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.04512704908847809\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.954944610595703\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.06652625650167465\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.6485620737075806\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.04384364187717438\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.0162641704082489\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00044300532317720354\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02045409567654133\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004363535554148257\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.017835231497883797\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003986402880400419\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02361208014190197\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004274131788406521\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04501236602663994\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007000237819738686\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013951795175671577\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005650047096423805\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.0430707223713398\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0005347297410480678\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04668610543012619\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0004921034560538828\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04312805458903313\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.000725458434317261\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.017977310344576836\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00047810806427150965\n",
      "epoch: 95 ( 1 ) recon_loss: 0.48614031076431274  perplexity:  190.07901000976562  commit_loss:  0.1900690197944641 \n",
      "\t codebook loss:  0.7602760791778564  total_loss:  1.2866439819335938 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.2306, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.2682955265045166\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.028515974059700966\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.5585627555847168\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.02835838310420513\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.2979756593704224\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.026424143463373184\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.583358883857727\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.027361609041690826\n",
      "After scaling - encoder.proj.weight: grad norm 3.006857395172119\n",
      "After scaling - encoder.proj.bias: grad norm 0.04636341705918312\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.0412747859954834\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.0410916805267334\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.1854138374328613\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.03844388574361801\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.3610191345214844\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.03604300320148468\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.14404559135437\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.05296168848872185\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.3634047508239746\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03416425734758377\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.017663704231381416\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003971454279962927\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.021706292405724525\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00039495062083005905\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01807706616818905\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00036801223177462816\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.022051632404327393\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00038106844294816256\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04187687113881111\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006457089330069721\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014501960016787052\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005722888163290918\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04436364769935608\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0005354126333259046\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04680931940674782\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0005019753007218242\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.043787501752376556\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007376038702204823\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01898830384016037\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004758097929880023\n",
      "Are there any dead codes on this epoch?  168\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.2960, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.6921586990356445\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.042609550058841705\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.1705269813537598\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.04257809370756149\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.8641061782836914\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.03936837241053581\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.441821575164795\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.042733851820230484\n",
      "After scaling - encoder.proj.weight: grad norm 4.510759353637695\n",
      "After scaling - encoder.proj.bias: grad norm 0.0703074261546135\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.4162166118621826\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.056391652673482895\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 4.376208782196045\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.05448681488633156\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 4.790112495422363\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.0502486526966095\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 4.4798383712768555\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.0754552111029625\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.885895013809204\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.050605762749910355\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01650470681488514\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004155982460360974\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.021170539781451225\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004152914334554225\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.018181821331381798\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003839849669020623\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.023816652595996857\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00041681062430143356\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04399632662534714\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006857533589936793\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013813268393278122\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005500239203684032\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04268396645784378\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0005314447917044163\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04672103747725487\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0004901072825305164\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04369473084807396\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007359629962593317\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018394343554973602\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004935904289595783\n",
      "epoch: 96 ( 1 ) recon_loss: 0.5123278498649597  perplexity:  191.8776397705078  commit_loss:  0.18607373535633087 \n",
      "\t codebook loss:  0.7442949414253235  total_loss:  1.295959234237671 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.2342, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.5234730243682861\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03731381148099899\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.875720500946045\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.03607097640633583\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.5737102031707764\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.033048611134290695\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.9351385831832886\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.03510143980383873\n",
      "After scaling - encoder.proj.weight: grad norm 3.601390838623047\n",
      "After scaling - encoder.proj.bias: grad norm 0.05920473486185074\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.134201169013977\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.04603205621242523\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.5418410301208496\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.04450586065649986\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.8513998985290527\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.042220816016197205\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.5541980266571045\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.06418117880821228\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.532111406326294\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.042989134788513184\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01838112436234951\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004502014780882746\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.0226310882717371\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004352063115220517\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.018987249583005905\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00039874063804745674\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02334798499941826\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00042350857984274626\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04345178231596947\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007143216207623482\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013684452511370182\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005553895025514066\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04273329675197601\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0005369755672290921\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.046468205749988556\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0005094058578833938\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04288238659501076\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007743637543171644\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01848534867167473\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005186758353374898\n",
      "Are there any dead codes on this epoch?  168\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.2737, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.83643639087677\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.0428163968026638\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.3918614387512207\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.04368290305137634\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.9383606910705566\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.03972156345844269\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.358755111694336\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.042488932609558105\n",
      "After scaling - encoder.proj.weight: grad norm 4.424041271209717\n",
      "After scaling - encoder.proj.bias: grad norm 0.0695689469575882\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.3936827182769775\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.05395276099443436\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 4.413697719573975\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.052473124116659164\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 4.505187511444092\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.04672446474432945\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.872269868850708\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.06663963943719864\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.7242752313613892\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.04382430016994476\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01857924833893776\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.000433173991041258\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.024198491126298904\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00044194047222845256\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019610418006777763\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0004018635081592947\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02386355586349964\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004298610729165375\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04475807771086693\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007038299227133393\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014099904336035252\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.000545840710401535\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04465343430638313\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.000530871213413775\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.045579034835100174\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00047271198127418756\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03917580097913742\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006741940160281956\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.017444513738155365\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004433710128068924\n",
      "epoch: 97 ( 1 ) recon_loss: 0.48998522758483887  perplexity:  191.42071533203125  commit_loss:  0.18610426783561707 \n",
      "\t codebook loss:  0.7444170713424683  total_loss:  1.27370023727417 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.2692, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.0326907634735107\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.04538564756512642\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.5223495960235596\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.0434768944978714\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.0513875484466553\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.039615511894226074\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.5169527530670166\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.04263002425432205\n",
      "After scaling - encoder.proj.weight: grad norm 4.568458080291748\n",
      "After scaling - encoder.proj.bias: grad norm 0.0714702308177948\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.3902019262313843\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.05103534087538719\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 4.3544182777404785\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.04959273710846901\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 4.696260929107666\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.04696012660861015\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 4.4097466468811035\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.07280252128839493\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.9215437173843384\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.04898912087082863\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.019557563588023186\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00043667867430485785\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.024268822744488716\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004183135461062193\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019737454131245613\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003811612259596586\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.024216895923018456\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00041016540490090847\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.0439554825425148\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.000687651801854372\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013375847600400448\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004910372081212699\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04189610108733177\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004771572130266577\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.0451851412653923\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0004518275090958923\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04242844134569168\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007004704675637186\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018488159403204918\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00047134951455518603\n",
      "Are there any dead codes on this epoch?  163\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.2397, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.4091460704803467\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03418533504009247\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.870895504951477\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.035934872925281525\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.5151352882385254\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.03259870782494545\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.8709971904754639\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.03425457328557968\n",
      "After scaling - encoder.proj.weight: grad norm 3.549525260925293\n",
      "After scaling - encoder.proj.bias: grad norm 0.05632083863019943\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.0850014686584473\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.044654183089733124\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.3427412509918213\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.04215943440794945\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.445181369781494\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.03849588334560394\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.025702476501465\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.056770265102386475\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.3100887537002563\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.037977252155542374\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01834695413708687\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.000445089943241328\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02435888908803463\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00046786878374405205\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01972692459821701\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00042443221900612116\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.024360213428735733\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004459914634935558\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.046214498579502106\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007332921377383173\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014126620255410671\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005813933676108718\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.0435221903026104\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0005489120376296341\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04485594853758812\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0005012128967791796\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03939437121152878\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007391436374746263\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.017057236284017563\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.000494460400659591\n",
      "epoch: 98 ( 1 ) recon_loss: 0.4511950612068176  perplexity:  193.42608642578125  commit_loss:  0.1872629076242447 \n",
      "\t codebook loss:  0.7490516304969788  total_loss:  1.2397444248199463 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.2406, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.9666346311569214\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03156554698944092\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.6378979682922363\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.029933901503682137\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.030749797821045\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.027862612158060074\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.420701742172241\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.03009701892733574\n",
      "After scaling - encoder.proj.weight: grad norm 4.670834541320801\n",
      "After scaling - encoder.proj.bias: grad norm 0.05116702988743782\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.2996978759765625\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03526557981967926\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.7542319297790527\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.033868540078401566\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.9052646160125732\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.0325574092566967\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.685450315475464\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.05033890902996063\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.7209370136260986\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03397931531071663\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02070501074194908\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00033232662826776505\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.027772167697548866\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.000315148412482813\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.021380024030804634\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00029334158170968294\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02548549510538578\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003168657422065735\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04917521774768829\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005386938573792577\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013683405704796314\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0003712810867000371\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03952509164810181\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00035657285479828715\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04111518710851669\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003427690826356411\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03880095109343529\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.000529975222889334\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018118273466825485\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0003577391034923494\n",
      "Are there any dead codes on this epoch?  164\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.2355, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.5467967987060547\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03451181948184967\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.093003988265991\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.037179894745349884\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.6210615634918213\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.03325799107551575\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.9429699182510376\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.03328768536448479\n",
      "After scaling - encoder.proj.weight: grad norm 3.8533501625061035\n",
      "After scaling - encoder.proj.bias: grad norm 0.055445145815610886\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.2404390573501587\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.04631403833627701\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.698061466217041\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.04278096556663513\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.9150822162628174\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.039900463074445724\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.561011791229248\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.05981201305985451\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.5467133522033691\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03874162957072258\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.018022798001766205\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004021210188511759\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02438703365623951\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00043320859549567103\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01888810656964779\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003875117690768093\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02263888530433178\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003878577845171094\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04489804059267044\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006460296572186053\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014453211799263954\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005396368214860559\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04308866709470749\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004984705592505634\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.045617323368787766\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0004649078182410449\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.041491806507110596\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006969110108911991\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018021825700998306\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00045140544534660876\n",
      "epoch: 99 ( 1 ) recon_loss: 0.45939165353775024  perplexity:  192.89297485351562  commit_loss:  0.1843128651380539 \n",
      "\t codebook loss:  0.7372514605522156  total_loss:  1.2354750633239746 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.2300, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.7506427764892578\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.025198837742209435\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.45796275138855\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.02467837557196617\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.8550364971160889\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.022641360759735107\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.2644872665405273\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.02431335113942623\n",
      "After scaling - encoder.proj.weight: grad norm 4.541285037994385\n",
      "After scaling - encoder.proj.bias: grad norm 0.03953783959150314\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.2355417013168335\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.02751041017472744\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.3418798446655273\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.02518995851278305\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.2365882396698\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.024368280544877052\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.0529253482818604\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04069853574037552\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.4868885278701782\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.02809818647801876\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.020488537847995758\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0002949130430351943\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.028766615316271782\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00028882184415124357\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.02171030454337597\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0002649817615747452\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02650228701531887\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00028454980929382145\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.053148649632930756\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.00046272866893559694\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014460084959864616\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0003219663631170988\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.039111483842134476\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0002948091132566333\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03787920996546745\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00028519268380478024\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.035729724913835526\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.00047631279448978603\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01740170828998089\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00032884537358768284\n",
      "Are there any dead codes on this epoch?  168\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.2373, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.45711350440979\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.030244342982769012\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.9375907182693481\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.03312154486775398\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.5101834535598755\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.029949745163321495\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.805506706237793\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.02985648810863495\n",
      "After scaling - encoder.proj.weight: grad norm 3.5418825149536133\n",
      "After scaling - encoder.proj.bias: grad norm 0.05021805316209793\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.095227599143982\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.04087543115019798\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.288612127304077\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.03837138041853905\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.4043242931365967\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.03536931425333023\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.084176778793335\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.052705951035022736\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.3349173069000244\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03413146734237671\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.018990030512213707\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00039416353683918715\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02525191567838192\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004316610866226256\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019681671634316444\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00039032415952533484\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.023530513048171997\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003891087544616312\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04616006836295128\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006544736679643393\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01427370309829712\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005327146500349045\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.042859286069869995\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0005000802339054644\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04436732083559036\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00046095543075352907\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.040194954723119736\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006868974887765944\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01739749312400818\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00044482300290837884\n",
      "epoch: 100 ( 1 ) recon_loss: 0.43979594111442566  perplexity:  194.57139587402344  commit_loss:  0.18942107260227203 \n",
      "\t codebook loss:  0.7576842904090881  total_loss:  1.2373441457748413 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.2387, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.8535566329956055\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.019207892939448357\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.629218101501465\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.020471151918172836\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.9042080640792847\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.01814596727490425\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.268186330795288\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.018142322078347206\n",
      "After scaling - encoder.proj.weight: grad norm 4.61707067489624\n",
      "After scaling - encoder.proj.bias: grad norm 0.029118336737155914\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.2166558504104614\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.020688345655798912\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.282493829727173\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.017840394750237465\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.049679756164551\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.01755896769464016\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.716362953186035\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.02925804629921913\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.4200646877288818\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.0207289457321167\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.021937210112810135\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00022732920479029417\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.03111731819808483\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00024228013353422284\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.022536680102348328\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00021476112306118011\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.026844434440135956\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.000214717976632528\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05464394390583038\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0003446212795097381\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014399363659322262\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00024485067115165293\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.038848962634801865\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00021114459377713501\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.036093566566705704\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00020781386410817504\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03214869275689125\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.00034627478453330696\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.016806747764348984\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0002453311753924936\n",
      "Are there any dead codes on this epoch?  166\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.2330, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.5737196207046509\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03008812665939331\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.1618502140045166\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.03239003196358681\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.625861644744873\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02928687073290348\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.9612125158309937\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.02953275293111801\n",
      "After scaling - encoder.proj.weight: grad norm 3.894986867904663\n",
      "After scaling - encoder.proj.bias: grad norm 0.049672190099954605\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.1701984405517578\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.037124358117580414\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.4404916763305664\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.035613756626844406\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.5528674125671387\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.032799504697322845\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.2871031761169434\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.05067198723554611\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.5069503784179688\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03280825540423393\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01912221498787403\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003655998152680695\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.026268569752573967\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003935701970476657\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019755790010094643\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00035586379817686975\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.023830628022551537\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003588514809962362\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04732785373926163\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006035650731064379\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014219041913747787\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.000451096857432276\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04180529713630676\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00043274153722450137\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04317077621817589\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003985456714872271\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03994147852063179\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006157135940156877\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018310904502868652\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.000398652016883716\n",
      "epoch: 101 ( 1 ) recon_loss: 0.4403257966041565  perplexity:  192.1103515625  commit_loss:  0.18827326595783234 \n",
      "\t codebook loss:  0.7530930638313293  total_loss:  1.2329976558685303 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.2196, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.8541219234466553\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.016532598063349724\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.640117883682251\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.019719313830137253\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.8378533124923706\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.017868617549538612\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.1298344135284424\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.017366722226142883\n",
      "After scaling - encoder.proj.weight: grad norm 4.327343463897705\n",
      "After scaling - encoder.proj.bias: grad norm 0.027984702959656715\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.218035340309143\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.02096676453948021\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.37638521194458\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.017649073153734207\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.1422677040100098\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.017615951597690582\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.6546638011932373\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.02419031411409378\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.4534956216812134\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.017080973833799362\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.022326000034809113\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00019907364912796766\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.03179039806127548\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0002374457399128005\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.022130103781819344\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0002151609951397404\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02564593218266964\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00020911754108965397\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05210675299167633\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0003369716287124902\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014666703529655933\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00025246667792089283\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04065599665045738\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00021251742145977914\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03783692046999931\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00021211858256720006\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03196554630994797\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0002912823110818863\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.017501946538686752\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0002056767843896523\n",
      "Are there any dead codes on this epoch?  165\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.2311, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.6371999979019165\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.029056822881102562\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.2073476314544678\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.029921403154730797\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.6746158599853516\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02752072550356388\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.056405782699585\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.02851065620779991\n",
      "After scaling - encoder.proj.weight: grad norm 3.9661388397216797\n",
      "After scaling - encoder.proj.bias: grad norm 0.04867604747414589\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.1108075380325317\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03574899211525917\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.3160805702209473\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.034873317927122116\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.4804115295410156\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.032057542353868484\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.101280927658081\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.047467250376939774\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.431550145149231\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.031661875545978546\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.020100904628634453\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003567483799997717\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.027100957930088043\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003673633618745953\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.020560283213853836\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00033788877772167325\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02524774894118309\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003500427701510489\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.048694711178541183\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005976256215944886\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013638063333928585\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004389122477732599\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04071354866027832\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00042816103086806834\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.042731139808893204\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003935900458600372\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03807632252573967\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005827844724990427\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01757601648569107\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0003887322382070124\n",
      "epoch: 102 ( 1 ) recon_loss: 0.4279308319091797  perplexity:  194.80023193359375  commit_loss:  0.19078432023525238 \n",
      "\t codebook loss:  0.7631372809410095  total_loss:  1.2311311960220337 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.2177, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.0082128047943115\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.015329518355429173\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.816826343536377\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.019427087157964706\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.8836336135864258\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.017366508021950722\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.0730276107788086\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.01492789201438427\n",
      "After scaling - encoder.proj.weight: grad norm 4.3880534172058105\n",
      "After scaling - encoder.proj.bias: grad norm 0.023978067561984062\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.270074725151062\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.02118208073079586\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.5666491985321045\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.018143190070986748\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.3372445106506348\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.017120083793997765\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.8331456184387207\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.02386152371764183\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.5944160223007202\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.016675623133778572\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02317158505320549\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0001768782822182402\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.0325017012655735\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00022415773128159344\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.021734138950705528\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00020038190996274352\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.023919446393847466\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00017224415205419064\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.050631169229745865\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.000276668812148273\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014654644764959812\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0002444075362291187\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04115346819162369\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00020934359054081142\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03850650042295456\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00019753856759052724\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03268999978899956\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0002753240696620196\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018397027626633644\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00019241019617766142\n",
      "Are there any dead codes on this epoch?  160\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.2464, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.7122161388397217\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03349387273192406\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.283201217651367\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.03324795886874199\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.7749632596969604\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.03058011643588543\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.2055366039276123\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.03263461962342262\n",
      "After scaling - encoder.proj.weight: grad norm 4.098304271697998\n",
      "After scaling - encoder.proj.bias: grad norm 0.056959424167871475\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.1643412113189697\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.04010145366191864\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.543612003326416\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.03846214339137077\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.8761003017425537\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.035944268107414246\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.4196383953094482\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.0517549030482769\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.5325907468795776\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.034812282770872116\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.019646771252155304\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00038432443398050964\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.026198523119091988\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003815027012024075\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.020366761833429337\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00035089062293991446\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.025307361036539078\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003744649875443429\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04702586680650711\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.000653579190839082\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01336019765585661\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.000460142910014838\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04066107049584389\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004413326969370246\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04447619616985321\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00041244144085794687\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03923853859305382\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005938600515946746\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.017585666850209236\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00039945245953276753\n",
      "epoch: 103 ( 1 ) recon_loss: 0.43314307928085327  perplexity:  192.08946228027344  commit_loss:  0.1931830197572708 \n",
      "\t codebook loss:  0.7727320790290833  total_loss:  1.2463982105255127 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.2088, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.991564393043518\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.016173698008060455\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.822974681854248\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.01982121169567108\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.8507095575332642\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.01805463433265686\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.041093587875366\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.01630404405295849\n",
      "After scaling - encoder.proj.weight: grad norm 4.334780216217041\n",
      "After scaling - encoder.proj.bias: grad norm 0.026870599016547203\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.22904372215271\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.02267804741859436\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.5056333541870117\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.019039612263441086\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.2703697681427\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.01810043677687645\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.755319595336914\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.025358470156788826\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.5401743650436401\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.01810976304113865\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.023344630375504494\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0001895841269288212\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.033090218901634216\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00023233940009959042\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.02169356495141983\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00021163201017770916\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02392520010471344\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00019111200526822358\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05081122741103172\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.00031497058807872236\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01440655067563057\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00026582652935758233\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.041092175990343094\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0002231776888947934\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.038334473967552185\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00021216888853814453\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03229718282818794\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.00029724580235779285\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01805354654788971\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00021227820252534002\n",
      "Are there any dead codes on this epoch?  157\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.2376, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.692286729812622\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.036576252430677414\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.141547441482544\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.034987568855285645\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.7300478219985962\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.032029811292886734\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.250485420227051\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.034724678844213486\n",
      "After scaling - encoder.proj.weight: grad norm 4.3617987632751465\n",
      "After scaling - encoder.proj.bias: grad norm 0.06002167612314224\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.2462561130523682\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.04599880427122116\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.703284978866577\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.04226541891694069\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.9145467281341553\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.04007602483034134\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.485978364944458\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.059599876403808594\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.5718308687210083\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.04026990011334419\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.018948066979646683\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00040953420102596283\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02397831715643406\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003917461435776204\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01937086693942547\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003586289822123945\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.025198064744472504\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003888026694767177\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04883785545825958\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006720461533404887\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013953985646367073\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005150358774699271\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.041464656591415405\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004732342204079032\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04383010044693947\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0004487202095333487\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03903153911232948\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006673233583569527\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.017599355429410934\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004508909769356251\n",
      "epoch: 104 ( 1 ) recon_loss: 0.4390738904476166  perplexity:  197.39085388183594  commit_loss:  0.18968293070793152 \n",
      "\t codebook loss:  0.7587317228317261  total_loss:  1.2375520467758179 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.2059, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.5214128494262695\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.01712305285036564\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 3.5290544033050537\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.020605675876140594\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.3407328128814697\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.01946345530450344\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.599755048751831\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.018957344815135002\n",
      "After scaling - encoder.proj.weight: grad norm 5.466825485229492\n",
      "After scaling - encoder.proj.bias: grad norm 0.0338960662484169\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.5050307512283325\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.022354500368237495\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 4.240815162658691\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.018947236239910126\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 4.009695529937744\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.017528589814901352\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.335423707962036\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.022491242736577988\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.8979567289352417\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.016110744327306747\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.023825662210583687\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00016180137754417956\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.033347200602293015\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00019470983534120023\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.022118357941508293\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00018391662160865963\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.024565942585468292\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.000179134207428433\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.051657840609550476\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0003202951338607818\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01422153227031231\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00021123506303410977\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04007286578416824\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.000179038688656874\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03788893669843674\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.000165633435244672\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03151752054691315\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.00021252718579489738\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.017934419214725494\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00015223576338030398\n",
      "Are there any dead codes on this epoch?  163\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.2678, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.8053020238876343\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.04255581647157669\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.200493097305298\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.04034613445401192\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.8151822090148926\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.03614196181297302\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.3989410400390625\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.03825563192367554\n",
      "After scaling - encoder.proj.weight: grad norm 4.705957889556885\n",
      "After scaling - encoder.proj.bias: grad norm 0.06542132049798965\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.36444091796875\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.05137835443019867\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.999671697616577\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.04655143618583679\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 4.314316272735596\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.04483475163578987\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.790482521057129\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.06575452536344528\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.6383376121520996\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.0445331446826458\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.018735723569989204\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004416513256728649\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.022837083786725998\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004187188751529902\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01883826218545437\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003750872565433383\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02489660680294037\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00039702330832369626\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04883921146392822\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006789533654227853\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014160394668579102\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005332131404429674\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04150925949215889\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00048311852151528\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04477468878030777\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0004653025243896991\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.039338257163763046\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006824113661423326\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01700294017791748\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00046217237832024693\n",
      "epoch: 105 ( 1 ) recon_loss: 0.4386321008205414  perplexity:  191.85311889648438  commit_loss:  0.19698765873908997 \n",
      "\t codebook loss:  0.7879506349563599  total_loss:  1.2678163051605225 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.1983, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.190840005874634\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.021572021767497063\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 3.02882719039917\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.022907551378011703\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.0510971546173096\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.021807942539453506\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.356022596359253\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.024646347388625145\n",
      "After scaling - encoder.proj.weight: grad norm 4.747951030731201\n",
      "After scaling - encoder.proj.bias: grad norm 0.04332144930958748\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.3193244934082031\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.02667263150215149\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.769407272338867\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.02244045026600361\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.6330578327178955\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.02217651903629303\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.196176290512085\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.036314431577920914\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.7226296663284302\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.025838354602456093\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02327348105609417\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00022916140733286738\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.03217548877000809\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0002433488698443398\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.021788982674479485\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00023166764003690332\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.025028228759765625\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0002618202706798911\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.050437889993190765\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.000460207462310791\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01401529647409916\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0002833456383086741\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04004273563623428\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00023838679771870375\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.038594286888837814\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00023558305110782385\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03395325317978859\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0003857713018078357\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018299641087651253\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0002744830562733114\n",
      "Are there any dead codes on this epoch?  156\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.1944, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.2904285192489624\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.034759197384119034\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.5343544483184814\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.03326345235109329\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.331784963607788\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02969163842499256\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.740747332572937\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.031209345906972885\n",
      "After scaling - encoder.proj.weight: grad norm 3.3382303714752197\n",
      "After scaling - encoder.proj.bias: grad norm 0.05291534960269928\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.057534098625183\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.04378291219472885\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.1484735012054443\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.03983014076948166\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.528153419494629\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.03883231803774834\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.229585647583008\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.05838803946971893\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.3662760257720947\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03990231826901436\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.017281124368309975\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00046548727550543845\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02054772526025772\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004454565641935915\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.017834961414337158\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00039762366213835776\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.023311691358685493\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00041794843855313957\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04470482096076012\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007086303667165339\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014162255451083183\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005863307742401958\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04216364026069641\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0005333962035365403\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.047248225659132004\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0005200335872359574\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04324987530708313\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007819192833267152\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018296856433153152\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005343627999536693\n",
      "epoch: 106 ( 1 ) recon_loss: 0.4182121753692627  perplexity:  197.0678253173828  commit_loss:  0.18440203368663788 \n",
      "\t codebook loss:  0.7376081347465515  total_loss:  1.1943789720535278 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.1509, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.7351438999176025\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.018643304705619812\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.381084442138672\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.018638432025909424\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.6331937313079834\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.01761116459965706\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.9519109725952148\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.020489761605858803\n",
      "After scaling - encoder.proj.weight: grad norm 3.8794631958007812\n",
      "After scaling - encoder.proj.bias: grad norm 0.036120567470788956\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.0610332489013672\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.021535977721214294\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.0001447200775146\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.01793728582561016\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.006528615951538\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.018223311752080917\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.702888011932373\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.0295094046741724\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.4276878833770752\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.020875323563814163\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.022642914205789566\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0002432874753139913\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.03107217513024807\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00024322389799635857\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.02131250873208046\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00022981846996117383\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.025471635162830353\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.000267382973106578\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.050625402480363846\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.00047135859495028853\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013846048153936863\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00028103566728532314\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.0391506552696228\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00023407420667354017\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.039233963936567307\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0002378067292738706\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03527158126235008\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0003850856446661055\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01863074116408825\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00027241441421210766\n",
      "Are there any dead codes on this epoch?  154\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.2112, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.4698610305786133\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03876970708370209\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.7555263042449951\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.03667118027806282\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.4958443641662598\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.03239548206329346\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.9392436742782593\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.034444309771060944\n",
      "After scaling - encoder.proj.weight: grad norm 3.582901954650879\n",
      "After scaling - encoder.proj.bias: grad norm 0.05861581489443779\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.0680394172668457\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.04489579796791077\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.2480661869049072\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.04085062816739082\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.7244713306427\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.04005124792456627\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.400564193725586\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.059745945036411285\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.4474058151245117\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.04057779163122177\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01847195439040661\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00048722446081228554\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.022061949595808983\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004608519375324249\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01879849098622799\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0004071186704095453\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.024370752274990082\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00043286653817631304\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04502684250473976\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007366333738900721\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013422204181551933\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.000564211921300739\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04081891477108002\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0005133756785653532\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.046805962920188904\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0005033297347836196\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04273537918925285\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007508358103223145\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018189754337072372\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005099468980915844\n",
      "epoch: 107 ( 1 ) recon_loss: 0.41315972805023193  perplexity:  194.5085906982422  commit_loss:  0.1895975023508072 \n",
      "\t codebook loss:  0.7583900094032288  total_loss:  1.2111537456512451 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.1380, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.380213975906372\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.01768002286553383\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.8782265186309814\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.017506254836916924\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.3410346508026123\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.01652473583817482\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.6479520797729492\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.01913638971745968\n",
      "After scaling - encoder.proj.weight: grad norm 3.1947317123413086\n",
      "After scaling - encoder.proj.bias: grad norm 0.03288241848349571\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.869304358959198\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.021053355187177658\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.4827189445495605\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.017827779054641724\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.61201548576355\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.01834699884057045\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.505614995956421\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.0310360100120306\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.2441896200180054\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.02152794972062111\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02136688306927681\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00027370176394470036\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02907654084265232\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00027101169689558446\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.02076035365462303\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0002558169362600893\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02551169879734516\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0002962475409731269\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.049457158893346786\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005090477643534541\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013457569293677807\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0003259238146711141\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.038434598594903946\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0002759891503956169\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.040436215698719025\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.000284027133602649\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.0387890450656414\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.00048046375741250813\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.019261110574007034\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0003332709602545947\n",
      "Are there any dead codes on this epoch?  156\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.1890, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.7128618955612183\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.04111112281680107\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.118760347366333\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.03893328458070755\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.7750285863876343\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.03506205976009369\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.28485107421875\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.03814319893717766\n",
      "After scaling - encoder.proj.weight: grad norm 4.21619176864624\n",
      "After scaling - encoder.proj.bias: grad norm 0.0646924078464508\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.1482789516448975\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.04640921950340271\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.37976336479187\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.04135490953922272\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.8535377979278564\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.04081442952156067\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.628065824508667\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.06419732421636581\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.6019821166992188\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.04468049854040146\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01953095570206642\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004687707405537367\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.024159224703907967\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00044393789721652865\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.0202398132532835\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003997961466666311\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.026053080335259438\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004349288938101381\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04807524383068085\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007376570720225573\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013093283399939537\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005291824927553535\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.038537848740816116\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004715505347121507\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04394007846713066\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00046538771130144596\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04136912524700165\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007320118020288646\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018266648054122925\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005094706430099905\n",
      "epoch: 108 ( 1 ) recon_loss: 0.42364197969436646  perplexity:  197.76516723632812  commit_loss:  0.181862935423851 \n",
      "\t codebook loss:  0.727451741695404  total_loss:  1.1890430450439453 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.1550, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.8427577018737793\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.02726735733449459\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.4737727642059326\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.02544395439326763\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.809705376625061\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.023525569587945938\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.247154712677002\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.02651146799325943\n",
      "After scaling - encoder.proj.weight: grad norm 4.373940467834473\n",
      "After scaling - encoder.proj.bias: grad norm 0.045178938657045364\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.1475746631622314\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.030311884358525276\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.2814877033233643\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.027020806446671486\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.5144457817077637\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.027724042534828186\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.347914218902588\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04525194317102432\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.607420563697815\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03131011873483658\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02124856412410736\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00031441583996638656\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02852470427751541\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0002933904470410198\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.020867442712187767\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0002712698478717357\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02591160498559475\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003056997957173735\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05043525993824005\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005209516384638846\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013232512399554253\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0003495218406897038\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.037838343530893326\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00031157289049588144\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04052455350756645\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003196817997377366\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03860430046916008\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005217934376560152\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01853492669761181\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0003610323474276811\n",
      "Are there any dead codes on this epoch?  157\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.1590, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.3963035345077515\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.0298257227987051\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.7657945156097412\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.028560420498251915\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.3979572057724\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02596626803278923\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.7547457218170166\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.028607971966266632\n",
      "After scaling - encoder.proj.weight: grad norm 3.238445520401001\n",
      "After scaling - encoder.proj.bias: grad norm 0.04882226884365082\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.8997500538825989\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03435617312788963\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.6105856895446777\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.030103666707873344\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.9668281078338623\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.029852330684661865\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.781403064727783\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04754149168729782\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.2384717464447021\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03244916722178459\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02052116021513939\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00043834198731929064\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.025951487943530083\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004197461239527911\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.020545465871691704\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00038162042619660497\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.025789104402065277\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004204449651297182\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.047594714909791946\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007175300270318985\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013223426416516304\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.000504925032146275\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.038367193192243576\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004424268554430455\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04360281303524971\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00043873305548913777\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04087765887379646\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006987067172303796\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018201543018221855\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00047689818893559277\n",
      "epoch: 109 ( 1 ) recon_loss: 0.38978293538093567  perplexity:  195.11947631835938  commit_loss:  0.18278850615024567 \n",
      "\t codebook loss:  0.7311540246009827  total_loss:  1.1590405702590942 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.1537, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.6416130065917969\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.021853972226381302\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.252917766571045\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.02086302824318409\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.7161024808883667\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.019092461094260216\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.305011749267578\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.021811826154589653\n",
      "After scaling - encoder.proj.weight: grad norm 4.60595703125\n",
      "After scaling - encoder.proj.bias: grad norm 0.03724713996052742\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.1178066730499268\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.025679247453808784\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.0944128036499023\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.022300712764263153\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.1664814949035645\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.023114720359444618\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.1533734798431396\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04040759429335594\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.4558985233306885\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.027587661519646645\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.019597861915826797\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0002608965151011944\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02689572609961033\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00024906647740863264\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.02048713155090809\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00022792912204749882\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02751763164997101\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0002603933971840888\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05498671531677246\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.00044466290273703635\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013344570994377136\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00030656333547085524\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.036941640079021454\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00026622979203239083\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03780200704932213\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0002759475610218942\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03764551877975464\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0004823929339181632\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.017380770295858383\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0003293462796136737\n",
      "Are there any dead codes on this epoch?  156\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.1302, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.164425015449524\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.026497097685933113\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.4842190742492676\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.02549681067466736\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.1989127397537231\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.022939426824450493\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.5420032739639282\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.025436988100409508\n",
      "After scaling - encoder.proj.weight: grad norm 2.7959787845611572\n",
      "After scaling - encoder.proj.bias: grad norm 0.042723480612039566\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.7788981199264526\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.02961849793791771\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.2210936546325684\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.02467823587357998\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.5022695064544678\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.024293087422847748\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.5217390060424805\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.041772060096263885\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.137450933456421\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.028770938515663147\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01978018507361412\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004501083749346435\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.025212552398443222\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.000433116452768445\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.02036602981388569\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00038967395084910095\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.026194138452410698\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004321002052165568\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.0474955253303051\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007257473189383745\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013231206685304642\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005031318869441748\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.0377299040555954\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00041921125375665724\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04250626266002655\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00041266868356615305\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.0428369902074337\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.000709585496224463\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.019321972504258156\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004887343384325504\n",
      "epoch: 110 ( 1 ) recon_loss: 0.3766992688179016  perplexity:  197.4368133544922  commit_loss:  0.1790483593940735 \n",
      "\t codebook loss:  0.716193437576294  total_loss:  1.1301790475845337 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.1410, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.2004618644714355\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.015407650731503963\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.7300056219100952\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.015341870486736298\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.3238036632537842\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.01353794801980257\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.8279216289520264\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.014702455140650272\n",
      "After scaling - encoder.proj.weight: grad norm 3.7195394039154053\n",
      "After scaling - encoder.proj.bias: grad norm 0.024948805570602417\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.9237679243087769\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.018032468855381012\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.462350845336914\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.01495429128408432\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.398953914642334\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.015321444720029831\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.5784823894500732\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.029828371480107307\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.164875864982605\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.019920719787478447\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01808873936533928\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00023216479166876525\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.026067985221743584\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00023117360251490027\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019947271794080734\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00020399183267727494\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02754339948296547\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0002215387939941138\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05604657530784607\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0003759323444683105\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013919474557042122\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00027171592228114605\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.037103071808815\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00022533351148013026\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03614779934287071\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0002308658295078203\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.038852959871292114\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0004494583990890533\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01755252294242382\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00030016840901225805\n",
      "Are there any dead codes on this epoch?  155\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.1247, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.3050992488861084\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.025935525074601173\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.7026546001434326\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.02538294717669487\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.3320647478103638\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.022970767691731453\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.7312343120574951\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.025849152356386185\n",
      "After scaling - encoder.proj.weight: grad norm 3.1821537017822266\n",
      "After scaling - encoder.proj.bias: grad norm 0.043813638389110565\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.8480899930000305\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.02926294319331646\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.339595317840576\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.023527856916189194\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.6309139728546143\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.02373824268579483\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.6476876735687256\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04112016782164574\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.188602089881897\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.028669433668255806\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02037064917385578\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00040481481119059026\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02657589688897133\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00039618986193090677\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.020791539922356606\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00035853934241458774\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02702198177576065\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00040346666355617344\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04966866225004196\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006838655099272728\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013237417675554752\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004567508294712752\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.036517586559057236\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00036723469384014606\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04106463864445686\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00037051853723824024\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04132645204663277\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006418243865482509\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018552303314208984\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00044748702202923596\n",
      "epoch: 111 ( 1 ) recon_loss: 0.3783711791038513  perplexity:  194.4318084716797  commit_loss:  0.17736512422561646 \n",
      "\t codebook loss:  0.7094604969024658  total_loss:  1.1247305870056152 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.1433, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.2380526065826416\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.01990353874862194\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.802444338798523\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.020021185278892517\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.4455089569091797\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.01755191758275032\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.9795984029769897\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.018714522942900658\n",
      "After scaling - encoder.proj.weight: grad norm 4.0064897537231445\n",
      "After scaling - encoder.proj.bias: grad norm 0.031330402940511703\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.953517496585846\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.022459203377366066\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.536665439605713\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.017946360632777214\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.493870496749878\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.018073951825499535\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.775866746902466\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.036616891622543335\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.20468008518219\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.02369965799152851\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.017584964632987976\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00028270448092371225\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.025601431727409363\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0002843754773493856\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.02053161710500717\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0002493026840966195\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.028117679059505463\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.000265815993770957\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05690709501504898\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.000445008568931371\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013543504290282726\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00031900443718768656\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.036030109971761703\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00025490522966720164\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03542225807905197\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.000256717496085912\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03942766413092613\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.000520096393302083\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.017110951244831085\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0003366235177963972\n",
      "Are there any dead codes on this epoch?  153\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.1245, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.3467179536819458\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.025395413860678673\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.77054762840271\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.025154856964945793\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.3697667121887207\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02246316708624363\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.829587459564209\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.024980848655104637\n",
      "After scaling - encoder.proj.weight: grad norm 3.5838606357574463\n",
      "After scaling - encoder.proj.bias: grad norm 0.04244813695549965\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.9907410740852356\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.029317444190382957\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.624276876449585\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.0231269933283329\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.7650444507598877\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.0231493953615427\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.7021026611328125\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04022632911801338\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.2631245851516724\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.02779311127960682\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.0195535309612751\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00036872606142424047\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02570728026330471\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003652332816272974\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01988818496465683\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003261515812482685\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.026564504951238632\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00036270683631300926\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.052035488188266754\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006163212819956243\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014384961687028408\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00042567154741846025\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03810291364789009\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003357899549882859\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04014677181839943\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003361152485013008\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03923289105296135\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005840619560331106\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01833980530500412\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00040353916119784117\n",
      "epoch: 112 ( 1 ) recon_loss: 0.3824223279953003  perplexity:  195.7626953125  commit_loss:  0.17636197805404663 \n",
      "\t codebook loss:  0.7054479122161865  total_loss:  1.1245241165161133 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.1411, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.0977412462234497\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.023884912952780724\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.5419224500656128\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.024540487676858902\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.2556711435317993\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.021035009995102882\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.699974536895752\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.021622130647301674\n",
      "After scaling - encoder.proj.weight: grad norm 3.421600818634033\n",
      "After scaling - encoder.proj.bias: grad norm 0.03594620153307915\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.842021644115448\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.02549753710627556\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.252993583679199\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.020212600007653236\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.2458009719848633\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.02008446864783764\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.5267627239227295\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.039852481335401535\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.0661181211471558\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.026315554976463318\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.017781199887394905\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003868875792250037\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.024976041167974472\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00039750654832459986\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.020339347422122955\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003407248295843601\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.027536168694496155\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003502350300550461\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05542305111885071\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005822561797685921\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.0136390570551157\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004130088200327009\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03649396449327469\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003274034825153649\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03637745976448059\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003253279719501734\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04092847555875778\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.000645530060864985\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.017268968746066093\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004262590373400599\n",
      "Are there any dead codes on this epoch?  154\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.1065, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.236654281616211\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.017528044059872627\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.6880548000335693\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.017540715634822845\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.2064889669418335\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.015642084181308746\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.622333288192749\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.01665368117392063\n",
      "After scaling - encoder.proj.weight: grad norm 3.4935619831085205\n",
      "After scaling - encoder.proj.bias: grad norm 0.029246922582387924\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.9837537407875061\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.02268453687429428\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.5473201274871826\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.018624600023031235\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.4824061393737793\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.018277712166309357\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.2097830772399902\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.030468717217445374\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.0696040391921997\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.020799772813916206\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.019474966451525688\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00027603356284089386\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02658367156982422\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0002762331278063357\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.018999923020601273\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0002463332493789494\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.025548681616783142\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00026226398767903447\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05501699447631836\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0004605837457347661\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01549226138740778\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.000357238546712324\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04011547937989235\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00029330223333090544\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03909320384263992\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00028783941525034606\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.034799907356500626\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.00047982469550333917\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.016844240948557854\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0003275570925325155\n",
      "epoch: 113 ( 1 ) recon_loss: 0.36609047651290894  perplexity:  194.83888244628906  commit_loss:  0.17596399784088135 \n",
      "\t codebook loss:  0.7038559913635254  total_loss:  1.1064817905426025 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.1240, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.1455191373825073\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.02496189996600151\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.5413261651992798\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.025667261332273483\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.243156909942627\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.021890398114919662\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.613776683807373\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.02219390496611595\n",
      "After scaling - encoder.proj.weight: grad norm 3.2696001529693604\n",
      "After scaling - encoder.proj.bias: grad norm 0.0367610864341259\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.8589539527893066\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.027175122871994972\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.302978754043579\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.021827131509780884\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.400367259979248\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.02216663770377636\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.7446258068084717\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.044090595096349716\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.189894199371338\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.028930136933922768\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.018275609239935875\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003982420894317329\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02459031343460083\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.000409495405619964\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01983332261443138\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00034923935891129076\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.025746189057826996\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00035408150870352983\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05216319113969803\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005864862469024956\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013703749515116215\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004335518169682473\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.0367417111992836\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003482300089672208\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.038295451551675797\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003536464646458626\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04378775134682655\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.000703421188518405\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018983567133545876\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004615513316821307\n",
      "Are there any dead codes on this epoch?  152\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.0955, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.088315486907959\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.018244385719299316\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.4584590196609497\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.018384288996458054\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.098598837852478\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.016445210203528404\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.4900803565979004\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.017326125875115395\n",
      "After scaling - encoder.proj.weight: grad norm 3.0781123638153076\n",
      "After scaling - encoder.proj.bias: grad norm 0.029794292524456978\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.9015694856643677\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.021967081353068352\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.379603147506714\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.019507797434926033\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.3313677310943604\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.01828937605023384\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.2849631309509277\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.031826481223106384\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.0817174911499023\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.021750271320343018\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01852330006659031\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00031052230042405427\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.024823199957609177\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003129034594167024\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.018698325380682945\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00027990006492473185\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.025361400097608566\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0002948933688458055\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.052389953285455704\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005071035120636225\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.015344854444265366\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00037388314376585186\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04050121828913689\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003320257819723338\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03968024253845215\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003112880513072014\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.038890428841114044\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005416916683316231\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018410999327898026\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0003701930108945817\n",
      "epoch: 114 ( 1 ) recon_loss: 0.3690916895866394  perplexity:  197.3782501220703  commit_loss:  0.17264340817928314 \n",
      "\t codebook loss:  0.6905736327171326  total_loss:  1.0954762697219849 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.1151, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.1522783041000366\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.024158762767910957\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.5366225242614746\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.0242705587297678\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.1831443309783936\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02056135982275009\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.5423697233200073\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.021140413358807564\n",
      "After scaling - encoder.proj.weight: grad norm 3.129750967025757\n",
      "After scaling - encoder.proj.bias: grad norm 0.03340751677751541\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.882033109664917\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.027403008192777634\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.340955972671509\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.022824859246611595\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.3331377506256104\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.02211732044816017\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.4737441539764404\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04103890806436539\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.1248376369476318\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.02780110388994217\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01911020092666149\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00040066608926281333\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.025484440848231316\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00040252017788589\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019622107967734337\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00034100422635674477\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.025579756125807762\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00035060764639638364\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05190601572394371\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.000554053985979408\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014628264121711254\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.000454470980912447\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03882407769560814\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00037854371475987136\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03869441896677017\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00036680937046185136\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04102633148431778\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006806185119785368\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01865510642528534\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004610732721630484\n",
      "Are there any dead codes on this epoch?  154\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.1063, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.2333476543426514\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.01797151006758213\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.6676565408706665\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.018977699801325798\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.215822696685791\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.016560178250074387\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.626797080039978\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.0169571153819561\n",
      "After scaling - encoder.proj.weight: grad norm 3.3328964710235596\n",
      "After scaling - encoder.proj.bias: grad norm 0.028072234243154526\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.9620845913887024\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.02011500857770443\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.531270980834961\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.016871832311153412\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.43617582321167\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.016065435484051704\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.4328389167785645\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.030748926103115082\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.2012948989868164\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.021741114556789398\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.019459817558526993\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00028355533140711486\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.026312364265322685\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0002994310634676367\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019183307886123657\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00026128729223273695\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02566768229007721\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00026755017461255193\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05258660018444061\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.00044292505481280386\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.015179816633462906\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0003173755540046841\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03993851691484451\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0002662045881152153\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.0384380966424942\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0002534812083467841\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03838545083999634\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0004851580015383661\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018954088911414146\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00034303232678212225\n",
      "epoch: 115 ( 1 ) recon_loss: 0.37337037920951843  perplexity:  196.4046630859375  commit_loss:  0.17420926690101624 \n",
      "\t codebook loss:  0.6968370676040649  total_loss:  1.1063083410263062 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.1199, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.3239161968231201\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.024194052442908287\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.7882767915725708\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.024147426709532738\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.350494384765625\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.021080514416098595\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.805692434310913\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.02264256216585636\n",
      "After scaling - encoder.proj.weight: grad norm 3.4989328384399414\n",
      "After scaling - encoder.proj.bias: grad norm 0.03548172116279602\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.9727916121482849\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.028165766969323158\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.640679359436035\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.024821726605296135\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.6296498775482178\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.023951824754476547\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.698585271835327\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04184538498520851\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.2503386735916138\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.028225285932421684\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.019527340307831764\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003568545507732779\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02637651190161705\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00035616682725958526\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019919361919164658\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003109308599960059\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.026633387431502342\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00033397055813111365\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.051608145236968994\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005233440897427499\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014348366297781467\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004154360794927925\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03894917666912079\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00036611256655305624\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.038786496967077255\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003532817936502397\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03980327397584915\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.000617206038441509\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018442098051309586\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004163139674346894\n",
      "Are there any dead codes on this epoch?  161\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.0926, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.1380512714385986\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.023475173860788345\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.4704915285110474\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.02425081841647625\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.1314032077789307\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02087804675102234\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.5105042457580566\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.02190011739730835\n",
      "After scaling - encoder.proj.weight: grad norm 2.9790291786193848\n",
      "After scaling - encoder.proj.bias: grad norm 0.03565230593085289\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.8770264983177185\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.025670640170574188\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.3240556716918945\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.021001292392611504\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.339010238647461\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.020843321457505226\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.4764158725738525\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.03904862329363823\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.142755150794983\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.027630871161818504\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.019241835922002792\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003969113458879292\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.024862639605998993\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004100256774108857\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019129430875182152\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003529998648446053\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.025539161637425423\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003702807007357478\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.050368547439575195\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006027986528351903\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014828505925834179\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.000434031622717157\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.039294447749853134\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.000355083670001477\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03954729437828064\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003524127241689712\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04187050834298134\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006602226058021188\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.019321367144584656\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004671746282838285\n",
      "epoch: 116 ( 1 ) recon_loss: 0.37045878171920776  perplexity:  198.18505859375  commit_loss:  0.17165954411029816 \n",
      "\t codebook loss:  0.6866381764411926  total_loss:  1.0926355123519897 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.1121, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.276079773902893\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.01964709162712097\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.7859258651733398\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.019458962604403496\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.3153892755508423\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.01695442385971546\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.7744821310043335\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.017916204407811165\n",
      "After scaling - encoder.proj.weight: grad norm 3.413879156112671\n",
      "After scaling - encoder.proj.bias: grad norm 0.028659086674451828\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.944631814956665\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.02088320255279541\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.4462404251098633\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.0176008902490139\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.2954368591308594\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.01689497008919716\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.3173768520355225\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.02923675999045372\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.1627256870269775\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.020135991275310516\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.020120425149798393\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00030978303402662277\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.028159359470009804\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00030681671341881156\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.020740235224366188\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0002673267154023051\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02797892317175865\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0002824914990924299\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05382790416479111\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.00045187852811068296\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014894362539052963\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0003292732289992273\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03857078775763512\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0002775198081508279\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.036193009465932846\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0002663893101271242\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03653894364833832\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0004609868919942528\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018333129584789276\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00031749167828820646\n",
      "Are there any dead codes on this epoch?  157\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.0994, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.1635255813598633\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.028579534962773323\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.4694632291793823\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.029319902881979942\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.2098338603973389\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.025377335026860237\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.5958040952682495\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.02665039338171482\n",
      "After scaling - encoder.proj.weight: grad norm 2.996286630630493\n",
      "After scaling - encoder.proj.bias: grad norm 0.043016258627176285\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.8526800274848938\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.02938251942396164\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.3860373497009277\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.024397123605012894\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.535095453262329\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.024066761136054993\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.7593181133270264\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.0452602244913578\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.257164716720581\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03212594613432884\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.018739165738224983\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004602878470905125\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.023666447028517723\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004722118319477886\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.0194849856197834\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00040871481178328395\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.025701230391860008\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00042921805288642645\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.048256706446409225\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006927985232323408\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013732842169702053\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004732203087769449\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03842833638191223\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00039292796282097697\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04082898795604706\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003876073460560292\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04444020986557007\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007289387285709381\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.02024727314710617\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005174045800231397\n",
      "epoch: 117 ( 1 ) recon_loss: 0.3719186782836914  perplexity:  196.15869140625  commit_loss:  0.1729300618171692 \n",
      "\t codebook loss:  0.6917202472686768  total_loss:  1.0994065999984741 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.1020, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.3403033018112183\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.018777841702103615\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.9180397987365723\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.01809883303940296\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.4136101007461548\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.016061898320913315\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.9327096939086914\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.017848830670118332\n",
      "After scaling - encoder.proj.weight: grad norm 3.7423954010009766\n",
      "After scaling - encoder.proj.bias: grad norm 0.030002133920788765\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.9427708387374878\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.021435203030705452\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.5192439556121826\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.01820407621562481\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.482558488845825\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.018159709870815277\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.5472981929779053\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.031763143837451935\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.2211554050445557\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.02129053883254528\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.0196345504373312\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0002750828571151942\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.028098003938794136\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0002651358372531831\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.020708445459604263\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0002352961164433509\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.028312908485531807\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00026147346943616867\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05482359975576401\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.00043951126281172037\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013810963369905949\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0003140114131383598\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03690524399280548\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00026667758356779814\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03636782988905907\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0002660276659298688\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.0373162180185318\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0004653088981285691\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.0178891122341156\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00031189218861982226\n",
      "Are there any dead codes on this epoch?  158\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.0760, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 0.8806383609771729\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.02272782288491726\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.110819697380066\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.023506103083491325\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 0.9070584774017334\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.020160773769021034\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.1523520946502686\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.02075168490409851\n",
      "After scaling - encoder.proj.weight: grad norm 2.1875722408294678\n",
      "After scaling - encoder.proj.bias: grad norm 0.033526401966810226\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.6113747954368591\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.024201493710279465\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 1.7288085222244263\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.0198198352009058\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 1.9919110536575317\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.020107872784137726\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.339646816253662\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.039092980325222015\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.013135552406311\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.02730143629014492\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.018449842929840088\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00047616002848371863\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.023272264748811722\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004924654494971037\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019003357738256454\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0004223789437673986\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.024142393842339516\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00043475886923260987\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04583080857992172\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007023960934020579\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01280862744897604\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005070341867394745\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.036219462752342224\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00041523610707372427\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04173159971833229\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0004212706407997757\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04901684820652008\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0008190188091248274\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.02122572995722294\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005719796754419804\n",
      "epoch: 118 ( 1 ) recon_loss: 0.35362929105758667  perplexity:  198.9921417236328  commit_loss:  0.17170996963977814 \n",
      "\t codebook loss:  0.6868398785591125  total_loss:  1.075951099395752 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.1001, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.1155471801757812\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.025343136861920357\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.5109584331512451\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.025879383087158203\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.1645339727401733\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02288242243230343\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.5470106601715088\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.02535388618707657\n",
      "After scaling - encoder.proj.weight: grad norm 2.987671375274658\n",
      "After scaling - encoder.proj.bias: grad norm 0.04038183391094208\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.8662760853767395\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.031342536211013794\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.568840980529785\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.027742423117160797\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.6739344596862793\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.02661127597093582\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.5540263652801514\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.042131341993808746\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.1182911396026611\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.027836348861455917\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.017982158809900284\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004085208638571203\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.024356024339795113\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004171649634372443\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.018771804869174957\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.000368855195119977\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02493717148900032\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004086941771674901\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04816003143787384\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006509384838864207\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013964012265205383\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005052287015132606\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04140865430235863\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.000447196391178295\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04310271888971329\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00042896275408566\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.041169848293066025\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.000679139862768352\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01802639104425907\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004487104306463152\n",
      "Are there any dead codes on this epoch?  160\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.0839, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 0.9668000936508179\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.024175914004445076\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.2323312759399414\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.02518814243376255\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.0008569955825806\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.021796919405460358\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.2367165088653564\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.02265525609254837\n",
      "After scaling - encoder.proj.weight: grad norm 2.4023454189300537\n",
      "After scaling - encoder.proj.bias: grad norm 0.03695664927363396\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.6800044775009155\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.023959850892424583\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.0001213550567627\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.021729208528995514\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.1529221534729004\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.020415380597114563\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.4522600173950195\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04037352278828621\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.115796685218811\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.028628593310713768\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.018564924597740173\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00046423659659922123\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.023663772270083427\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004836738808080554\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019218899309635162\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00041855411836877465\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02374797873198986\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00043503622873686254\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04613090306520462\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007096579647623003\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013057747855782509\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004600876709446311\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03840722143650055\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004172538756392896\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04134136438369751\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00039202519110403955\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.047089383006095886\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007752702222205698\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.021426023915410042\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005497389356605709\n",
      "epoch: 119 ( 1 ) recon_loss: 0.35739749670028687  perplexity:  199.81747436523438  commit_loss:  0.17271631956100464 \n",
      "\t codebook loss:  0.6908652782440186  total_loss:  1.0839194059371948 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.0548, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 0.6100925207138062\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.01569470576941967\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 0.7751971483230591\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.015714146196842194\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 0.6423054933547974\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.013489661738276482\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 0.8318771123886108\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.014088612049818039\n",
      "After scaling - encoder.proj.weight: grad norm 1.5393760204315186\n",
      "After scaling - encoder.proj.bias: grad norm 0.023271216079592705\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.4910343289375305\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.019333459436893463\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 1.4460015296936035\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.016754165291786194\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 1.7113538980484009\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.01784677803516388\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 1.9141783714294434\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.03268339857459068\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 0.8251771330833435\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.021777862682938576\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.016272258013486862\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004186058649793267\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.020675893872976303\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004191243788227439\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.017131436616182327\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00035979339736513793\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.022187650203704834\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00037576848990283906\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04105791077017784\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006206849939189851\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.0130967628210783\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005156579427421093\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.038567446172237396\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004468635597731918\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.045644864439964294\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0004760054871439934\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.05105455964803696\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0008717247401364148\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.022008948028087616\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005808544810861349\n",
      "Are there any dead codes on this epoch?  153\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.0663, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.0321124792099\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.02582498826086521\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.2479987144470215\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.02538352832198143\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.0355545282363892\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.022032935172319412\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.3648278713226318\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.02413446269929409\n",
      "After scaling - encoder.proj.weight: grad norm 2.514957904815674\n",
      "After scaling - encoder.proj.bias: grad norm 0.0394335500895977\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.778879702091217\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.030927766114473343\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.2615160942077637\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.026999391615390778\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.6071889400482178\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.02683967724442482\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.787693738937378\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04770272225141525\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.1903057098388672\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03113686852157116\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01780586875975132\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004455293237697333\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.021530311554670334\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004379133170004934\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.0178652536123991\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003801093262154609\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.023545831441879272\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004163646080996841\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.043387725949287415\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006803024443797767\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01343713141977787\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005335618043318391\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.039015382528305054\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00046578998444601893\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04497888684272766\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0004630346375051886\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.048092927783727646\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0008229612139984965\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.020534997805953026\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005371692241169512\n",
      "epoch: 120 ( 1 ) recon_loss: 0.3539723753929138  perplexity:  196.8759307861328  commit_loss:  0.16934631764888763 \n",
      "\t codebook loss:  0.6773852705955505  total_loss:  1.066286325454712 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.0924, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 0.8500062823295593\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.018725281581282616\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.132666826248169\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.018528327345848083\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 0.9078649282455444\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.016533689573407173\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.1991907358169556\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.017699573189020157\n",
      "After scaling - encoder.proj.weight: grad norm 2.324547052383423\n",
      "After scaling - encoder.proj.bias: grad norm 0.030800145119428635\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.7010831832885742\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.021034803241491318\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.012408971786499\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.018817665055394173\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.2209181785583496\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.01960531435906887\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.7445316314697266\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04022714123129845\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.1588644981384277\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.025390252470970154\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.0160561203956604\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003537095617502928\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02139541134238243\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00034998924820683897\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01714903675019741\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003123116912320256\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02265200950205326\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00033433453063480556\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04390932619571686\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005817966302856803\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013243049383163452\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0003973350685555488\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03801321983337402\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00035545462742447853\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04195184260606766\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003703328547999263\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.0518425889313221\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007598670781590044\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.021890271455049515\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00047960696974769235\n",
      "Are there any dead codes on this epoch?  156\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.0404, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 0.8403958082199097\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.02140573039650917\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.038001298904419\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.021046889945864677\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 0.8611649870872498\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.018772847950458527\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.0751104354858398\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.019507192075252533\n",
      "After scaling - encoder.proj.weight: grad norm 2.0495920181274414\n",
      "After scaling - encoder.proj.bias: grad norm 0.03378601372241974\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.6595127582550049\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.02739373780786991\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.006824016571045\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.02511158399283886\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.203247547149658\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.024099182337522507\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.277531623840332\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04126332327723503\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 0.9724195003509521\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.02768726460635662\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01739589311182499\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00044309094664640725\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.021486258134245872\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00043566309614107013\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.017825806513428688\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00038859125925228\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.022254401817917824\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004037919279653579\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04242582246661186\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006993584102019668\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013651678338646889\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005670405807904899\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.041540540754795074\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0005198007565923035\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04560643434524536\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0004988444270566106\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.0471440926194191\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0008541360730305314\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.020128736272454262\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005731164710596204\n",
      "epoch: 121 ( 1 ) recon_loss: 0.33926379680633545  perplexity:  201.0983123779297  commit_loss:  0.1667066365480423 \n",
      "\t codebook loss:  0.6668265461921692  total_loss:  1.040442943572998 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.0913, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.0678966045379639\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.02700740098953247\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.3479981422424316\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.028794344514608383\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.1059175729751587\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02475597895681858\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.2811603546142578\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.023449808359146118\n",
      "After scaling - encoder.proj.weight: grad norm 2.690770387649536\n",
      "After scaling - encoder.proj.bias: grad norm 0.041056133806705475\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.869823694229126\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03235601261258125\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.730727195739746\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.030286556109786034\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.778391122817993\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.028142601251602173\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.6842684745788574\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04429416358470917\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.1808756589889526\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.029373668134212494\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01741435006260872\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004404137434903532\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02198200672864914\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004695536626968533\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01803436130285263\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0004036994359921664\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.020892074331641197\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00038239950663410127\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.043878789991140366\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006695085321553051\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014184344559907913\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.000527634343598038\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04453037679195404\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004938874044455588\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04530763998627663\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00045892561320215464\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04377276822924614\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007223114953376353\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.0192567165941\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004790007951669395\n",
      "Are there any dead codes on this epoch?  151\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.0187, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 0.6721914410591125\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.014268670231103897\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 0.8567205667495728\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.014545654878020287\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 0.6606550216674805\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.013053862378001213\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 0.8381025791168213\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.013048947788774967\n",
      "After scaling - encoder.proj.weight: grad norm 1.7431774139404297\n",
      "After scaling - encoder.proj.bias: grad norm 0.021569961681962013\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.5047582983970642\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.015703652054071426\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 1.4258390665054321\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.013403203338384628\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 1.4133771657943726\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.013002638705074787\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 1.5412036180496216\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.024167831987142563\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 0.7528394460678101\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.016810962930321693\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.018941693007946014\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00040207707206718624\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.024141540750861168\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004098822537343949\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.018616607412695885\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00036784500116482377\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.023616904392838478\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003677064669318497\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04912102222442627\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.000607820285949856\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01422359049320221\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004425134393386543\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04017873853445053\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003776890807785094\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03982757776975632\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00036640153848566115\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.043429598212242126\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006810256745666265\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.021214274689555168\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00047371635446324944\n",
      "epoch: 122 ( 1 ) recon_loss: 0.32312291860580444  perplexity:  199.59445190429688  commit_loss:  0.1653742492198944 \n",
      "\t codebook loss:  0.6614969968795776  total_loss:  1.0186669826507568 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.0513, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 0.9579835534095764\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.021372919902205467\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.1900992393493652\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.02065039426088333\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 0.9436079263687134\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.018319610506296158\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.1287022829055786\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.019254056736826897\n",
      "After scaling - encoder.proj.weight: grad norm 2.1989471912384033\n",
      "After scaling - encoder.proj.bias: grad norm 0.033517953008413315\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.6969618201255798\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.025152232497930527\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.14719557762146\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.023453768342733383\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.1723108291625977\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.02170560508966446\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.1393046379089355\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.03459123149514198\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.0752192735671997\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.025095803663134575\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.019202785566449165\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00042842034599743783\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02385554648935795\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004139372904319316\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01891462691128254\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003672167076729238\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.022624846547842026\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00038594770012423396\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04407791048288345\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006718676304444671\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013970603235065937\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005041766562499106\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.043040547519922256\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00047013096627779305\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.0435439869761467\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0004350889939814806\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04288237541913986\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006933814729563892\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.021552778780460358\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005030455067753792\n",
      "Are there any dead codes on this epoch?  158\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.0711, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.3429983854293823\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.023677270859479904\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.7656641006469727\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.024027466773986816\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.3308134078979492\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.021520253270864487\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.758640170097351\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.022414853796362877\n",
      "After scaling - encoder.proj.weight: grad norm 3.521231174468994\n",
      "After scaling - encoder.proj.bias: grad norm 0.03848034143447876\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.0206501483917236\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.027351858094334602\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.793889284133911\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.02491472288966179\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.8701975345611572\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.023713592439889908\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.722555160522461\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.040004272013902664\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.2309544086456299\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.026960404589772224\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.019343219697475433\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00034102395875379443\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02543087676167488\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003460678271949291\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019167719408869743\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00030995640554465353\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.0253297109156847\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003228413697797805\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.050716325640678406\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005542327417060733\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014700434170663357\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0003939490707125515\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.040240414440631866\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00035884699900634587\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.041339483112096786\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003415471001062542\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03921298682689667\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005761819193139672\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.017729448154568672\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0003883109602611512\n",
      "epoch: 123 ( 1 ) recon_loss: 0.3490599989891052  perplexity:  200.29196166992188  commit_loss:  0.17167861759662628 \n",
      "\t codebook loss:  0.6867144703865051  total_loss:  1.0710877180099487 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.0460, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 0.8549729585647583\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.01704498939216137\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.1023637056350708\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.0166438277810812\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 0.8392965197563171\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.014797106385231018\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 0.9993502497673035\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.01575002819299698\n",
      "After scaling - encoder.proj.weight: grad norm 1.9914437532424927\n",
      "After scaling - encoder.proj.bias: grad norm 0.02619973011314869\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.5897657871246338\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.018910570070147514\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 1.7220351696014404\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.01607467792928219\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 1.751213788986206\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.015844469889998436\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 1.7929127216339111\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.027385905385017395\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 0.8986062407493591\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.02022740989923477\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.020140208303928375\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004015210724901408\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.025967877358198166\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00039207105874083936\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019770924001932144\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003485686902422458\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02354123815894127\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00037101624184288085\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.046911533921957016\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.000617175071965903\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013892843388020992\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00044546768185682595\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04056519642472267\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003786638262681663\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04125254228711128\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00037324093864299357\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04223482683300972\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006451172521337867\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.0211680568754673\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004764878540299833\n",
      "Are there any dead codes on this epoch?  148\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.0793, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.0054166316986084\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.030461786314845085\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.5936832427978516\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.028357526287436485\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.9255852699279785\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02614382840692997\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.337608575820923\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.029455479234457016\n",
      "After scaling - encoder.proj.weight: grad norm 4.546419620513916\n",
      "After scaling - encoder.proj.bias: grad norm 0.0517103485763073\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.2152653932571411\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03200983256101608\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.384920597076416\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.02804660052061081\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.5991859436035156\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.02779589407145977\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.2089107036590576\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04415471851825714\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.6325643062591553\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.030511101707816124\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.022498197853565216\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003417421248741448\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.029097795486450195\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00031813501846045256\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.02160259336233139\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.000293300166958943\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02622496522963047\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00033045263262465596\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05100499093532562\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005801236839033663\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01363371778279543\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0003591091954149306\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.037974461913108826\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00031464683706872165\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04037824273109436\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00031183421378955245\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.035999856889247894\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0004953592433594167\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.0183152724057436\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0003422953304834664\n",
      "epoch: 124 ( 1 ) recon_loss: 0.3650640845298767  perplexity:  202.56704711914062  commit_loss:  0.16982530057430267 \n",
      "\t codebook loss:  0.6793012022972107  total_loss:  1.07926607131958 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.0603, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.3816877603530884\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.018736176192760468\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.840301752090454\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.01885806769132614\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.2957834005355835\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.01683902181684971\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.5648244619369507\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.018507802858948708\n",
      "After scaling - encoder.proj.weight: grad norm 3.1964855194091797\n",
      "After scaling - encoder.proj.bias: grad norm 0.0308737363666296\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.9149872064590454\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.02211037464439869\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.5925559997558594\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.01848451979458332\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.5660288333892822\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.018718622624874115\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.4478907585144043\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.031043948605656624\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.2676990032196045\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.022571180015802383\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02147688716650009\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0002912342024501413\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.028605561703443527\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00029312889091670513\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.020141594111919403\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0002617449499666691\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.024323556572198868\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0002876844082493335\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.049686018377542496\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.00047989984159357846\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014222517609596252\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00034368259366601706\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04029856622219086\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00028732247301377356\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.039886225014925\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00029096135403960943\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03804989531636238\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.00048254564171656966\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01970505341887474\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00035084530827589333\n",
      "Are there any dead codes on this epoch?  150\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.0992, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.210578203201294\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03146691247820854\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.906818389892578\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.030356185510754585\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.142347812652588\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.028373563662171364\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.542658805847168\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.031950078904628754\n",
      "After scaling - encoder.proj.weight: grad norm 4.868021011352539\n",
      "After scaling - encoder.proj.bias: grad norm 0.055981963872909546\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.2957056760787964\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.0349271297454834\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.684081554412842\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.030742904171347618\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.873685598373413\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.030952300876379013\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.46331524848938\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04997522383928299\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.742378830909729\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.034665267914533615\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.022874772548675537\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00032561548869125545\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.030079374089837074\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003141218039672822\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.022168733179569244\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.000293605902697891\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.026311099529266357\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003306152066215873\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05037363991141319\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005792939919047058\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013407791033387184\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00036142132012173533\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03812239319086075\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00031812352244742215\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.040084391832351685\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00032029033172875643\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03583792969584465\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005171370576135814\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01802990585565567\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0003587116370908916\n",
      "epoch: 125 ( 1 ) recon_loss: 0.38556259870529175  perplexity:  198.57357788085938  commit_loss:  0.16970275342464447 \n",
      "\t codebook loss:  0.6788110136985779  total_loss:  1.0992182493209839 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.0918, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.34232497215271\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.025832949206233025\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 3.0575931072235107\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.025993578135967255\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.0989229679107666\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.024119170382618904\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.323265790939331\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.027041908353567123\n",
      "After scaling - encoder.proj.weight: grad norm 4.703275680541992\n",
      "After scaling - encoder.proj.bias: grad norm 0.0469752661883831\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.3512654304504395\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.030021701008081436\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.9265637397766113\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.02550983428955078\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.929603338241577\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.026543496176600456\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.4739041328430176\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.041863519698381424\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.851379156112671\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.029232170432806015\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.024038052186369896\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.000265110022155568\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.03137847036123276\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0002667584631126374\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.021540144458413124\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00024752237368375063\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.023842457681894302\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0002775168977677822\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04826725274324417\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.00048208251246251166\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013867327943444252\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0003080969618167728\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04029626399278641\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0002617940481286496\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04032745957374573\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00027240198687650263\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03565085679292679\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.00042962332372553647\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01899973303079605\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0002999944263137877\n",
      "Are there any dead codes on this epoch?  156\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.0973, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.660180687904358\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.028062084689736366\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.165274143218994\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.027097048237919807\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.671360969543457\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02498571015894413\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.073686122894287\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.02692936174571514\n",
      "After scaling - encoder.proj.weight: grad norm 4.014256477355957\n",
      "After scaling - encoder.proj.bias: grad norm 0.04602469503879547\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.0763356685638428\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.029531516134738922\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.9606313705444336\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.025774087756872177\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.981395959854126\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.02445470727980137\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.8155455589294434\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.0413980670273304\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.401492714881897\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.028735028579831123\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.021546946838498116\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003642086812760681\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.028102388605475426\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00035168376052752137\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.021692050620913506\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00032428139820694923\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.026913698762655258\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00034950743429362774\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05209973081946373\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005973395309410989\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013969410210847855\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0003832799557130784\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03842507302761078\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.000334513490088284\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03869456797838211\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00031738969846628606\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.036542050540447235\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005372920422814786\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01818951964378357\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00037294256617315114\n",
      "epoch: 126 ( 1 ) recon_loss: 0.34764426946640015  perplexity:  201.0129852294922  commit_loss:  0.17827412486076355 \n",
      "\t codebook loss:  0.7130964994430542  total_loss:  1.0973131656646729 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.0636, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.5036849975585938\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.02290646731853485\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 3.2772531509399414\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.024156438186764717\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.2756779193878174\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.022365303710103035\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.4476699829101562\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.023723864927887917\n",
      "After scaling - encoder.proj.weight: grad norm 5.1490159034729\n",
      "After scaling - encoder.proj.bias: grad norm 0.04108083248138428\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.4427697658538818\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.027053257450461388\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 4.126041889190674\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.02270437777042389\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.894930124282837\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.022925948724150658\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.37384295463562\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.034949298948049545\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.9077385663986206\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.02422655187547207\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02455100603401661\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00022461963817477226\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.03213657811284065\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0002368767891312018\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.02231517992913723\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0002193130349041894\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.024001726880669594\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00023263499315362424\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05049098655581474\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0004028365365229547\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014147724956274033\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00026528286980465055\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04045975208282471\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00022263795835897326\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.0381934829056263\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00022481067571789026\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.033083729445934296\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0003427110204938799\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018707185983657837\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00023756432346999645\n",
      "Are there any dead codes on this epoch?  141\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.0250, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.0943564176559448\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.024302909150719643\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.3998044729232788\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.024203496053814888\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.0901025533676147\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.021090885624289513\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.3591742515563965\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.022086627781391144\n",
      "After scaling - encoder.proj.weight: grad norm 2.485147714614868\n",
      "After scaling - encoder.proj.bias: grad norm 0.036407470703125\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.70416259765625\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.025574367493391037\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 1.9874274730682373\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.02172212116420269\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.174177646636963\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.02096850983798504\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.355797529220581\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.03823002427816391\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.0656808614730835\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.025737199932336807\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.0205954909324646\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004573741462081671\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02634393982589245\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00045550326467491686\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.020515434443950653\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003969247336499393\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.025579290464520454\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00041566434083506465\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04676980525255203\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006851787329651415\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01325214933604002\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00048130261711776257\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03740284591913223\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00040880442247726023\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04091743752360344\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00039462168933823705\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04433547332882881\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007194786448962986\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.020055824890732765\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00048436710494570434\n",
      "epoch: 127 ( 1 ) recon_loss: 0.32610708475112915  perplexity:  198.80076599121094  commit_loss:  0.16619840264320374 \n",
      "\t codebook loss:  0.6647936105728149  total_loss:  1.0249660015106201 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.0467, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.808083415031433\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.021538151428103447\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.3855748176574707\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.024121252819895744\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.633569598197937\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.021194780245423317\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.775787591934204\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.021445946767926216\n",
      "After scaling - encoder.proj.weight: grad norm 3.784454584121704\n",
      "After scaling - encoder.proj.bias: grad norm 0.03433278203010559\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.1621174812316895\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.029616987332701683\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.4466724395751953\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.02562522515654564\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.2678568363189697\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.024464961141347885\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.8170526027679443\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.036056410521268845\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.5086174011230469\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.023691877722740173\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.022737877443432808\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0002708568936213851\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.0300002284348011\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003033411630894989\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.02054324932396412\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0002665387582965195\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.022331736981868744\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00026969736791215837\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04759208858013153\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0004317580605857074\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014614418148994446\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0003724537673406303\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.043344251811504364\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00032225463655777276\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04109552130103111\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00030766354757361114\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03542635217308998\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0004534338950179517\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018971892073750496\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0002979415003210306\n",
      "Are there any dead codes on this epoch?  146\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.0386, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.3814582824707031\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03158363699913025\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.7334871292114258\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.03202955797314644\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.3542888164520264\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.027202803641557693\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.7011128664016724\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.028423750773072243\n",
      "After scaling - encoder.proj.weight: grad norm 3.021929979324341\n",
      "After scaling - encoder.proj.bias: grad norm 0.045804351568222046\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.8150421380996704\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.030868664383888245\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.30008864402771\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.024912845343351364\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.5743141174316406\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.024679705500602722\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.879871129989624\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04791700094938278\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.3362658023834229\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03299567475914955\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.021458446979522705\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004905944224447012\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.026926573365926743\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.000497521017678082\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.021036416292190552\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.000422546174377203\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.026423698291182518\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004415113653521985\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.046940192580223083\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007114873733371496\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.012660198844969273\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00047948863357305527\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03572769835591316\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00038697582203894854\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03998729586601257\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003833544033113867\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04473356902599335\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007443035719916224\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.020756464451551437\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005125278839841485\n",
      "epoch: 128 ( 1 ) recon_loss: 0.3355863392353058  perplexity:  201.73143005371094  commit_loss:  0.16719232499599457 \n",
      "\t codebook loss:  0.6687692999839783  total_loss:  1.0385949611663818 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.0329, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.4467546939849854\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.026641570031642914\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.881955623626709\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.02793760597705841\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.3663835525512695\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.024174237623810768\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.589787244796753\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.024933895096182823\n",
      "After scaling - encoder.proj.weight: grad norm 3.1902291774749756\n",
      "After scaling - encoder.proj.bias: grad norm 0.03880830854177475\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.9940220713615417\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03132124990224838\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.006223201751709\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.027317797765135765\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.867905855178833\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.02498721145093441\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.444493532180786\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.037160541862249374\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.2358858585357666\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.024824175983667374\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.021338490769267082\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003929421363864094\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02775735966861248\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00041205761954188347\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.02015307918190956\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00035655093961395323\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02344810776412487\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003677553031593561\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.047053366899490356\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005723919603042305\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014661042019724846\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00046196376206353307\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04433942213654518\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00040291596087627113\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04229934886097908\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00036854163045063615\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.036054350435733795\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005480886320583522\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01822834089398384\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0003661370137706399\n",
      "Are there any dead codes on this epoch?  144\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.0304, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.5557811260223389\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03130289912223816\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.0352628231048584\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.032059185206890106\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.5822323560714722\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02803599275648594\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.0599076747894287\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.03157060965895653\n",
      "After scaling - encoder.proj.weight: grad norm 3.645838975906372\n",
      "After scaling - encoder.proj.bias: grad norm 0.05137636139988899\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.8731920719146729\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.030280200764536858\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.414044141769409\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.024909190833568573\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.7286112308502197\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.025366129353642464\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.0349042415618896\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.049056507647037506\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.402058720588684\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03333556652069092\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.021611090749502182\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00043482324690558016\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.028271488845348358\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00044532871106639504\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.02197851985692978\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00038944321568123996\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.028613828122615814\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00043854195973835886\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05064373090863228\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007136602653190494\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.012129361741244793\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00042061711428686976\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03353307396173477\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00034600935759954154\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.037902671843767166\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003523565537761897\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.042157337069511414\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006814355729147792\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01947575807571411\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004630586481653154\n",
      "epoch: 129 ( 1 ) recon_loss: 0.34786662459373474  perplexity:  201.02957153320312  commit_loss:  0.16233870387077332 \n",
      "\t codebook loss:  0.6493548154830933  total_loss:  1.0304383039474487 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.0115, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.0333083868026733\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.018446149304509163\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.347233533859253\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.019772838801145554\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 0.9419698715209961\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.0166837889701128\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.1230032444000244\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.01707330532371998\n",
      "After scaling - encoder.proj.weight: grad norm 2.2104403972625732\n",
      "After scaling - encoder.proj.bias: grad norm 0.026972724124789238\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.6856116056442261\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.021178239956498146\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 1.9719706773757935\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.017671000212430954\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 1.9278939962387085\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.017054622992873192\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 1.9613467454910278\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.029821602627635002\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.0043678283691406\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.020478229969739914\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.021615250036120415\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003858655982185155\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02818208932876587\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004136179341003299\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019704587757587433\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00034899968886747956\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02349153347313404\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00035714777186512947\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04623907431960106\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005642286851070821\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014341959729790688\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00044301681919023395\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.041250649839639664\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00036965063191019\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.040328629314899445\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00035675696562975645\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04102841392159462\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006238229107111692\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.021009858697652817\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00042837366345338523\n",
      "Are there any dead codes on this epoch?  143\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.0473, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.7404314279556274\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03638412430882454\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.2606546878814697\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.03706677258014679\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.7806658744812012\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.03263343870639801\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.376227855682373\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.037981998175382614\n",
      "After scaling - encoder.proj.weight: grad norm 4.208624839782715\n",
      "After scaling - encoder.proj.bias: grad norm 0.061022043228149414\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.0699126720428467\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.038739439100027084\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.1036863327026367\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.03391193598508835\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.501572608947754\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.03309363126754761\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.5630393028259277\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.05785607919096947\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.6015610694885254\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.038405291736125946\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.020372195169329643\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004258855478838086\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.026461543515324593\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004338761209510267\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.020843151956796646\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003819827688857913\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.027814358472824097\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00044458903721533716\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.049263034015893936\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.000714278663508594\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.012523602694272995\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00045345508260652423\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.036329448223114014\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003969479294028133\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.0409868061542511\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00038736945134587586\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04170629382133484\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006772202905267477\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01874668523669243\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00044954378972761333\n",
      "epoch: 130 ( 1 ) recon_loss: 0.3489967882633209  perplexity:  204.37554931640625  commit_loss:  0.16608388721942902 \n",
      "\t codebook loss:  0.6643355488777161  total_loss:  1.0472865104675293 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.0434, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.6010977029800415\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.019531164318323135\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.132523536682129\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.022325478494167328\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.4151393175125122\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.019095174968242645\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.6733742952346802\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.01910846307873726\n",
      "After scaling - encoder.proj.weight: grad norm 3.4013967514038086\n",
      "After scaling - encoder.proj.bias: grad norm 0.031237684190273285\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.0131901502609253\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.022554518654942513\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.76436185836792\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.018310947343707085\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.6535661220550537\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.01809801533818245\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.863945722579956\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.0329211950302124\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.5556472539901733\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.023597178980708122\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.022683629766106606\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00027670874260365963\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.030212635174393654\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003162973443977535\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.02004905790090561\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0002705318620428443\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.023707613348960876\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0002707201347220689\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04818945750594139\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0004425614606589079\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014354422688484192\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.000319542275974527\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.039164233952760696\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.000259421271039173\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03759452700614929\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00025640454259701073\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.040575090795755386\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.00046641266089864075\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.022039709612727165\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0003343141870573163\n",
      "Are there any dead codes on this epoch?  147\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.0211, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.516447901725769\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03472772613167763\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.9180299043655396\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.03484417870640755\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.5746359825134277\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.031184954568743706\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.1160733699798584\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.036153510212898254\n",
      "After scaling - encoder.proj.weight: grad norm 3.6671290397644043\n",
      "After scaling - encoder.proj.bias: grad norm 0.059303540736436844\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.982490062713623\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.039064276963472366\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.863917589187622\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.033738262951374054\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.3260154724121094\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.033904850482940674\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.398535966873169\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.059630461037158966\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.4240552186965942\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03926637023687363\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01954335719347\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004475566674955189\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02471878193318844\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00044905743561685085\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.020293261855840683\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00040189886931329966\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.027271082624793053\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004659315454773605\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.047260452061891556\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007642796263098717\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.012661927379667759\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005034443456679583\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.036908991634845734\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00043480482418090105\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04286431893706322\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0004369517264422029\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04379893094301224\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007684928714297712\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018352637067437172\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005060488474555314\n",
      "epoch: 131 ( 1 ) recon_loss: 0.3431883454322815  perplexity:  201.6585235595703  commit_loss:  0.16124878823757172 \n",
      "\t codebook loss:  0.6449951529502869  total_loss:  1.0211212635040283 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.0393, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.7039861679077148\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.01999550499022007\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.325575590133667\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.02224583923816681\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.5962809324264526\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.020767882466316223\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.903629183769226\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.021920518949627876\n",
      "After scaling - encoder.proj.weight: grad norm 3.7920784950256348\n",
      "After scaling - encoder.proj.bias: grad norm 0.03696585074067116\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.0648289918899536\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.02481013350188732\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.0164036750793457\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.02189752086997032\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.949430227279663\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.0205805916339159\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.9454801082611084\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.0336250439286232\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.590487003326416\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.023425819352269173\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.022182652726769447\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0002603033499326557\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.030274558812379837\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0002895984216593206\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.02078053541481495\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0002703582576941699\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.024781623855233192\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.000285363377770409\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.049365635961294174\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.00048122493899427354\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01386204268783331\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00032298066071234643\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.039267826825380325\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00028506398666650057\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.0383959598839283\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.000267920084297657\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03834453597664833\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.000437733979197219\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.02070510759949684\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.000304959510685876\n",
      "Are there any dead codes on this epoch?  143\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.0377, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.64242422580719\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.04010118171572685\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.0119168758392334\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.039678219705820084\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.6678320169448853\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.03497226908802986\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.192366361618042\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.03940491005778313\n",
      "After scaling - encoder.proj.weight: grad norm 4.043838977813721\n",
      "After scaling - encoder.proj.bias: grad norm 0.06621997803449631\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.0881375074386597\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.0438043549656868\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.1011476516723633\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.036229491233825684\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.5407378673553467\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.03664882853627205\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.595062732696533\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.06518587470054626\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.578100323677063\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.04371898993849754\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01970687136054039\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004811599792446941\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.024140281602740288\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00047608502791263163\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.02001173235476017\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0004196199879515916\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.026305433362722397\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00047280569560825825\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04852060601115227\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007945502293296158\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013056179508566856\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005255930591374636\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03720958158373833\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00043470493983477354\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04248407110571861\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0004397364391479641\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04313589632511139\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007821424514986575\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.0189350713044405\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005245687789283693\n",
      "epoch: 132 ( 1 ) recon_loss: 0.35488590598106384  perplexity:  205.7101287841797  commit_loss:  0.16241256892681122 \n",
      "\t codebook loss:  0.6496502757072449  total_loss:  1.0376838445663452 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.0138, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.8632440567016602\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.022910673171281815\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.505577564239502\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.02430613711476326\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.7471446990966797\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02278575301170349\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.0165836811065674\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.0241199042648077\n",
      "After scaling - encoder.proj.weight: grad norm 4.040919780731201\n",
      "After scaling - encoder.proj.bias: grad norm 0.041555166244506836\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.1296426057815552\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.028133608400821686\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.283132791519165\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.026565441861748695\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.217761993408203\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.02396490052342415\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.9333252906799316\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.03610875457525253\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.5391058921813965\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.024282371625304222\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02283346839249134\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0002807630808092654\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.030705060809850693\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0002978640259243548\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.021410705521702766\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0002792322193272412\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.024712596088647842\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00029558181995525956\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04952019453048706\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005092453793622553\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013843413442373276\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0003447685157880187\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.040233757346868515\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003255510819144547\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03943265601992607\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0002936823002528399\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03594697639346123\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.00044250141945667565\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018861256539821625\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00029757278389297426\n",
      "Are there any dead codes on this epoch?  141\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.0140, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.4728707075119019\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03506047651171684\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.774873971939087\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.0339302197098732\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.4502087831497192\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.03019888512790203\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.8692970275878906\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.03361338749527931\n",
      "After scaling - encoder.proj.weight: grad norm 3.5338332653045654\n",
      "After scaling - encoder.proj.bias: grad norm 0.05749927833676338\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.9553825259208679\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03818497061729431\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.707878589630127\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.03169018402695656\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.0755090713500977\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.03218551725149155\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.11215877532959\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.056691545993089676\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.3911778926849365\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03891659528017044\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02028011530637741\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004827514349017292\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02443842962384224\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004671888018492609\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019968081265687943\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0004158116935286671\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02573855035007\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00046282634139060974\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.048657726496458054\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007917137700133026\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013154763728380203\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005257729208096862\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03728506714105606\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004363455227576196\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.0423470102250576\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00044316580169834197\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04285164922475815\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007805919740349054\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01915527693927288\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005358468042686582\n",
      "epoch: 133 ( 1 ) recon_loss: 0.34142324328422546  perplexity:  204.16661071777344  commit_loss:  0.1599893420934677 \n",
      "\t codebook loss:  0.6399573683738708  total_loss:  1.014006495475769 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.0416, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.79306161403656\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.02913820557296276\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.346298933029175\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.030113400891423225\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.680173635482788\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02734968066215515\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.9623862504959106\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.02886432409286499\n",
      "After scaling - encoder.proj.weight: grad norm 3.947648525238037\n",
      "After scaling - encoder.proj.bias: grad norm 0.05033399537205696\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.1426818370819092\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.035182543098926544\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.467114210128784\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.03341630473732948\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.4101903438568115\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.02967923879623413\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.9163172245025635\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.043592847883701324\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.481146216392517\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.029702795669436455\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.021964052692055702\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00035692754318006337\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.028740914538502693\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00036887318128719926\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.020581234246492386\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00033501905272714794\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02403819002211094\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00035357262822799385\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04835659638047218\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006165647064335644\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013997245579957962\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004309674841351807\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.042470306158065796\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00040933198761194944\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04177301749587059\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003635549219325185\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03572333604097366\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005339891649782658\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01814325526356697\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00036384345730766654\n",
      "Are there any dead codes on this epoch?  138\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9915, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.1468229293823242\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.028541715815663338\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.3685219287872314\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.0273523461073637\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.1539579629898071\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.024207552894949913\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.5255138874053955\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.026681752875447273\n",
      "After scaling - encoder.proj.weight: grad norm 2.913517951965332\n",
      "After scaling - encoder.proj.bias: grad norm 0.04584645852446556\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.8071934580802917\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.030266953632235527\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.259974956512451\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.02558414451777935\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.516373634338379\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.025938907638192177\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.4742417335510254\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.043993838131427765\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.1230809688568115\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03091096319258213\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01943490281701088\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004836888983845711\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02319197915494442\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00046353298239409924\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019555820152163506\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00041023900848813355\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02585247904062271\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00045216857688501477\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04937461391091347\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007769478252157569\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013679292984306812\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005129260243847966\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03829919546842575\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004335676785558462\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04264431819319725\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.000439579802332446\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04193032160401344\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007455519516952336\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01903255470097065\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005238399025984108\n",
      "epoch: 134 ( 1 ) recon_loss: 0.3204018175601959  perplexity:  207.53054809570312  commit_loss:  0.15964379906654358 \n",
      "\t codebook loss:  0.6385751962661743  total_loss:  0.99150550365448 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.0027, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.5555247068405151\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.027147162705659866\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.0496206283569336\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.02956859953701496\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.4758877754211426\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02647712267935276\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.7059944868087769\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.027791662141680717\n",
      "After scaling - encoder.proj.weight: grad norm 3.4771058559417725\n",
      "After scaling - encoder.proj.bias: grad norm 0.04737816005945206\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.0577019453048706\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03611398860812187\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.2644917964935303\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.03480856865644455\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.2237327098846436\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.031041547656059265\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.6903879642486572\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04458849877119064\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.3267124891281128\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03012372925877571\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02097894437611103\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00036612653639167547\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.027642684057354927\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00039878382813185453\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019904904067516327\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00035708988434635103\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02300829254090786\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.000374818715499714\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04689479619264603\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006389765767380595\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014264943078160286\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004870596749242395\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04402732849121094\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00046945386566221714\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.0434776209294796\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00041864902595989406\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.036284543573856354\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006013531237840652\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01789301633834839\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00040627067210152745\n",
      "Are there any dead codes on this epoch?  145\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9925, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.2670212984085083\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.02664313092827797\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.5577000379562378\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.02510458417236805\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.2353427410125732\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.022122392430901527\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.6833992004394531\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.024338018149137497\n",
      "After scaling - encoder.proj.weight: grad norm 3.285752296447754\n",
      "After scaling - encoder.proj.bias: grad norm 0.042068250477313995\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.9137591123580933\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.02901436761021614\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.515369415283203\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.025115959346294403\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.6439168453216553\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.024767013266682625\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.4946017265319824\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.040669068694114685\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.1664860248565674\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.028152864426374435\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01980382390320301\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00041643803706392646\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.024347195401787758\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00039239018224179745\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019308680668473244\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00034577789483591914\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.026311902329325676\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00038040857180021703\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05135703459382057\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006575360312126577\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014282256364822388\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004535009793471545\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03931577876210213\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003925680066458881\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.041325002908706665\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003871138906106353\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03899117559194565\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006356665398925543\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01823243498802185\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00044003548100590706\n",
      "epoch: 135 ( 1 ) recon_loss: 0.3201557397842407  perplexity:  204.00953674316406  commit_loss:  0.1599499136209488 \n",
      "\t codebook loss:  0.6397996544837952  total_loss:  0.9925194978713989 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.0059, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.344912052154541\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.02676132321357727\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.7701741456985474\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.02896714396774769\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.3321791887283325\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.025788219645619392\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.5647140741348267\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.0272346381098032\n",
      "After scaling - encoder.proj.weight: grad norm 3.1296892166137695\n",
      "After scaling - encoder.proj.bias: grad norm 0.04583248868584633\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.935434103012085\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03383178263902664\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.9714815616607666\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.03261234611272812\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.9189724922180176\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.02886093035340309\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.4207634925842285\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.040628161281347275\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.1910099983215332\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.02728547900915146\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.020166853442788124\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004012840217910707\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.026543626561760902\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00043436011765152216\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019975924864411354\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00038669240893796086\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.023462766781449318\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004083813400939107\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04692945256829262\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006872546509839594\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014026762917637825\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.000507304968778044\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04455713927745819\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004890196141786873\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.043769773095846176\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0004327674105297774\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03629916533827782\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006092161638662219\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01785910315811634\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00040914371493272483\n",
      "Are there any dead codes on this epoch?  145\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.0018, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.524253249168396\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.034559041261672974\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.8434454202651978\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.032751403748989105\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.5308786630630493\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.029738815501332283\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.022549629211426\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.0324629545211792\n",
      "After scaling - encoder.proj.weight: grad norm 3.7861084938049316\n",
      "After scaling - encoder.proj.bias: grad norm 0.05580243095755577\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.082414984703064\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03931206464767456\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.0984766483306885\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.036074619740247726\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.3377344608306885\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.03361055999994278\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.0952413082122803\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.05430324375629425\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.3905091285705566\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.037506964057683945\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.019689036533236504\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00044640497071668506\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02381209470331669\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00042305540409870446\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01977461576461792\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003841412835754454\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02612561546266079\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00041932944441214204\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04890580102801323\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007208094466477633\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013981737196445465\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005078005488030612\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.040023546665906906\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00046598195331171155\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.0431140772998333\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0004341532476246357\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03998175263404846\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.000701444165315479\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.017961440607905388\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004844837822020054\n",
      "epoch: 136 ( 1 ) recon_loss: 0.33875784277915955  perplexity:  210.2209014892578  commit_loss:  0.15773960947990417 \n",
      "\t codebook loss:  0.6309584379196167  total_loss:  1.001804232597351 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9661, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 0.9863251447677612\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.0195265281945467\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.3208543062210083\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.021403586491942406\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.0160473585128784\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.019430020824074745\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.2080557346343994\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.0208669975399971\n",
      "After scaling - encoder.proj.weight: grad norm 2.3249874114990234\n",
      "After scaling - encoder.proj.bias: grad norm 0.0349443219602108\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.677786111831665\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.024851124733686447\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.1087565422058105\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.02402682788670063\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.1398866176605225\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.021464694291353226\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 1.8395448923110962\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.0312496330589056\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 0.9049342274665833\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.02159484103322029\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.020004913210868835\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003960423346143216\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02678992599248886\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004341133462730795\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.02060774900019169\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003940849273931235\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.024502113461494446\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004232300852891058\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04715602472424507\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007087501580826938\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013747040182352066\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005040372489020228\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.042770370841026306\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004873185825999826\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04340175911784172\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00043535270378924906\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.037310149520635605\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.000633813499007374\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01835412159562111\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00043799239210784435\n",
      "Are there any dead codes on this epoch?  136\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.0482, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.046693801879883\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.05167607218027115\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.512026309967041\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.050833553075790405\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.185944080352783\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.04668594151735306\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.732391595840454\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.0503191202878952\n",
      "After scaling - encoder.proj.weight: grad norm 4.9148101806640625\n",
      "After scaling - encoder.proj.bias: grad norm 0.08583180606365204\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.440310001373291\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.061744190752506256\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 4.536269187927246\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.05882342904806137\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 5.043299198150635\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.05362945422530174\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 4.409163951873779\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.0808153823018074\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.873022198677063\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.05380680784583092\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01884359121322632\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004757735878229141\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.023127835243940353\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00046801657299511135\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.020125646144151688\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0004298302228562534\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02515670470893383\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004632803029380739\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04524989053606987\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007902400684542954\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01326071098446846\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005684691714122891\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04176472872495651\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0005415781633928418\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.046432871371507645\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0004937580088153481\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04059448465704918\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007440546760335565\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.017244623973965645\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004953909083269536\n",
      "epoch: 137 ( 1 ) recon_loss: 0.3734501898288727  perplexity:  204.331787109375  commit_loss:  0.1605207771062851 \n",
      "\t codebook loss:  0.6420831084251404  total_loss:  1.0481605529785156 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.0489, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.1822879314422607\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.04143327474594116\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.818925619125366\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.041329871863126755\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.200608015060425\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.038198091089725494\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.62553334236145\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.042240772396326065\n",
      "After scaling - encoder.proj.weight: grad norm 4.780189514160156\n",
      "After scaling - encoder.proj.bias: grad norm 0.07155077159404755\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.3193883895874023\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.04642616584897041\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.9549481868743896\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.04319869354367256\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 4.23259162902832\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.04053356871008873\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.820892333984375\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.06463930755853653\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.798966884613037\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.04470560699701309\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.021773789077997208\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00041340067400597036\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.028125841170549393\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00041236900142394006\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.02195657603442669\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.000381121615646407\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.026196271181106567\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004214575164951384\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.047694362699985504\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007138981600292027\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013164203613996506\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004632172640413046\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03946051374077797\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00043101515620946884\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04223070666193962\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00040442385943606496\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03812297061085701\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006449390202760696\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.017949199303984642\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00044605039875023067\n",
      "Are there any dead codes on this epoch?  138\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9773, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.2190260887145996\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.02993575856089592\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.4942141771316528\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.0294132549315691\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.2473437786102295\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.026734547689557076\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.5842877626419067\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.028454281389713287\n",
      "After scaling - encoder.proj.weight: grad norm 3.046617031097412\n",
      "After scaling - encoder.proj.bias: grad norm 0.04892734810709953\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.9233470559120178\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03580789268016815\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.7627859115600586\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.034421008080244064\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.947402238845825\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.03194426745176315\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.6550557613372803\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.050134893506765366\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.1457926034927368\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03307875618338585\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.018673958256840706\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00045857843360863626\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02288949489593506\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004505743272602558\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019107749685645103\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00040953990537673235\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.024269308894872665\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004358840815257281\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04667036607861519\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007495058816857636\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01414452400058508\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005485322326421738\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04232243075966835\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0005272869020700455\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.0451505184173584\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0004893462755717337\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04067213833332062\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007680039270780981\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01755211129784584\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005067252204753458\n",
      "epoch: 138 ( 1 ) recon_loss: 0.3159462511539459  perplexity:  209.0503387451172  commit_loss:  0.1573590487241745 \n",
      "\t codebook loss:  0.629436194896698  total_loss:  0.9773414134979248 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9968, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.1269655227661133\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03282928466796875\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.763521432876587\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.031611498445272446\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.033623456954956\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.028915835544466972\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.351067304611206\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.032526761293411255\n",
      "After scaling - encoder.proj.weight: grad norm 4.460056304931641\n",
      "After scaling - encoder.proj.bias: grad norm 0.05587153509259224\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.199505090713501\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.0346994511783123\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.458965301513672\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.030076006427407265\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.6666533946990967\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.03044053167104721\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.3973329067230225\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.05139327049255371\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.7256075143814087\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.035721555352211\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.023332776501774788\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.000360136735253036\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.030315784737467766\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00034677761141210794\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.02230881340801716\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00031720625702291727\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.025791170075535774\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003568180254660547\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.048926740884780884\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006129097891971469\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013158551417291164\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00038065240369178355\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.037944793701171875\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00032993327477015555\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.040223125368356705\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00033393214107491076\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.037268687039613724\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005637832800857723\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01892988570034504\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00039186488720588386\n",
      "Are there any dead codes on this epoch?  140\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9722, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.2523285150527954\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.02729065902531147\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.5807567834854126\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.027322132140398026\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.2370543479919434\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.024298686534166336\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.6017531156539917\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.025107424706220627\n",
      "After scaling - encoder.proj.weight: grad norm 3.3355510234832764\n",
      "After scaling - encoder.proj.bias: grad norm 0.04337037727236748\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.0241708755493164\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03297116979956627\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.855952262878418\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.030186090618371964\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.9045157432556152\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.02835829183459282\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.6911988258361816\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.046984970569610596\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.197502851486206\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03088163211941719\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.018541792407631874\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00040406148764304817\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.023404452949762344\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00040452746907249093\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.018315646797418594\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00035976278013549745\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.023715322837233543\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00037173685268498957\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.0493856780230999\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006421354482881725\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.015163722448050976\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004881663480773568\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04228481277823448\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004469308187253773\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.043003834784030914\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00041986870928667486\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03984549641609192\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.00069565256126225\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.017730051651597023\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.000457228918094188\n",
      "epoch: 139 ( 1 ) recon_loss: 0.3209719955921173  perplexity:  208.64642333984375  commit_loss:  0.15494178235530853 \n",
      "\t codebook loss:  0.6197671294212341  total_loss:  0.972182035446167 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9682, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.5501253604888916\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.024902820587158203\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.982480764389038\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.02487504668533802\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.4491695165634155\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.021888336166739464\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.6643375158309937\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.02377309836447239\n",
      "After scaling - encoder.proj.weight: grad norm 3.2063350677490234\n",
      "After scaling - encoder.proj.bias: grad norm 0.041327208280563354\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.8736085295677185\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.025690780952572823\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.489952802658081\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.020636674016714096\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.6323390007019043\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.021610582247376442\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.4756860733032227\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.03835918754339218\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.2511671781539917\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.026995686814188957\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.023617705330252647\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003794193034991622\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.030205070972442627\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003789961338043213\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.02207954041659832\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003334906359668821\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.025357840582728386\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00036220686160959303\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04885170981287956\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006296612555161119\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013310296460986137\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00039142463356256485\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03793691098690033\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00031442029285244644\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.040106311440467834\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003292587643954903\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.037719544023275375\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005844404804520309\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01906277984380722\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004113062168471515\n",
      "Are there any dead codes on this epoch?  138\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9545, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 0.9967370629310608\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.0213199183344841\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.2103981971740723\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.021146167069673538\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 0.9380316138267517\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.01894202269613743\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.1616991758346558\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.019631696864962578\n",
      "After scaling - encoder.proj.weight: grad norm 2.400132894515991\n",
      "After scaling - encoder.proj.bias: grad norm 0.03415946662425995\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.73660808801651\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.02657913602888584\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.132758617401123\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.02429608441889286\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.2101666927337646\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.02276299148797989\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.1350667476654053\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.03803268074989319\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 0.9683118462562561\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.02525821328163147\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.019574901089072227\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.000418701529270038\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02377099171280861\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00041528925066813827\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01842198707163334\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00037200210499577224\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.022814592346549034\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00038554653292521834\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04713616892695427\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006708572036586702\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014466234482824802\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005219872109591961\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04188520833849907\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004771504027303308\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.043405428528785706\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0004470420244615525\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.041930537670850754\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007469231495633721\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.019016660749912262\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004960456280969083\n",
      "epoch: 140 ( 1 ) recon_loss: 0.29616016149520874  perplexity:  209.38534545898438  commit_loss:  0.15665142238140106 \n",
      "\t codebook loss:  0.6266056895256042  total_loss:  0.9545305967330933 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9873, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.7218716144561768\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.031248018145561218\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.2404308319091797\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.03292841464281082\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.6945303678512573\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02869049273431301\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.996236801147461\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.030668912455439568\n",
      "After scaling - encoder.proj.weight: grad norm 3.870119571685791\n",
      "After scaling - encoder.proj.bias: grad norm 0.0517343208193779\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.0255115032196045\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03357953950762749\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.001948118209839\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.02810625731945038\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.196277618408203\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.028523171320557594\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.880579710006714\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04697537422180176\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.4291479587554932\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.032364603132009506\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02214621752500534\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00040190303116105497\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.028815776109695435\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00042351582669653\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.02179456315934658\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.000369008892448619\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.025675024837255478\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00039445472066290677\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04977636784315109\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006653919699601829\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013189834542572498\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004318903957027942\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03861019387841225\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003614946035668254\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.0411096028983593\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003668568388093263\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03704918548464775\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006041838205419481\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018381288275122643\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004162642580922693\n",
      "Are there any dead codes on this epoch?  134\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9355, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 0.8595992922782898\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.018365884199738503\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.0849963426589966\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.018627852201461792\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 0.8332080245018005\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.01685251109302044\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.0197625160217285\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.017548974603414536\n",
      "After scaling - encoder.proj.weight: grad norm 2.1054818630218506\n",
      "After scaling - encoder.proj.bias: grad norm 0.03027091734111309\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.6672966480255127\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.024549102410674095\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 1.9540146589279175\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.022242093458771706\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.036074638366699\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.020680880174040794\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 1.9360737800598145\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.03358764201402664\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 0.8796523809432983\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.022508952766656876\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01874910481274128\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00040058649028651416\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02366533875465393\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004063003871124238\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.018173472955822945\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003675776533782482\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02224249392747879\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00038276848499663174\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04592360556125641\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006602524663321674\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014554704539477825\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.000535451399628073\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04261988773941994\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00048513224464841187\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04440973326563835\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00045107988989911973\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.042228568345308304\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007325949845835567\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.019186489284038544\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004909528070129454\n",
      "epoch: 141 ( 1 ) recon_loss: 0.2873186767101288  perplexity:  209.2462158203125  commit_loss:  0.15423405170440674 \n",
      "\t codebook loss:  0.616936206817627  total_loss:  0.9355048537254333 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9944, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.6214886903762817\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.04059569537639618\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.0773937702178955\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.04320617765188217\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.7473340034484863\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.038477204740047455\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.204089641571045\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.0420132651925087\n",
      "After scaling - encoder.proj.weight: grad norm 4.055826663970947\n",
      "After scaling - encoder.proj.bias: grad norm 0.06976157426834106\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.153515338897705\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.05055537819862366\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.6517677307128906\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.04611584171652794\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.969703435897827\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.04419202730059624\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.530149221420288\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.07077942043542862\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.4584904909133911\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.046168409287929535\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.018557699397206306\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.000464611774077639\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.023775465786457062\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004944883403368294\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019997980445623398\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0004403659258969128\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.025225481018424034\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004808356170542538\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.046418339014053345\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007984109688550234\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013201813213527203\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005785988760180771\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04179394245147705\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.000527789001353085\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.045432671904563904\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.000505771255120635\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04040203616023064\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0008100600098259747\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.016692208126187325\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005283906357362866\n",
      "Are there any dead codes on this epoch?  141\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9563, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.0379806756973267\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.02706189826130867\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.2873866558074951\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.0281559769064188\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.0675040483474731\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.025155682116746902\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.3414498567581177\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.026118528097867966\n",
      "After scaling - encoder.proj.weight: grad norm 2.7556657791137695\n",
      "After scaling - encoder.proj.bias: grad norm 0.04458802938461304\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.8856051564216614\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.036704715341329575\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.65757155418396\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.03270149976015091\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.7829740047454834\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.030843887478113174\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.553436279296875\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.049029137939214706\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.1054048538208008\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.032724786549806595\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.017154226079583168\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004472395230550319\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.021276045590639114\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004653208306990564\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.017642147839069366\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00041573637281544507\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02216952294111252\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00043164886301383376\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04554161801934242\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007368858205154538\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014635988511145115\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0006066019413992763\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04392046108841896\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0005404426483437419\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04599292576313019\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0005097427056171\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04219946265220642\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0008102819556370378\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018268516287207603\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005408274591900408\n",
      "epoch: 142 ( 1 ) recon_loss: 0.3076270520687103  perplexity:  209.6892852783203  commit_loss:  0.15435351431369781 \n",
      "\t codebook loss:  0.6174140572547913  total_loss:  0.9562908411026001 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.0524, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 2.1969449520111084\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.055838026106357574\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.806696653366089\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.058583859354257584\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 2.385246753692627\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.051770731806755066\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.9851508140563965\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.05552893131971359\n",
      "After scaling - encoder.proj.weight: grad norm 5.442195892333984\n",
      "After scaling - encoder.proj.bias: grad norm 0.09220676124095917\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.4869056940078735\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.06242939829826355\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 4.728722095489502\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.05747595056891441\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 5.214637279510498\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.05475113168358803\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 4.611317157745361\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.08576132357120514\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.9811487197875977\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.055414799600839615\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01898459903895855\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004825166834052652\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.024253688752651215\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0005062444251962006\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.02061178721487522\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0004473697335924953\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.025795774534344673\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004798457375727594\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04702799394726753\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007967922720126808\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.012848892249166965\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005394751788116992\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04086260870099068\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004966706037521362\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04506157711148262\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00047312446986325085\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03984806686639786\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007410948164761066\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01711982674896717\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004788594669662416\n",
      "Are there any dead codes on this epoch?  132\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.0204, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.7697194814682007\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.05059610679745674\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.1426708698272705\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.052256081253290176\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.9014813899993896\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.04643753916025162\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.3986618518829346\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.04880282282829285\n",
      "After scaling - encoder.proj.weight: grad norm 4.6756134033203125\n",
      "After scaling - encoder.proj.bias: grad norm 0.08244499564170837\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.3814128637313843\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.0607437938451767\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 4.305228233337402\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.054875440895557404\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 4.65611457824707\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.05196039006114006\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 4.0608134269714355\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.08063375949859619\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.6841838359832764\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.05253523588180542\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.017658548429608345\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0005048561142757535\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02137991599738598\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0005214196862652898\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01897329092025757\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0004633612697944045\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02393423579633236\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004869624972343445\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.046654026955366135\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.000822649453766644\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013783961534500122\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0006061114254407585\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04295826703310013\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0005475560901686549\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.046459466218948364\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.000518469198141247\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04051945358514786\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0008045767317526042\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01680505834519863\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005242050974629819\n",
      "epoch: 143 ( 1 ) recon_loss: 0.35526934266090393  perplexity:  208.6827392578125  commit_loss:  0.15828827023506165 \n",
      "\t codebook loss:  0.6331530809402466  total_loss:  1.0204439163208008 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9645, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.334828495979309\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.028007792308926582\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.7477633953094482\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.03032934106886387\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.3943380117416382\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.026862667873501778\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.7838486433029175\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.0292904544621706\n",
      "After scaling - encoder.proj.weight: grad norm 3.1445443630218506\n",
      "After scaling - encoder.proj.bias: grad norm 0.04848456010222435\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.8665424585342407\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03316785395145416\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.681643486022949\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.03123977780342102\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.003221035003662\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.030594835057854652\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.968951463699341\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.05175882577896118\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.3226970434188843\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03411371633410454\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01939038746058941\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00040685522253625095\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.025388887152075768\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004405792278703302\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.02025485225021839\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003902206080965698\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02591307833790779\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00042548784404061735\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.045679230242967606\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.00070431106723845\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.012587829492986202\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00048181292368099093\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.038954898715019226\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004538046778179705\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.043626293540000916\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0004444359219633043\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04312847554683685\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007518747006542981\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01921416074037552\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004955529002472758\n",
      "Are there any dead codes on this epoch?  136\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9473, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.1916311979293823\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.0321497768163681\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.432106375694275\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.033293236047029495\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.245768427848816\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.029768524691462517\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.600577712059021\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.03194652125239372\n",
      "After scaling - encoder.proj.weight: grad norm 3.0275464057922363\n",
      "After scaling - encoder.proj.bias: grad norm 0.05394529923796654\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.8946052193641663\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.04080481082201004\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.759014368057251\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.037125442177057266\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.9896700382232666\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.035401154309511185\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.7390518188476562\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.05736740306019783\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.1330090761184692\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.037605829536914825\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01819264516234398\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004908309783786535\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02186398394405842\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0005082881543785334\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01901915855705738\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00045447636512108147\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.024436036124825478\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00048772786976769567\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04622158035635948\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0008235834538936615\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013657947070896626\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0006229674909263849\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.042121898382902145\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0005667944788001478\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04564332216978073\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0005404697731137276\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.041817132383584976\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0008758286712691188\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.017297660931944847\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005741285276599228\n",
      "epoch: 144 ( 1 ) recon_loss: 0.31077805161476135  perplexity:  212.014404296875  commit_loss:  0.1514783650636673 \n",
      "\t codebook loss:  0.6059134602546692  total_loss:  0.9473118782043457 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9912, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.658599615097046\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.030483843758702278\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.1685404777526855\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.0326835922896862\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.6558809280395508\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02853110060095787\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.140491008758545\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.030879180878400803\n",
      "After scaling - encoder.proj.weight: grad norm 3.8795361518859863\n",
      "After scaling - encoder.proj.bias: grad norm 0.05154087021946907\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.0698494911193848\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.035122111439704895\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.153585433959961\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.03274652734398842\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.476818561553955\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.03161640465259552\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.458585023880005\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.05318531394004822\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.58207368850708\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.0341474749147892\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.0201901625841856\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003710803866852075\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02639768272638321\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003978579770773649\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.020157068967819214\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003473096003290266\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.026056235656142235\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00037589282146655023\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04722566530108452\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006274079787544906\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013023297302424908\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00042754210880957544\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03838865086436272\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003986240772064775\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04232337698340416\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003848671040032059\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.0421014204621315\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006474258261732757\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.019258610904216766\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004156778450123966\n",
      "Are there any dead codes on this epoch?  134\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9516, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.1663280725479126\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.0290218535810709\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.4178452491760254\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.029360366985201836\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.2116706371307373\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.026100492104887962\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.609036922454834\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.027998819947242737\n",
      "After scaling - encoder.proj.weight: grad norm 3.054666519165039\n",
      "After scaling - encoder.proj.bias: grad norm 0.04735417664051056\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.8852174282073975\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.033989306539297104\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.6479620933532715\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.03134975582361221\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.867225408554077\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.0302608460187912\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.7278313636779785\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.051077425479888916\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.1475708484649658\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03247359022498131\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01809507980942726\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00045026163570582867\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02199726179242134\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004555135383270681\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.018798550590872765\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0004049379494972527\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.024963518604636192\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00043438971624709666\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.047391846776008606\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007346798083744943\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01373376976698637\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005273295100778341\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.041081998497247696\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004863780632149428\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04448377341032028\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00046948407543823123\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.042321134358644485\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007924443343654275\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.017804069444537163\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005038139061070979\n",
      "epoch: 145 ( 1 ) recon_loss: 0.3008078932762146  perplexity:  211.1610565185547  commit_loss:  0.1548725664615631 \n",
      "\t codebook loss:  0.6194902658462524  total_loss:  0.9515806436538696 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9640, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.5080602169036865\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.029861224815249443\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.9029455184936523\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.03214200213551521\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.4658094644546509\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.028362786397337914\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.8237099647521973\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.030085518956184387\n",
      "After scaling - encoder.proj.weight: grad norm 3.2514772415161133\n",
      "After scaling - encoder.proj.bias: grad norm 0.05137660726904869\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.9229162931442261\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.0348140224814415\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.8030500411987305\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.032882481813430786\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.146052360534668\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.03195447847247124\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.1246368885040283\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.052981551736593246\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.4299372434616089\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03506070375442505\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.020838966593146324\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00041263410821557045\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.026295647025108337\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004441507626324892\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.020255129784345627\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003919281007256359\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02520074136555195\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00041573349153622985\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04493018984794617\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007099421345628798\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.012753218412399292\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004810738319065422\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.038733646273612976\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004543830582406372\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04347338527441025\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0004415595321916044\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04317745938897133\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007321198936551809\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.019759435206651688\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004844825598411262\n",
      "Are there any dead codes on this epoch?  134\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9267, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.0032271146774292\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.024472998455166817\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.2290610074996948\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.025700757279992104\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.0339235067367554\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.022621087729930878\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.322375774383545\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.023377032950520515\n",
      "After scaling - encoder.proj.weight: grad norm 2.5246593952178955\n",
      "After scaling - encoder.proj.bias: grad norm 0.04037455841898918\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.7389584183692932\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.029683072119951248\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.1872377395629883\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.027580389752984047\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.401336431503296\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.02675296552479267\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.333176612854004\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.045039158314466476\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.0027105808258057\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.029845451936125755\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.018545398488640785\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004524015530478209\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.022720105946063995\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00047509759315289557\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01911284402012825\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00041816761950030923\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.024445099756121635\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004321418527979404\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.046670202165842056\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007463537622243166\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013660196214914322\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.000548713665921241\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04043271392583847\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0005098440451547503\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.0443904884159565\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.000494548468850553\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.043130505830049515\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0008325823000632226\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018535850569605827\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005517153767868876\n",
      "epoch: 146 ( 1 ) recon_loss: 0.29314297437667847  perplexity:  213.5446319580078  commit_loss:  0.150783509016037 \n",
      "\t codebook loss:  0.603134036064148  total_loss:  0.9267104864120483 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(1.0069, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.84114670753479\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03982771188020706\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.26419997215271\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.04246606305241585\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.7945884466171265\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.037248704582452774\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.159496545791626\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.038114700466394424\n",
      "After scaling - encoder.proj.weight: grad norm 3.954183578491211\n",
      "After scaling - encoder.proj.bias: grad norm 0.06566604226827621\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.1636605262756348\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.0458512119948864\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.6220624446868896\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.04295004531741142\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 4.078127861022949\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.041281986981630325\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.870422840118408\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.06566488742828369\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.7786320447921753\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.04276212304830551\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.020418580621480942\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00044169503962621093\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02511030249297619\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00047095472109504044\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019902244210243225\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00041309348307549953\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02394912764430046\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00042269748519174755\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.043852467089891434\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007282458827830851\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.012905161827802658\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005084964795969427\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04016919434070587\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004763221659231931\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04522702097892761\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00045782316010445356\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.042923543602228165\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007282330188900232\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.019725283607840538\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004742380988318473\n",
      "Are there any dead codes on this epoch?  134\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9591, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.2706372737884521\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03122042864561081\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.5525200366973877\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.03268428519368172\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.3116205930709839\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.028612812981009483\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.6708450317382812\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.029637716710567474\n",
      "After scaling - encoder.proj.weight: grad norm 3.1213464736938477\n",
      "After scaling - encoder.proj.bias: grad norm 0.05158914625644684\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.9868494272232056\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.04026499390602112\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.0427119731903076\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.037320513278245926\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.406961679458618\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.03606173023581505\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.156301736831665\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.05636894330382347\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.3267894983291626\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.035707030445337296\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.017644869163632393\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004335465782787651\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.021559271961450577\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004538745852187276\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.018213991075754166\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.000397335592424497\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.023202408105134964\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00041156800580210984\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04334498569369316\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007163994014263153\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013704014010727406\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005591451190412045\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04225301742553711\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0005182561581023037\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.047311216592788696\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0005007759318687022\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04383039474487305\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007827747031114995\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018424635753035545\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004958503413945436\n",
      "epoch: 147 ( 1 ) recon_loss: 0.30637338757514954  perplexity:  211.66387939453125  commit_loss:  0.15535633265972137 \n",
      "\t codebook loss:  0.6214253306388855  total_loss:  0.9591314792633057 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9574, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.3848986625671387\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.032221436500549316\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.69597589969635\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.03475324064493179\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.3674876689910889\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.030580833554267883\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.6287131309509277\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.031313955783843994\n",
      "After scaling - encoder.proj.weight: grad norm 3.0024356842041016\n",
      "After scaling - encoder.proj.bias: grad norm 0.05435182899236679\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.9018993973731995\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03931460157036781\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.807061195373535\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.03658417612314224\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.1738295555114746\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.035243645310401917\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.023890972137451\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.0582306869328022\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.3476799726486206\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03859308362007141\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01997387409210205\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004647176538128406\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02446042187511921\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0005012329202145338\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01972276158630848\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0004410558904055506\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.023490317165851593\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004516294284258038\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.043303001672029495\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007838960154913366\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.0130077563226223\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005670197424478829\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.0404851920902729\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0005276398151181638\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.045774951577186584\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0005083058495074511\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04361244663596153\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0008398393401876092\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01943708211183548\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005566136096604168\n",
      "Are there any dead codes on this epoch?  133\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9321, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.18537437915802\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.029271185398101807\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.4630646705627441\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.03128951042890549\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.1879478693008423\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.027266593649983406\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.4231929779052734\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.02734505757689476\n",
      "After scaling - encoder.proj.weight: grad norm 2.8098976612091064\n",
      "After scaling - encoder.proj.bias: grad norm 0.04793569818139076\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.8881622552871704\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03964954987168312\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.7365965843200684\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.036354947835206985\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.0029022693634033\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.035053279250860214\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.650622844696045\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.0540362186729908\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.1480591297149658\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03566953167319298\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01861925795674324\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00045977687113918364\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.022981075569987297\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.000491479760967195\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.0186596792191267\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00042828978621400893\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.022354792803525925\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00042952224612236023\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.044136445969343185\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007529495633207262\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013950800523161888\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0006227949634194374\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04298506677150726\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0005710450350306928\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.0471680648624897\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0005505990702658892\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.041634637862443924\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0008487734012305737\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01803312823176384\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005602788296528161\n",
      "epoch: 148 ( 1 ) recon_loss: 0.30272847414016724  perplexity:  214.96121215820312  commit_loss:  0.14979484677314758 \n",
      "\t codebook loss:  0.5991793870925903  total_loss:  0.9320963621139526 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9753, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.3978588581085205\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03485169634222984\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.7452788352966309\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.03799234330654144\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.441500186920166\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.033140722662210464\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.6697295904159546\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.03244191035628319\n",
      "After scaling - encoder.proj.weight: grad norm 3.1984007358551025\n",
      "After scaling - encoder.proj.bias: grad norm 0.05580361559987068\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.9401577711105347\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03921869769692421\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.931048631668091\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.03610045835375786\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.275214433670044\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.03470175713300705\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.0151913166046143\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.05582214891910553\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.3509254455566406\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03686433658003807\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.019529854878783226\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004869222466368228\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.024383751675486565\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0005308010731823742\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.020139580592513084\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00046301784459501505\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.023328233510255814\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004532545281108469\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04468570277094841\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007796470890752971\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013135192915797234\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005479347892105579\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04095045477151871\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0005043690325692296\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04575888812541962\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0004848273820243776\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04212603345513344\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007799059967510402\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018874136731028557\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005150414071977139\n",
      "Are there any dead codes on this epoch?  137\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9344, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.0486782789230347\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.027575615793466568\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.280558466911316\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.028458403423428535\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.0643091201782227\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.024577682837843895\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.3095033168792725\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.02498547174036503\n",
      "After scaling - encoder.proj.weight: grad norm 2.6024463176727295\n",
      "After scaling - encoder.proj.bias: grad norm 0.044176697731018066\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.8370366096496582\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.034699391573667526\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.5399398803710938\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.03138982877135277\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.8285481929779053\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.03059483878314495\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.5499491691589355\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04828081652522087\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.1109058856964111\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03179055452346802\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.017660414800047874\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00046439110883511603\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.021565426141023636\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004792578110937029\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.017923647537827492\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00041390396654605865\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02205287665128708\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004207713936921209\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.0438268706202507\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007439639302901924\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014096233993768692\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005843600956723094\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04277421906590462\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0005286248633638024\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04763456806540489\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0005152368103154004\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04294278472661972\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0008130799396894872\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01870836690068245\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.000535373343154788\n",
      "epoch: 149 ( 1 ) recon_loss: 0.28661805391311646  perplexity:  211.7969512939453  commit_loss:  0.1541757881641388 \n",
      "\t codebook loss:  0.6167031526565552  total_loss:  0.9343695640563965 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9470, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.2312055826187134\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.033727116882801056\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.5898118019104004\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.037276528775691986\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.3556936979293823\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.032454147934913635\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.603271722793579\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.032184116542339325\n",
      "After scaling - encoder.proj.weight: grad norm 2.98514723777771\n",
      "After scaling - encoder.proj.bias: grad norm 0.055183880031108856\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.8941461443901062\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.041755612939596176\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.7778871059417725\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.03783823549747467\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.171879529953003\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.03714599460363388\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.939459800720215\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.0610787495970726\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.183919072151184\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.039673641324043274\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.018153244629502296\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.000497282191645354\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.023440636694431305\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.000549615710042417\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019988732412457466\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0004785131604876369\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.023639094084501266\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00047453175648115575\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.044013861566782\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0008136467076838017\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013183544389903545\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0006156566087156534\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04095795378088951\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.000557897612452507\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04676709324121475\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0005476910737343132\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.043340228497982025\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0009005624451674521\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.017456037923693657\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005849594599567354\n",
      "Are there any dead codes on this epoch?  134\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9116, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.0352561473846436\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.026771709322929382\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.2179327011108398\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.02774648740887642\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.018874168395996\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02424480952322483\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.224947452545166\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.02444770187139511\n",
      "After scaling - encoder.proj.weight: grad norm 2.4352364540100098\n",
      "After scaling - encoder.proj.bias: grad norm 0.04344462230801582\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.7957502007484436\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03616523742675781\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.4580297470092773\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.03327462822198868\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.7660462856292725\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.03228260576725006\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.505152702331543\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.05130172520875931\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.050755262374878\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03380517289042473\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.018092380836606026\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00046786878374405205\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.021284881979227066\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00048490421613678336\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01780608855187893\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00042370808660052717\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02140747383236885\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00042725386447273195\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04255877435207367\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007592486217617989\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013906720094382763\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0006320323445834219\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.0429571159183979\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0005815153126604855\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.0483400858938694\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0005641785683110356\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.043780647218227386\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0008965611341409385\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01836324855685234\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005907872109673917\n",
      "epoch: 150 ( 1 ) recon_loss: 0.28816354274749756  perplexity:  215.52540588378906  commit_loss:  0.14840184152126312 \n",
      "\t codebook loss:  0.5936073660850525  total_loss:  0.9116343855857849 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9830, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.4977155923843384\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.04266737028956413\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.8995895385742188\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.046464432030916214\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.6650164127349854\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.04026065021753311\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.9350075721740723\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.039091505110263824\n",
      "After scaling - encoder.proj.weight: grad norm 3.6191728115081787\n",
      "After scaling - encoder.proj.bias: grad norm 0.06646721810102463\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.0677335262298584\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.046997811645269394\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.3357553482055664\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.04269380867481232\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.7461979389190674\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.04107854142785072\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.3637290000915527\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.06578483432531357\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.4135242700576782\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.04248708114027977\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.018562031909823418\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0005288007669150829\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.023542681708931923\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0005758599145337939\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.020635485649108887\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0004989729495719075\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.023981638252735138\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00048448305460624397\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04485444724559784\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0008237656438723207\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013233022764325142\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005824703257530928\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04134189337491989\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0005291284178383648\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04642874002456665\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0005091094644740224\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04168859124183655\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0008153085364028811\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.017518602311611176\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005265662912279367\n",
      "Are there any dead codes on this epoch?  138\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9333, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.1829828023910522\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.0333077535033226\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.423472285270691\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.03442101180553436\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.2438708543777466\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02993924170732498\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.5179675817489624\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.03014397621154785\n",
      "After scaling - encoder.proj.weight: grad norm 2.920441150665283\n",
      "After scaling - encoder.proj.bias: grad norm 0.05294664949178696\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.9202316403388977\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.040891133248806\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.8456363677978516\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.037223849445581436\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.272596597671509\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.0362548753619194\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.9840967655181885\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.057769786566495895\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.2209265232086182\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03697717562317848\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.017466451972723007\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004917808691971004\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.021017219871282578\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0005082178395241499\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.018365448340773582\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00044204556616023183\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.022412419319152832\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004450684355106205\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04311959818005562\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007817443110980093\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01358699519187212\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0006037476123310626\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04201512411236763\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0005496009835042059\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04831908643245697\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0005352943553589284\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04405945539474487\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0008529567858204246\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01802668161690235\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005459589301608503\n",
      "epoch: 151 ( 1 ) recon_loss: 0.29159191250801086  perplexity:  211.49681091308594  commit_loss:  0.15274769067764282 \n",
      "\t codebook loss:  0.6109907627105713  total_loss:  0.9332981109619141 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9261, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.2234894037246704\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03290322795510292\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.5788382291793823\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.03669044002890587\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.3536535501480103\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.031889185309410095\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.602220892906189\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.031778935343027115\n",
      "After scaling - encoder.proj.weight: grad norm 2.8712785243988037\n",
      "After scaling - encoder.proj.bias: grad norm 0.05386358126997948\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.8435190916061401\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03840428963303566\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.5726330280303955\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.03466808795928955\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.895036220550537\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.03380738943815231\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.679434061050415\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.05651479586958885\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.102927803993225\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03702210262417793\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.019164767116308212\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0005153969977982342\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02473096176981926\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0005747199757024646\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.02120366133749485\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0004995130002498627\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.025097226724028587\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.000497786037158221\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.044975776225328445\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0008437203941866755\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013212904334068298\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0006015657563693821\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04029778763651848\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.000543041795026511\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.0453479178249836\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0005295597948133945\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04197072237730026\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0008852491155266762\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01727628894150257\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005799151258543134\n",
      "Are there any dead codes on this epoch?  133\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9212, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.2548874616622925\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.028814682736992836\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.5858125686645508\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.031108519062399864\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.2327337265014648\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02721685729920864\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.4520360231399536\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.026776423677802086\n",
      "After scaling - encoder.proj.weight: grad norm 2.9442625045776367\n",
      "After scaling - encoder.proj.bias: grad norm 0.04650134593248367\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.897030770778656\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03809804469347\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.7191808223724365\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.03476821631193161\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.977464199066162\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.034200772643089294\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.68753981590271\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.05442439392209053\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.1820629835128784\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.035966984927654266\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.019344041123986244\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00044417715980671346\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02444523759186268\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00047953653847798705\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019002540037035942\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00041954676271416247\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.022383077070116997\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004127574502490461\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.0453856885433197\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007168163429014385\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013827693648636341\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005872798501513898\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04191606119275093\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0005359506467357278\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04589748755097389\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0005272035486996174\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04142831638455391\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0008389498107135296\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018221452832221985\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005544296582229435\n",
      "epoch: 152 ( 1 ) recon_loss: 0.30193689465522766  perplexity:  215.4492645263672  commit_loss:  0.147409588098526 \n",
      "\t codebook loss:  0.589638352394104  total_loss:  0.9211956858634949 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9351, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.153660774230957\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.02980928309261799\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.440907597541809\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.03303971886634827\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.2106837034225464\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02829182893037796\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.4095606803894043\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.027673684060573578\n",
      "After scaling - encoder.proj.weight: grad norm 2.5534229278564453\n",
      "After scaling - encoder.proj.bias: grad norm 0.04629296436905861\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.737836480140686\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.031330544501543045\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.212519884109497\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.027547545731067657\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.473004102706909\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.026904260739684105\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.427164077758789\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04616521671414375\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.0581438541412354\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.030149048194289207\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02040770836174488\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0005273120477795601\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.025488967075943947\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0005844568950124085\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.021416418254375458\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0005004690028727055\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02493445761501789\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004895342863164842\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.045168839395046234\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0008189004729501903\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013051975518465042\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005542223807424307\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03913842514157295\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00048730295384302735\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04374626651406288\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00047592355986125767\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04293537884950638\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0008166407351382077\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018718061968684196\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005333223380148411\n",
      "Are there any dead codes on this epoch?  136\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9196, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.197005271911621\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.02485727146267891\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.4790351390838623\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.024754837155342102\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.1069163084030151\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.021906128153204918\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.425538182258606\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.024100173264741898\n",
      "After scaling - encoder.proj.weight: grad norm 2.6862170696258545\n",
      "After scaling - encoder.proj.bias: grad norm 0.040913116186857224\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.7846183776855469\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03028702177107334\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.3714663982391357\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.02760281227529049\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.6143388748168945\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.02722473256289959\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.4758238792419434\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04464423656463623\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.1125373840332031\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.028728721663355827\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02036605216562748\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00042292592115700245\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.025164557620882988\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004211830673739314\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.018833264708518982\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.000372714624973014\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02425435185432434\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00041004447848536074\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04570375755429268\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006961027393117547\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013349631801247597\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005153085803613067\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.040348537266254425\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004696389951277524\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.044480808079242706\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00046320632100105286\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04212409257888794\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007595847127959132\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018928902223706245\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004887953982688487\n",
      "epoch: 153 ( 1 ) recon_loss: 0.2830312252044678  perplexity:  213.19076538085938  commit_loss:  0.1515415906906128 \n",
      "\t codebook loss:  0.6061663627624512  total_loss:  0.9196262359619141 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9038, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.1818222999572754\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.024603093042969704\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.5014508962631226\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.028335629031062126\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.1554614305496216\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.024428963661193848\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.3586589097976685\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.02405621111392975\n",
      "After scaling - encoder.proj.weight: grad norm 2.5474205017089844\n",
      "After scaling - encoder.proj.bias: grad norm 0.041001591831445694\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.7287895679473877\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.026712799444794655\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.162613868713379\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.02397424727678299\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.2501442432403564\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.02291739545762539\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.2977054119110107\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.03973688557744026\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.080133080482483\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.027285348623991013\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.021560074761509895\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004488361009862274\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.027391083538532257\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0005169290816411376\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.021079171448946\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0004456594178918749\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.024786118417978287\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004388592788018286\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.046472784131765366\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007479951600544155\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013295364566147327\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004873236466664821\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.039452727884054184\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.000437364011304453\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04104955494403839\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00041808380046859384\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.041917216032743454\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007249230402521789\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.019704950973391533\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004977686912752688\n",
      "Are there any dead codes on this epoch?  131\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9094, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.3334755897521973\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.022695232182741165\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.6182323694229126\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.022219805046916008\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.152988314628601\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.01983753591775894\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.4945555925369263\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.02280840463936329\n",
      "After scaling - encoder.proj.weight: grad norm 2.7463462352752686\n",
      "After scaling - encoder.proj.bias: grad norm 0.03832336515188217\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.7689388990402222\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.025319945067167282\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.2399253845214844\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.02278599701821804\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.4703164100646973\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.022921614348888397\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.5824947357177734\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.040258463472127914\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.2504197359085083\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.026941591873764992\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02239520289003849\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00038115758798085153\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.027177583426237106\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003731729811988771\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019363990053534508\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003331637126393616\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.025100478902459145\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.000383058242732659\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.046123817563056946\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006436260300688446\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.012914028950035572\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00042523862794041634\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03761867433786392\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00038268195930868387\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04148799926042557\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00038495956687256694\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04337199404835701\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006761252298019826\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.021000312641263008\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004524735559243709\n",
      "epoch: 154 ( 1 ) recon_loss: 0.2877633273601532  perplexity:  214.31336975097656  commit_loss:  0.14799675345420837 \n",
      "\t codebook loss:  0.5919870138168335  total_loss:  0.9094458818435669 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9400, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.3444952964782715\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.023476850241422653\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.7481492757797241\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.02697373740375042\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.247667670249939\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02349153161048889\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.505128264427185\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.0236845500767231\n",
      "After scaling - encoder.proj.weight: grad norm 2.993222951889038\n",
      "After scaling - encoder.proj.bias: grad norm 0.04052909091114998\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.8418030738830566\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.02547921985387802\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.478424072265625\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.023861439898610115\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.4047467708587646\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.021513016894459724\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.299389123916626\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.03414633125066757\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.1666760444641113\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.023705532774329185\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.0221228189766407\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003862967132590711\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.028764687478542328\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004438357427716255\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.02052958309650421\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.000386538275051862\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02476593293249607\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.000389714288758114\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04925158992409706\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006668805726803839\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013851337134838104\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004192444321233779\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04078090190887451\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.000392624904634431\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03956858441233635\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00035398307954892516\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03783499076962471\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.000561856257263571\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01919691637158394\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00039005951839499176\n",
      "Are there any dead codes on this epoch?  129\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9039, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.133346438407898\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.018511353060603142\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.429038166999817\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.018086925148963928\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.0004671812057495\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.015916025266051292\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.3017332553863525\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.018758075311779976\n",
      "After scaling - encoder.proj.weight: grad norm 2.409940481185913\n",
      "After scaling - encoder.proj.bias: grad norm 0.03101455606520176\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.6358174085617065\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.019290966913104057\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 1.7942767143249512\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.016578303650021553\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 1.9938266277313232\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.017126303166151047\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.135366201400757\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.032382309436798096\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.0375664234161377\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.02129671536386013\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.022635288536548615\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00036971032386645675\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.028540870174765587\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00036123357131145895\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019981414079666138\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003178761980962008\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.025998326018452644\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003746378642972559\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.048131536692380905\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006194253219291568\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.012698598206043243\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00038528081495314837\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.035835448652505875\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00033110324875451624\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.039820872247219086\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003420479770284146\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04264771565794945\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.000646742177195847\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.020722363144159317\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00042533979285508394\n",
      "epoch: 155 ( 1 ) recon_loss: 0.27165037393569946  perplexity:  212.967041015625  commit_loss:  0.15052099525928497 \n",
      "\t codebook loss:  0.6020839810371399  total_loss:  0.903914749622345 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9251, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.3492480516433716\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.024329964071512222\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.7520039081573486\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.027026470750570297\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.2603965997695923\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02399338223040104\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.6070672273635864\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.02589096501469612\n",
      "After scaling - encoder.proj.weight: grad norm 3.2291083335876465\n",
      "After scaling - encoder.proj.bias: grad norm 0.043806303292512894\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.9198422431945801\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.02997511625289917\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.738534450531006\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.02812996506690979\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.6656551361083984\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.02551303431391716\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.494591474533081\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.039483316242694855\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.2395853996276855\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.02698947675526142\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02064594440162182\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00037229261943139136\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.0268088411539793\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00041355405119247735\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01928635686635971\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003671423182822764\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.024591045454144478\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00039617877337150276\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.049411218613386154\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006703159306198359\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014075255021452904\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004586736613418907\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.041904550045728683\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00043043954065069556\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04078936204314232\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00039039572584442794\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03817177936434746\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.000604166416451335\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01896790787577629\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004129879816900939\n",
      "Are there any dead codes on this epoch?  131\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8901, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.0225602388381958\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.017242535948753357\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.3130971193313599\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.01712590456008911\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 0.9392634630203247\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.014899708330631256\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.212786078453064\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.017142951488494873\n",
      "After scaling - encoder.proj.weight: grad norm 2.2652201652526855\n",
      "After scaling - encoder.proj.bias: grad norm 0.02856891043484211\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.5999847650527954\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.018844105303287506\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 1.670351505279541\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.015811821445822716\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 1.8517577648162842\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.016548095270991325\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 1.965620756149292\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.03127511218190193\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 0.9623764157295227\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.021348975598812103\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02198811247944832\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003707662399392575\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.028235526755452156\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00036825830466113985\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.020196981728076935\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003203884116373956\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.026078538969159126\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00036862483830191195\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04870903119444847\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006143172504380345\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.012901471927762032\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00040520477341488004\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03591756895184517\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003400015993975103\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03981834650039673\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003558337048161775\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04226674512028694\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.000672508729621768\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.020693982020020485\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00045906699961051345\n",
      "epoch: 156 ( 1 ) recon_loss: 0.26885026693344116  perplexity:  213.49839782714844  commit_loss:  0.14789724349975586 \n",
      "\t codebook loss:  0.5915889739990234  total_loss:  0.8900725245475769 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9592, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.4431670904159546\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.033631231635808945\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.8052186965942383\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.03477766737341881\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.4438929557800293\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.030989930033683777\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.908360481262207\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.034945301711559296\n",
      "After scaling - encoder.proj.weight: grad norm 3.6627490520477295\n",
      "After scaling - encoder.proj.bias: grad norm 0.05876118317246437\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.0039327144622803\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03753102943301201\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.0565319061279297\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.03464055061340332\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.1530232429504395\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.032343506813049316\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.876626491546631\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.052889641374349594\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.31276535987854\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.035443585366010666\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01949695497751236\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00045435252832248807\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.024388212710618973\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00046984065556898713\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01950676180422306\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0004186689620837569\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.025781642645597458\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004721053992398083\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.049483150243759155\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007938541239127517\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01356296893209219\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005070382030680776\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.041293252259492874\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00046798825496807694\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.042596835643053055\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00043695559725165367\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03886276111006737\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007145304698497057\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.017735248431563377\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004788371443282813\n",
      "Are there any dead codes on this epoch?  130\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9042, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 0.867701530456543\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.020591098815202713\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.0404462814331055\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.020309031009674072\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 0.8732545375823975\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.017590239644050598\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.182812213897705\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.019818713888525963\n",
      "After scaling - encoder.proj.weight: grad norm 2.162217855453491\n",
      "After scaling - encoder.proj.bias: grad norm 0.03295140713453293\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.5809035301208496\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.021289484575390816\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 1.6018096208572388\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.017419157549738884\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 1.7670820951461792\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.017796503379940987\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 1.8845115900039673\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.03459986671805382\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 0.8600679636001587\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.023890893906354904\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.019909249618649483\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00047245892346836627\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.023872846737504005\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00046598693006671965\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.020036661997437477\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00040360476123169065\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02713940665125847\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00045473669888451695\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.049611687660217285\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007560638478025794\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01332872360944748\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004884832887910306\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03675322234630585\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003996792947873473\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04054536670446396\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0004083374224137515\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.0432397685945034\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007938874769024551\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.019734099507331848\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005481720436364412\n",
      "epoch: 157 ( 1 ) recon_loss: 0.26257163286209106  perplexity:  212.72142028808594  commit_loss:  0.1527690589427948 \n",
      "\t codebook loss:  0.6110762357711792  total_loss:  0.9042357206344604 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9482, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.5456030368804932\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.036726873368024826\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.8848739862442017\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.03709053248167038\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.5143283605575562\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.03289209306240082\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.9639102220535278\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.03761760890483856\n",
      "After scaling - encoder.proj.weight: grad norm 3.722450017929077\n",
      "After scaling - encoder.proj.bias: grad norm 0.06294476240873337\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.025508165359497\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.04185202717781067\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.1522562503814697\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.03744172304868698\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.3282246589660645\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.0357915423810482\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 3.2243502140045166\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.06346472352743149\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.4155409336090088\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.042206425219774246\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.019881954416632652\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.000472438259748742\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02424618788063526\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00047711620572954416\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019479651004076004\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00042310936260037124\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.025262873619794846\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00048389637959189713\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.047883953899145126\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0008096935926005244\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013191684149205685\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005383659736253321\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04054923355579376\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00048163378960452974\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04281281307339668\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00046040661982260644\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.041476618498563766\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.00081638217670843\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01820889301598072\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005429248558357358\n",
      "Are there any dead codes on this epoch?  134\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9150, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.3338251113891602\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03165411204099655\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.5724610090255737\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.03166796639561653\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.3257555961608887\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.027913382276892662\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.7981986999511719\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.03126838430762291\n",
      "After scaling - encoder.proj.weight: grad norm 3.3595266342163086\n",
      "After scaling - encoder.proj.bias: grad norm 0.052876465022563934\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.947523832321167\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03486553579568863\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.5996885299682617\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.029610948637127876\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.7995564937591553\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.02960154600441456\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.9253931045532227\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.05646580085158348\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.2842131853103638\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.038674551993608475\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.01961415447294712\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00046547973761335015\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02312334068119526\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004656835226342082\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019495490938425064\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0004104716354049742\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02644285373389721\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004598075756803155\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04940248280763626\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007775584817864001\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013933519832789898\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005127043114043772\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03822891786694527\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00043543463107198477\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04116801545023918\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00043529641698114574\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.043018463999032974\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0008303404320031404\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018884599208831787\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005687166703864932\n",
      "epoch: 158 ( 1 ) recon_loss: 0.30255404114723206  perplexity:  213.916015625  commit_loss:  0.14582403004169464 \n",
      "\t codebook loss:  0.5832961201667786  total_loss:  0.9150269627571106 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9246, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.2339946031570435\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.028843635693192482\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.5185092687606812\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.028438711538910866\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.2011795043945312\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02487693727016449\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.5445698499679565\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.028823573142290115\n",
      "After scaling - encoder.proj.weight: grad norm 2.8986799716949463\n",
      "After scaling - encoder.proj.bias: grad norm 0.047789566218853\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.7922675609588623\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.031507380306720734\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.4352667331695557\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.02804388850927353\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.587848424911499\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.026825765147805214\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.4170773029327393\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04609997197985649\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.1061879396438599\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.030525967478752136\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.020455649122595787\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00047813437413424253\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.025171982124447823\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004714220413006842\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01991168037056923\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0004123793332837522\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02560398355126381\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004778018337674439\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04805075749754906\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007921967771835625\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01313323900103569\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005222907057031989\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04036886617541313\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00046487717190757394\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04289817437529564\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00044468461419455707\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.040067341178655624\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007641887641511858\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018337024375796318\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005060220137238503\n",
      "Are there any dead codes on this epoch?  136\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9216, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.2855557203292847\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03066144324839115\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.4809012413024902\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.030920693650841713\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.2000608444213867\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.026438429951667786\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.5084095001220703\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.028019294142723083\n",
      "After scaling - encoder.proj.weight: grad norm 2.9029250144958496\n",
      "After scaling - encoder.proj.bias: grad norm 0.04724907502532005\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.8465924263000488\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03289154917001724\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.3571503162384033\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.0267607644200325\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.598435878753662\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.02705303765833378\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.7213048934936523\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.050736505538225174\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.2437076568603516\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.034427203238010406\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02086075209081173\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004975442425347865\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02403063140809536\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0005017510848119855\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019473424181342125\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.000429017236456275\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02447700686752796\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00045466996380127966\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.047105852514505386\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007667121826671064\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01373768225312233\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005337321781553328\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03824955224990845\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00043424777686595917\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04216489940881729\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0004389904788695276\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.0441587008535862\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0008233028347603977\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.02018168196082115\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005586512852460146\n",
      "epoch: 159 ( 1 ) recon_loss: 0.2853872776031494  perplexity:  213.0151824951172  commit_loss:  0.15148265659809113 \n",
      "\t codebook loss:  0.6059306263923645  total_loss:  0.9216057658195496 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9129, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.3517564535140991\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.02207646332681179\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.7657932043075562\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.022116390988230705\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.2898375988006592\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.019366402179002762\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.6216692924499512\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.02240268886089325\n",
      "After scaling - encoder.proj.weight: grad norm 3.130474090576172\n",
      "After scaling - encoder.proj.bias: grad norm 0.036933381110429764\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.834814727306366\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.024069003760814667\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.417447805404663\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.020788801833987236\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.4534752368927\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.0209461972117424\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.318443536758423\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.03773098811507225\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.137916922569275\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.025682086125016212\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.021849200129508972\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00035683429450728\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.028541510924696922\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00035747967194765806\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.02084837295114994\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003130300610791892\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.026211954653263092\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003621072683017701\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05059961602091789\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005969750345684588\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013493581674993038\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0003890408552251756\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03907456994056702\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003360210976097733\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.039656899869441986\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003385651798453182\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03747430443763733\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006098672747612\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018392791971564293\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00041511401650495827\n",
      "Are there any dead codes on this epoch?  127\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8918, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.1787612438201904\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.02483176253736019\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.424306035041809\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.025011874735355377\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.0784080028533936\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02170560508966446\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.2988770008087158\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.022755291312932968\n",
      "After scaling - encoder.proj.weight: grad norm 2.5934464931488037\n",
      "After scaling - encoder.proj.bias: grad norm 0.03901045769453049\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.7701707482337952\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.030198292806744576\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.179408311843872\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.02503887377679348\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.377687931060791\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.025324782356619835\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.407686710357666\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04563065618276596\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.1099880933761597\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03074425272643566\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02116144448518753\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00044578660163097084\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.025569533929228783\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00044902003719471395\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019359875470399857\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003896649868693203\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.023317797109484673\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004085091932211071\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04655826464295387\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007003264036029577\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013826316222548485\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005421280511654913\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03912533447146416\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004495047323871404\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04268490895628929\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0004546374548226595\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04322345554828644\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0008191741071641445\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01992681249976158\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005519292899407446\n",
      "epoch: 160 ( 1 ) recon_loss: 0.2790767550468445  perplexity:  215.00274658203125  commit_loss:  0.14589792490005493 \n",
      "\t codebook loss:  0.5835916996002197  total_loss:  0.8918194770812988 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9368, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.3882983922958374\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.015572335571050644\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.8347842693328857\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.016223061829805374\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.2698339223861694\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.014363597147166729\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.5553745031356812\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.01597769558429718\n",
      "After scaling - encoder.proj.weight: grad norm 3.1179752349853516\n",
      "After scaling - encoder.proj.bias: grad norm 0.026445478200912476\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.837619960308075\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.017651628702878952\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.3122706413269043\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.014623367227613926\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.270542621612549\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.014605756849050522\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.2353949546813965\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.027361618354916573\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.180193305015564\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.01923595927655697\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.022934023290872574\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00025724750594235957\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.030309755355119705\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00026799720944836736\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.020977048203349113\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00023727973166387528\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.025694040581583977\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00026394386077299714\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.051507458090782166\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.00043686662684194744\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01383708044886589\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0002915964287240058\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.038197603076696396\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00024157101870514452\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.0375082790851593\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00024128009681589901\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03692765533924103\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.00045200082240626216\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01949622854590416\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0003177688049618155\n",
      "Are there any dead codes on this epoch?  130\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9176, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.5540392398834229\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.028042705729603767\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.9935230016708374\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.027390247210860252\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.4530963897705078\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02415372245013714\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.7801897525787354\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.02617952972650528\n",
      "After scaling - encoder.proj.weight: grad norm 3.5025670528411865\n",
      "After scaling - encoder.proj.bias: grad norm 0.0448184497654438\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.0362210273742676\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03336755186319351\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.9045772552490234\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.028407853096723557\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.021143913269043\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.02814621292054653\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.756788730621338\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04675929993391037\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.347090721130371\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03146066144108772\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02155052311718464\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003888800856657326\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.027645030990242958\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003798321995418519\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.02015070430934429\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00033494996023364365\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.024686647579073906\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00036304263630881906\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04857158660888672\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.000621516490355134\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014369716867804527\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004627220332622528\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04027900844812393\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00039394377381540835\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.041895486414432526\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00039031548658385873\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.0382295586168766\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006484310724772513\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01868068054318428\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00043627838022075593\n",
      "epoch: 161 ( 1 ) recon_loss: 0.29379647970199585  perplexity:  213.97669982910156  commit_loss:  0.1485297828912735 \n",
      "\t codebook loss:  0.594119131565094  total_loss:  0.9175720810890198 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9737, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.8252538442611694\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.014852424152195454\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.587155342102051\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.015697307884693146\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.8641364574432373\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.01461796835064888\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 2.416964530944824\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.016122134402394295\n",
      "After scaling - encoder.proj.weight: grad norm 5.011011600494385\n",
      "After scaling - encoder.proj.bias: grad norm 0.02732909843325615\n",
      "After scaling - decoder.in_proj.weight: grad norm 1.2489320039749146\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.018827160820364952\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 3.387514591217041\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.015345517545938492\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 3.118699312210083\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.014303693547844887\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.7811572551727295\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.024484338238835335\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.4897704124450684\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.016940535977482796\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02077765204012394\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00016907155804801732\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.029450705274939537\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00017868923896458\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.021220270544290543\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00016640264948364347\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.027513351291418076\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.000183525204192847\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05704250931739807\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.00031109893461689353\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014217132702469826\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00021431769710034132\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.038561541587114334\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00017468465375714004\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.035501498728990555\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0001628251193324104\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.031659115105867386\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0002787158009596169\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.016958700492978096\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00019284142763353884\n",
      "Are there any dead codes on this epoch?  131\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8776, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.0621299743652344\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.022885240614414215\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.27474045753479\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.02188011072576046\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 0.9988816380500793\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.01962335593998432\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.2190333604812622\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.021249113604426384\n",
      "After scaling - encoder.proj.weight: grad norm 2.3384742736816406\n",
      "After scaling - encoder.proj.bias: grad norm 0.036561641842126846\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.7232275009155273\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.02878859080374241\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.1336312294006348\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.025172725319862366\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.2559430599212646\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.02423105388879776\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.1171464920043945\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04092814400792122\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 0.9926472902297974\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.028327912092208862\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02069307118654251\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00044586436706595123\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02483527921140194\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004262818256393075\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019460827112197876\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003823143197223544\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.023749958723783493\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004139883676543832\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.0455595999956131\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007123164250515401\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014090364798903465\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005608770879916847\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04156871885061264\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004904305096715689\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04395167529582977\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00047208432806655765\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04124755412340164\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007973872707225382\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.019339367747306824\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005519018741324544\n",
      "epoch: 162 ( 1 ) recon_loss: 0.26781386137008667  perplexity:  216.52076721191406  commit_loss:  0.14519226551055908 \n",
      "\t codebook loss:  0.5807690620422363  total_loss:  0.8775532245635986 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9488, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.2857599258422852\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.016676796600222588\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.7181711196899414\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.01726057194173336\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.2025558948516846\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.01496847067028284\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.5697659254074097\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.015285445377230644\n",
      "After scaling - encoder.proj.weight: grad norm 3.206908941268921\n",
      "After scaling - encoder.proj.bias: grad norm 0.02627583220601082\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.8446497321128845\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.019139191135764122\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.327488422393799\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.01618521474301815\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.2362937927246094\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.015281070955097675\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.2308876514434814\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.026622775942087173\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.185698390007019\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.018592407926917076\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.021335339173674583\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0002767274563666433\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.028510581701993942\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00028641437529586256\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019954686984419823\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00024838026729412377\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02604801021516323\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00025363999884575605\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.053214047104120255\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0004360096645541489\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014015748165547848\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0003175873716827482\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03862132132053375\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00026857038028538227\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.037108082324266434\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00025356741389259696\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03701837733387947\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0004417666932567954\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.019674958661198616\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0003085142816416919\n",
      "Are there any dead codes on this epoch?  132\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8927, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.075830340385437\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.023922977969050407\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.2690593004226685\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.022403893992304802\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 0.9916708469390869\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.019879383966326714\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.1992110013961792\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.02089661732316017\n",
      "After scaling - encoder.proj.weight: grad norm 2.3173372745513916\n",
      "After scaling - encoder.proj.bias: grad norm 0.03603392839431763\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.7304041385650635\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.02806100808084011\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.155698776245117\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.024859758093953133\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.3013699054718018\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.024106208235025406\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.137784719467163\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.039354097098112106\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.000746726989746\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.027131788432598114\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.020853407680988312\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004637122037820518\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.024598872289061546\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004342669271863997\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01922209933400154\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003853329981211573\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.023244963958859444\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004050506104249507\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04491822049021721\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006984653300605714\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014157822355628014\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005439218948595226\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.041785091161727905\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004818703164346516\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.044608715921640396\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0004672638315241784\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04143785312771797\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007628220482729375\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.019398022443056107\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.000525910290889442\n",
      "epoch: 163 ( 1 ) recon_loss: 0.2650812566280365  perplexity:  213.8866424560547  commit_loss:  0.14944325387477875 \n",
      "\t codebook loss:  0.597773015499115  total_loss:  0.8926525115966797 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9223, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.0349438190460205\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.024101808667182922\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.3084468841552734\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.02369324490427971\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.027879238128662\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.019988838583230972\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.3466323614120483\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.021355556324124336\n",
      "After scaling - encoder.proj.weight: grad norm 2.596796751022339\n",
      "After scaling - encoder.proj.bias: grad norm 0.03723751753568649\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.6855709552764893\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.025150049477815628\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 1.9810287952423096\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.021738238632678986\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.116234302520752\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.021539833396673203\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.1223204135894775\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.0387750044465065\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 0.9676766991615295\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.026059092953801155\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02002265490591526\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00046628821291960776\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.025314008817076683\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.000458383874502033\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019885975867509842\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00038671621587127447\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.026052767410874367\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00041315756971016526\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.0502392053604126\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007204196881502867\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013263472355902195\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00048656814033165574\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.038326188921928406\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004205611767247319\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.040941957384347916\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00041672270162962377\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.0410596989095211\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.000750164850614965\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018721260130405426\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005041550612077117\n",
      "Are there any dead codes on this epoch?  131\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8795, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.112929344177246\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.02073216438293457\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.3209317922592163\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.019118232652544975\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 0.9937717318534851\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.01703626848757267\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.2250787019729614\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.01798318512737751\n",
      "After scaling - encoder.proj.weight: grad norm 2.3371076583862305\n",
      "After scaling - encoder.proj.bias: grad norm 0.030404791235923767\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.7160901427268982\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.023580214008688927\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.071441650390625\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.02097044512629509\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.1214349269866943\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.020286941900849342\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.171696662902832\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.03637067601084709\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.0783462524414062\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.0255600456148386\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.021774932742118835\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004056335601489991\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02584458887577057\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003740563115570694\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019443562254309654\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003333218046464026\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02396918088197708\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003518486046232283\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04572650045156479\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005948825855739415\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014010605402290821\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00046135683078318834\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04052862897515297\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004102956154383719\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04150677099823952\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003969225799664855\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.0424901619553566\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007116076303645968\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.021098298951983452\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005000930977985263\n",
      "epoch: 164 ( 1 ) recon_loss: 0.27022412419319153  perplexity:  215.67335510253906  commit_loss:  0.14510071277618408 \n",
      "\t codebook loss:  0.5804028511047363  total_loss:  0.8795397281646729 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9149, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.0588555335998535\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.027044586837291718\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.3056646585464478\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.02530992217361927\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.0841635465621948\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.021562213078141212\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.4074969291687012\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.023224564269185066\n",
      "After scaling - encoder.proj.weight: grad norm 2.6500370502471924\n",
      "After scaling - encoder.proj.bias: grad norm 0.04057503119111061\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.7171808481216431\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.026920147240161896\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.0694057941436768\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.02321660704910755\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.2785565853118896\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.023493552580475807\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.2884955406188965\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.042654383927583694\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.014594316482544\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.029339168220758438\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.0195719413459301\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004998936201445758\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02413397654891014\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004678299883380532\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.020039737224578857\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003985571092925966\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.026016250252723694\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004292840894777328\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04898343235254288\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007499910425394773\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013256410136818886\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004975934280082583\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03825100511312485\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004291370278224349\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04211696237325668\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00043425612966530025\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04230067506432533\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007884259684942663\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018753817304968834\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005423068068921566\n",
      "Are there any dead codes on this epoch?  131\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8687, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 0.8426243662834167\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.01824246160686016\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 0.9927880167961121\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.017075687646865845\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 0.7659247517585754\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.014779450371861458\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 0.9690748453140259\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.015452723018825054\n",
      "After scaling - encoder.proj.weight: grad norm 1.8022290468215942\n",
      "After scaling - encoder.proj.bias: grad norm 0.024798952043056488\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.5283306241035461\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.018418440595269203\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 1.5389460325241089\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.016248486936092377\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 1.6041948795318604\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.01530784647911787\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 1.8036503791809082\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.030072249472141266\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 0.8723105788230896\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.021345486864447594\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.021260278299450874\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00046027606003917754\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.025049060583114624\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00043083715718239546\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019325068220496178\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00037290071486495435\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.0244507547467947\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003898880968336016\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04547209292650223\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006257030181586742\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013330324552953243\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00046471619862131774\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03882918879389763\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00040996604366227984\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04047548770904541\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003862326848320663\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04550795629620552\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007587537984363735\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.022009292617440224\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005385685944929719\n",
      "epoch: 165 ( 1 ) recon_loss: 0.2527232766151428  perplexity:  213.3634490966797  commit_loss:  0.14668670296669006 \n",
      "\t codebook loss:  0.5867468118667603  total_loss:  0.8686792850494385 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9143, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.2534453868865967\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.02741214632987976\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.576588749885559\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.02506980672478676\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.2792539596557617\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02217908762395382\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.6174384355545044\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.024031635373830795\n",
      "After scaling - encoder.proj.weight: grad norm 2.995312213897705\n",
      "After scaling - encoder.proj.bias: grad norm 0.04112495481967926\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.8368804454803467\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.027587948366999626\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.307605028152466\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.02471235767006874\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.6025140285491943\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.025695031508803368\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.5993053913116455\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04641079902648926\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.2017691135406494\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03206871822476387\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.020256489515304565\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00044299804721958935\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.025478694587945938\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00040514435386285186\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.02067357301712036\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003584284277167171\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.026138853281736374\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003883668396156281\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04840618371963501\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006646059337072074\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013524530455470085\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00044583913404494524\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.037292394787073135\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.000399367738282308\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.0420583114027977\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.000415248388890177\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.042006462812423706\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007500286446884274\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.019421368837356567\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.000518251268658787\n",
      "Are there any dead codes on this epoch?  126\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8700, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.0327633619308472\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.02166031114757061\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.279319405555725\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.020879268646240234\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 0.9527410864830017\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.017376039177179337\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.1161487102508545\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.016522152349352837\n",
      "After scaling - encoder.proj.weight: grad norm 2.265782594680786\n",
      "After scaling - encoder.proj.bias: grad norm 0.026829469949007034\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.6487729549407959\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.02197282761335373\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 1.8471721410751343\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.01859375275671482\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 1.9290311336517334\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.018335245549678802\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.151432514190674\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.036810602992773056\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.0645896196365356\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.027598416432738304\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.021389944478869438\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004486147372517735\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.026496460661292076\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004324382753111422\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019732574000954628\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003598816110752523\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02311697043478489\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003421964356675744\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04692746326327324\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.000555675127543509\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013436977751553059\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004550874000415206\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03825746849179268\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.000385102117434144\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03995288163423538\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003797480894718319\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.044559113681316376\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007623980636708438\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.022049110382795334\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005716010928153992\n",
      "epoch: 166 ( 1 ) recon_loss: 0.272020548582077  perplexity:  216.55361938476562  commit_loss:  0.1424151360988617 \n",
      "\t codebook loss:  0.5696605443954468  total_loss:  0.8700207471847534 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9035, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.0778028964996338\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.020475804805755615\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.3824185132980347\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.01885635033249855\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.0705236196517944\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.01692943647503853\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.3921232223510742\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.01833771914243698\n",
      "After scaling - encoder.proj.weight: grad norm 2.6198482513427734\n",
      "After scaling - encoder.proj.bias: grad norm 0.03139343485236168\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.7518594264984131\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.0211544968187809\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.0134894847869873\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.018798332661390305\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.167510747909546\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.019575737416744232\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.267662763595581\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.03642917424440384\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.0972998142242432\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.024541985243558884\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.020122215151786804\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003822763392236084\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.025809284299612045\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003520416503306478\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019986314699053764\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00031606684206053615\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.025990469381213188\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00034235898056067526\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.0489116795361042\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005861047538928688\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014036960899829865\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00039494727388955653\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03759116679430008\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00035095849307253957\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04046669229865074\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003654723986983299\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.042336493730545044\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006801203708164394\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.020486215129494667\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004581905377563089\n",
      "Are there any dead codes on this epoch?  130\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8864, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.1845417022705078\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.024537410587072372\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.47133469581604\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.024394599720835686\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.112182378768921\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.020272087305784225\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.2399383783340454\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.01945778913795948\n",
      "After scaling - encoder.proj.weight: grad norm 2.4661364555358887\n",
      "After scaling - encoder.proj.bias: grad norm 0.03257805109024048\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.7069391012191772\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.02430715225636959\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.040848970413208\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.0201247688382864\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.130694627761841\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.020063530653715134\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.1657447814941406\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.037338025867938995\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.095292568206787\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.02719924785196781\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02257782593369484\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.000467692589154467\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.028044208884239197\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00046497053699567914\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.02119862660765648\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003863938618451357\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02363370731472969\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00037087302189320326\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04700551927089691\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006209503044374287\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013474533334374428\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.000463303760625422\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03889937698841095\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00038358592428267\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.0406118668615818\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00038241868605837226\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04127993434667587\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007116773049347103\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.020876701921224594\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.000518428219947964\n",
      "epoch: 167 ( 1 ) recon_loss: 0.2640727460384369  perplexity:  212.96337890625  commit_loss:  0.148215651512146 \n",
      "\t codebook loss:  0.592862606048584  total_loss:  0.8864097595214844 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9097, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.285994291305542\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.0131524121388793\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.8096520900726318\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.012791278772056103\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.2722277641296387\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.011416957713663578\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.7029982805252075\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.011988679878413677\n",
      "After scaling - encoder.proj.weight: grad norm 3.4446945190429688\n",
      "After scaling - encoder.proj.bias: grad norm 0.020337341353297234\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.8805112242698669\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.016458475962281227\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.25716495513916\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.013721742667257786\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.162623882293701\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.014234066940844059\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.3165369033813477\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.028607070446014404\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.1998614072799683\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.019283242523670197\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.020680269226431847\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00021150596148800105\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.029101291671395302\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00020569849584717304\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.0204588882625103\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00018359784735366702\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.027386175468564034\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00019279179105069488\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05539465695619583\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0003270478919148445\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01415963564068079\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00026467128191143274\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.036297813057899475\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0002206614299211651\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03477748483419418\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00022890021500643343\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03725258260965347\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0004600346728693694\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01929515413939953\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00031009677331894636\n",
      "Are there any dead codes on this epoch?  128\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8586, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.041635513305664\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.01959940977394581\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.3462382555007935\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.019966498017311096\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 0.9997597932815552\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.016961848363280296\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.1322071552276611\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.016491184011101723\n",
      "After scaling - encoder.proj.weight: grad norm 2.241302251815796\n",
      "After scaling - encoder.proj.bias: grad norm 0.027208231389522552\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.6408261060714722\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.021512914448976517\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 1.8439304828643799\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.017701808363199234\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 1.9381351470947266\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.01776168867945671\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 1.9409244060516357\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.03370947763323784\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 0.9622785449028015\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.02410714142024517\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02197553776204586\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004134916525799781\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02840178832411766\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004212361527606845\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.02109207957983017\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.000357846642145887\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.023886343464255333\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00034791696816682816\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.047285083681344986\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.000574016070459038\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013519602827727795\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00045386116835288703\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.0389016754925251\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003734576457645744\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04088912159204483\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003747209848370403\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04094797000288963\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007111738086678088\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.0203013326972723\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005085919401608407\n",
      "epoch: 168 ( 1 ) recon_loss: 0.25736263394355774  perplexity:  217.54823303222656  commit_loss:  0.14319351315498352 \n",
      "\t codebook loss:  0.5727740526199341  total_loss:  0.8585935831069946 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9213, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.1994357109069824\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.017459683120250702\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.5662068128585815\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.017946161329746246\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.0859103202819824\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.015623735263943672\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.3443803787231445\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.01650093123316765\n",
      "After scaling - encoder.proj.weight: grad norm 2.621988296508789\n",
      "After scaling - encoder.proj.bias: grad norm 0.02623242512345314\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.8217455148696899\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.02133740484714508\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.2714450359344482\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.017523853108286858\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.2898991107940674\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.017598632723093033\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.433800220489502\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.0346800722181797\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.2182281017303467\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.023315517231822014\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.021123006939888\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003074787382502109\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.027582135051488876\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003160460328217596\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019123733043670654\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0002751462743617594\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.023675596341490746\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0002905943838413805\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.0461752749979496\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.00046197359915822744\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014471584931015968\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0003757684607990086\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04000193625688553\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00030860883998684585\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04032691940665245\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003099257592111826\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04286113753914833\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006107433582656085\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.02145395427942276\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004106045817025006\n",
      "Are there any dead codes on this epoch?  125\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8551, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 0.6362400054931641\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.014558727853000164\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 0.7612815499305725\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.014210301451385021\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 0.5770769119262695\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.011931432411074638\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 0.67231684923172\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.01162553671747446\n",
      "After scaling - encoder.proj.weight: grad norm 1.3167130947113037\n",
      "After scaling - encoder.proj.bias: grad norm 0.019052747637033463\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.4350535273551941\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.016216013580560684\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 1.2788188457489014\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.013844538480043411\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 1.3593791723251343\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.013702034018933773\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 1.459890365600586\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.026589352637529373\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 0.7065383791923523\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.018472304567694664\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.020314719527959824\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004648504836950451\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.024307211861014366\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00045372542808763683\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.018425678834319115\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003809626796282828\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.021466627717018127\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00037119563785381615\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04204177111387253\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006083415355533361\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013890968635678291\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005177665152586997\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04083183407783508\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004420468467287719\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.043404072523117065\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00043749678297899663\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04661332443356514\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0008489802712574601\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.02255930006504059\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005898083909414709\n",
      "epoch: 169 ( 1 ) recon_loss: 0.23930080235004425  perplexity:  214.05987548828125  commit_loss:  0.1466577649116516 \n",
      "\t codebook loss:  0.5866310596466064  total_loss:  0.8550581336021423 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9074, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.1579797267913818\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.022635212168097496\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.4605531692504883\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.022702781483530998\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.0993549823760986\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.019795691594481468\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.2911653518676758\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.020325688645243645\n",
      "After scaling - encoder.proj.weight: grad norm 2.39656662940979\n",
      "After scaling - encoder.proj.bias: grad norm 0.033074717968702316\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.8151488900184631\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.02817671373486519\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.318544387817383\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.023050807416439056\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.561182975769043\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.024195291101932526\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.804887056350708\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04864128679037094\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.2925822734832764\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03223728388547897\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.019789598882198334\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003868304193019867\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.024960504844784737\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003879851137753576\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.018787715584039688\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.000338303652824834\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02206571027636528\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003473611723165959\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.040956754237413406\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.000565239111892879\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013930699788033962\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004815333231817931\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03962337225675583\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00039393280167132616\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04377000778913498\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0004134917398914695\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04793485254049301\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0008312679710797966\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.022089922800660133\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005509275360964239\n",
      "Are there any dead codes on this epoch?  134\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8592, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 0.8874948024749756\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.018654825165867805\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.0923374891281128\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.019290415570139885\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 0.8192198872566223\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.01647098921239376\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 0.9422164559364319\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.01580149307847023\n",
      "After scaling - encoder.proj.weight: grad norm 1.907165765762329\n",
      "After scaling - encoder.proj.bias: grad norm 0.026419594883918762\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.6427190899848938\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.024106202647089958\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 1.901141881942749\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.02096054144203663\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.0644166469573975\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.02127951569855213\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.23042631149292\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04125796630978584\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.0216153860092163\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.02795364521443844\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.019144359976053238\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004024076624773443\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.023563070222735405\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004161180986557156\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.017671585083007812\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00035529962042346597\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.020324774086475372\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00034085771767422557\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04113992303609848\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005699032917618752\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013864245265722275\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005200005834922194\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04100997745990753\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004521447990555316\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04453202337026596\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00045902543934062123\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.048113055527210236\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0008899852982722223\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.022037507966160774\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0006029946380294859\n",
      "epoch: 170 ( 1 ) recon_loss: 0.26071226596832275  perplexity:  217.03018188476562  commit_loss:  0.14255432784557343 \n",
      "\t codebook loss:  0.5702173113822937  total_loss:  0.8592224717140198 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9224, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.1941301822662354\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.023931490257382393\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.4779072999954224\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.023811280727386475\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.1021182537078857\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02043055184185505\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.2760612964630127\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.020269319415092468\n",
      "After scaling - encoder.proj.weight: grad norm 2.503075361251831\n",
      "After scaling - encoder.proj.bias: grad norm 0.033368803560733795\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.7896684408187866\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.026682406663894653\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.2570557594299316\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.021903324872255325\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.487652540206909\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.023089520633220673\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.6480159759521484\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.045227859169244766\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.2458555698394775\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03003472089767456\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.020725449547171593\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00041535747004672885\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.025650713592767715\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004132710746489465\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019128480926156044\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00035459481296129525\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02214745432138443\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00035179645055904984\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04344363883137703\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005791524308733642\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013705569319427013\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004631026240531355\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03917369619011879\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003801564162131399\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04317596182227135\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00040074417483992875\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.045959241688251495\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007849794928915799\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.021623199805617332\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005212857504375279\n",
      "Are there any dead codes on this epoch?  129\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8740, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 0.922217607498169\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.021919796243309975\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.0854758024215698\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.021522659808397293\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 0.87704998254776\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.018596628680825233\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.0458827018737793\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.01850046031177044\n",
      "After scaling - encoder.proj.weight: grad norm 2.000246047973633\n",
      "After scaling - encoder.proj.bias: grad norm 0.031068408861756325\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.6707096695899963\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.02550581283867359\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 1.9790358543395996\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.022395052015781403\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.207444667816162\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.022963987663388252\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.3835511207580566\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04488292708992958\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.059622883796692\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.02982141263782978\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.018846938386559486\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004479648487176746\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02218337170779705\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004398487217258662\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01792386919260025\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003800507984124124\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.021374225616455078\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003780854167416692\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.040878113359212875\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.000634930853266269\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013706985861063004\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005212506512179971\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04044465348124504\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00045767746632918715\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04511253535747528\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0004693044757004827\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.048711542040109634\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0009172518621198833\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.02165502868592739\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0006094465497881174\n",
      "epoch: 171 ( 1 ) recon_loss: 0.26045218110084534  perplexity:  213.1102752685547  commit_loss:  0.14613376557826996 \n",
      "\t codebook loss:  0.5845350623130798  total_loss:  0.8739717602729797 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8785, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 0.9930321574211121\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.023383652791380882\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.1230928897857666\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.02217443473637104\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 0.9049070477485657\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.019222181290388107\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.0895891189575195\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.020343076437711716\n",
      "After scaling - encoder.proj.weight: grad norm 1.9798318147659302\n",
      "After scaling - encoder.proj.bias: grad norm 0.034813813865184784\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.6153802871704102\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.024146204814314842\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 1.7765190601348877\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.020491983741521835\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.059112071990967\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.021601559594273567\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.2344515323638916\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04176170751452446\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.0808684825897217\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.02884150668978691\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.021115902811288834\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004972315509803593\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.023881522938609123\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00047151866601780057\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019242003560066223\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0004087417619302869\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.023169096559286118\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004325765185058117\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.042099278420209885\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007402832270599902\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013085487298667431\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005134465172886848\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.037776023149490356\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004357428988441825\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04378509148955345\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00045933699584566057\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04751352593302727\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0008880238165147603\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.02298366092145443\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.000613287731539458\n",
      "Are there any dead codes on this epoch?  131\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8465, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 0.8319644927978516\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.01808190532028675\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.0101494789123535\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.018122483044862747\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 0.8107495307922363\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.01588117517530918\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 0.9970835447311401\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.016266120597720146\n",
      "After scaling - encoder.proj.weight: grad norm 1.9012147188186646\n",
      "After scaling - encoder.proj.bias: grad norm 0.02699718065559864\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.6207224130630493\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.021343031898140907\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 1.8017783164978027\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.01873607747256756\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 1.9732826948165894\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.019150376319885254\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.1779863834381104\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04021166265010834\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.0030477046966553\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.027393726631999016\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.018483811989426613\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004017268947791308\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.022442558780312538\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00040262844413518906\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.018012477084994316\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003528331289999187\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02215227298438549\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00036138552241027355\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.042239412665367126\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005997982225380838\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013790631666779518\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004741795710287988\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04003022983670235\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004162606783211231\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04384055361151695\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00042546517215669155\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04838846996426582\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0008933852077461779\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.022284777835011482\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0006086082430556417\n",
      "epoch: 172 ( 1 ) recon_loss: 0.25796109437942505  perplexity:  216.47802734375  commit_loss:  0.14017687737941742 \n",
      "\t codebook loss:  0.5607075095176697  total_loss:  0.8464536666870117 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9053, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.273195743560791\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.026306960731744766\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.4627498388290405\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.024597665295004845\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.155640721321106\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.021630944684147835\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.4274911880493164\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.023076385259628296\n",
      "After scaling - encoder.proj.weight: grad norm 2.805428981781006\n",
      "After scaling - encoder.proj.bias: grad norm 0.0405108779668808\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.816844642162323\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.026537438854575157\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.3199405670166016\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.02376890368759632\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.5616540908813477\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.024201620370149612\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.6288957595825195\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.0457279309630394\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.2696465253829956\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03148319944739342\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.021176518872380257\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00043755240039899945\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02432929165661335\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00040912244003266096\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01922127604484558\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003597782051656395\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02374284900724888\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003838196280412376\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04666149988770485\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006738000665791333\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013586227782070637\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004413858405314386\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.0385865792632103\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00039533802191726863\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.0426068976521492\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0004025351663585752\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04372530058026314\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007605730788782239\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.02111748792231083\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005236465367488563\n",
      "Are there any dead codes on this epoch?  126\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8680, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.0618822574615479\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.01828816905617714\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.3314721584320068\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.018642308190464973\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 0.9888614416122437\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.01606857031583786\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.1891323328018188\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.015724269673228264\n",
      "After scaling - encoder.proj.weight: grad norm 2.4603047370910645\n",
      "After scaling - encoder.proj.bias: grad norm 0.02571728080511093\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.774257242679596\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.020476410165429115\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.130028486251831\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.01722223311662674\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.139832019805908\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.01744828000664711\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.1508195400238037\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.035273972898721695\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.0787241458892822\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.023967944085597992\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02049097791314125\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00035290399682708085\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02569321170449257\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003597377217374742\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019081907346844673\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003100727335549891\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02294650301337242\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003034287947230041\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04747611656785011\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0004962623934261501\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014940723776817322\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0003951301332563162\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.041102830320596695\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003323347482364625\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04129200801253319\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00033669674303382635\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04150403290987015\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006806764286011457\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.020815972238779068\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00046250573359429836\n",
      "epoch: 173 ( 1 ) recon_loss: 0.25642144680023193  perplexity:  215.33517456054688  commit_loss:  0.1456848382949829 \n",
      "\t codebook loss:  0.5827393531799316  total_loss:  0.8680192232131958 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8957, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.1554574966430664\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.0223739892244339\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.4123064279556274\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.02158624492585659\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.1187537908554077\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.019177649170160294\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.4470771551132202\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.020937414839863777\n",
      "After scaling - encoder.proj.weight: grad norm 2.981761932373047\n",
      "After scaling - encoder.proj.bias: grad norm 0.035963211208581924\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.7942454218864441\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.023139378055930138\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.1476807594299316\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.020388271659612656\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.1595613956451416\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.0200844407081604\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.302426338195801\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04025236889719963\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.151532530784607\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.02791222184896469\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02039162814617157\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003948583616875112\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.024924522265791893\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00038095618947409093\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01974387839436531\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00033844905556179583\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.025538161396980286\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003695055784191936\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05262243002653122\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006346823065541685\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014016922563314438\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00040836603147909045\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.037902481853961945\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00035981423570774496\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.038112156093120575\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003544522332958877\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04063344746828079\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007103778189048171\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.02032236009836197\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004925976390950382\n",
      "Are there any dead codes on this epoch?  128\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8527, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.29664146900177\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.019583623856306076\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.668289303779602\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.020539114251732826\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.2051405906677246\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.01770344190299511\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.4919527769088745\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.017771251499652863\n",
      "After scaling - encoder.proj.weight: grad norm 3.025285243988037\n",
      "After scaling - encoder.proj.bias: grad norm 0.02936718426644802\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.900870144367218\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.021103207021951675\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.362905979156494\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.017143724486231804\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.285836935043335\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.017684731632471085\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.1498427391052246\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.03425891697406769\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.119766354560852\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.024752574041485786\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02198142744600773\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003319930692669004\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.028281820937991142\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003481911262497306\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.020430251955986023\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003001191362272948\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02529245987534523\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00030126867932267487\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05128641426563263\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0004978497163392603\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.015272079035639763\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0003577539755497128\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04005736857652664\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0002906305016949773\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03875085338950157\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0002998019626829773\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03644539788365364\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005807772977277637\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018982937559485435\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00041962016257457435\n",
      "epoch: 174 ( 1 ) recon_loss: 0.26470085978507996  perplexity:  216.4637908935547  commit_loss:  0.14006105065345764 \n",
      "\t codebook loss:  0.5602442026138306  total_loss:  0.85267174243927 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8880, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.1976161003112793\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.020251745358109474\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.4799127578735352\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.01976321078836918\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.132741928100586\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.01751486398279667\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.4448221921920776\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.01890004426240921\n",
      "After scaling - encoder.proj.weight: grad norm 2.9486355781555176\n",
      "After scaling - encoder.proj.bias: grad norm 0.031421538442373276\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.7731075286865234\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.020344574004411697\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.069169044494629\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.01742575876414776\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.0039279460906982\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.017109937965869904\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.232883930206299\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.03663644567131996\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.1923049688339233\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.025897908955812454\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.021510783582925797\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00036374840419739485\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.026581211015582085\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.000354973686626181\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.02034555748105049\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003145903756376356\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02595093660056591\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00033947007614187896\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05296143516898155\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005643728072755039\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.0138860447332263\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0003654157044366002\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03716503828763962\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00031298992689698935\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.035993222147226334\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003073173575103283\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04010557755827904\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006580394692718983\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.021415390074253082\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004651610797736794\n",
      "Are there any dead codes on this epoch?  125\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8478, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.053993821144104\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.01933213695883751\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.3227379322052002\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.019703509286046028\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.0065604448318481\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.01681896485388279\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.2593791484832764\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.017464950680732727\n",
      "After scaling - encoder.proj.weight: grad norm 2.4359946250915527\n",
      "After scaling - encoder.proj.bias: grad norm 0.02880774438381195\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.6628288626670837\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.018667304888367653\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 1.808267593383789\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.014936445280909538\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 1.9110674858093262\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.015675794333219528\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 1.942762017250061\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.03135227411985397\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 0.9717795252799988\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.022281717509031296\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.021752266213297844\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003989755641669035\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.027298592031002045\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00040663991239853203\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.02077334001660347\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003471088712103665\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02599099837243557\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00036044069565832615\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.050273917615413666\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005945326411165297\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013679424300789833\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00038525479612872005\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03731892630457878\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003082575276494026\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03944050520658493\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00032351617119275033\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.040094614028930664\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006470465450547636\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.020055532455444336\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004598488158080727\n",
      "epoch: 175 ( 1 ) recon_loss: 0.24303211271762848  perplexity:  215.3461456298828  commit_loss:  0.14405788481235504 \n",
      "\t codebook loss:  0.5762315392494202  total_loss:  0.8477635383605957 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.9064, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.132285475730896\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.02117384597659111\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.3857219219207764\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.020795613527297974\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.0769758224487305\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.018010031431913376\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.3862215280532837\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.01944313757121563\n",
      "After scaling - encoder.proj.weight: grad norm 2.704803943634033\n",
      "After scaling - encoder.proj.bias: grad norm 0.03160515055060387\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.7410078644752502\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.020607270300388336\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 1.973510503768921\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.017138253897428513\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 1.9597117900848389\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.01716563291847706\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.27677321434021\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.037900540977716446\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.1904735565185547\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.026934785768389702\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.021213095635175705\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00039668692625127733\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.025961166247725487\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003896008129231632\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.020176881924271584\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00033741359948180616\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.025970526039600372\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003642624942585826\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05067384988069534\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.000592114869505167\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013882603496313095\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0003860722354147583\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.036973245441913605\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003210810536984354\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03671472519636154\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003215939796064049\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04265480116009712\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007100575021468103\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.022303234785795212\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005046167061664164\n",
      "Are there any dead codes on this epoch?  126\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8312, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 0.890094518661499\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.018411818891763687\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.0870431661605835\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.018883906304836273\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 0.8539445400238037\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.01633940264582634\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.057449460029602\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.016876041889190674\n",
      "After scaling - encoder.proj.weight: grad norm 2.0056309700012207\n",
      "After scaling - encoder.proj.bias: grad norm 0.02802353911101818\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.5441309809684753\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.01842782087624073\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 1.573645830154419\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.015372589230537415\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 1.739936351776123\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.015993310138583183\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 1.8061167001724243\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.03041427955031395\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 0.8773267269134521\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.021929960697889328\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02116888202726841\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004378833982627839\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.025852859020233154\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004491109575610608\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.02030913718044758\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003885957121383399\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.025149041786789894\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00040135844028554857\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04769938811659813\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006664764368906617\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.012940923683345318\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004382639890536666\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.037425607442855835\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003656022308859974\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.041380446404218674\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003803646832238883\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.042954396456480026\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.000723334786016494\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.02086522988975048\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005215544952079654\n",
      "epoch: 176 ( 1 ) recon_loss: 0.24082493782043457  perplexity:  216.8971405029297  commit_loss:  0.14063780009746552 \n",
      "\t codebook loss:  0.5625512003898621  total_loss:  0.8311818838119507 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8937, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.4397454261779785\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.02955280989408493\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.7723510265350342\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.029616527259349823\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.398760437965393\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.025497687980532646\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.6549197435379028\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.027124255895614624\n",
      "After scaling - encoder.proj.weight: grad norm 3.0003323554992676\n",
      "After scaling - encoder.proj.bias: grad norm 0.04493044316768646\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.8243317008018494\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.027840357273817062\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.305351972579956\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.022869830951094627\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.5283801555633545\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.02399655431509018\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.6186187267303467\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04672985151410103\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.340998649597168\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03299503028392792\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.022774619981646538\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004674812371376902\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02803594060242176\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004684891609940678\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.022126298397779465\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00040333528886549175\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.026178354397416115\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00042906514136120677\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04746077209711075\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.000710732361767441\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013039695098996162\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004403928469400853\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03646722063422203\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00036176652065478265\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03999519348144531\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00037958959001116455\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.041422631591558456\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007391963736154139\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.021212592720985413\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.000521932088304311\n",
      "Are there any dead codes on this epoch?  130\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8475, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 0.9063864350318909\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.02108100615441799\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.0285059213638306\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.02078153006732464\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 0.812070906162262\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.018036114051938057\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 0.9914499521255493\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.01877581886947155\n",
      "After scaling - encoder.proj.weight: grad norm 2.009732246398926\n",
      "After scaling - encoder.proj.bias: grad norm 0.03155498579144478\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.5916914343833923\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.021366287022829056\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 1.72176992893219\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.01839972659945488\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 1.861484408378601\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.018734069541096687\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.0126872062683105\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.036940932273864746\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 0.9951757192611694\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.026326093822717667\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.020585034042596817\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004787728830706328\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.023358503356575966\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00047197146341204643\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.018443025648593903\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0004096200573258102\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.022516921162605286\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004264195158611983\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04564323276281357\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007166485884226859\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013437964022159576\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00048525197780691087\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03910329192876816\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00041787809459492564\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04227636009454727\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00042547137127257884\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04571034386754036\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0008389692520722747\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.02260153740644455\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005978945991955698\n",
      "epoch: 177 ( 1 ) recon_loss: 0.24623210728168488  perplexity:  216.0218505859375  commit_loss:  0.1432431936264038 \n",
      "\t codebook loss:  0.5729727745056152  total_loss:  0.8475080728530884 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8646, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.281034231185913\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.026038015261292458\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.6204134225845337\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.02714722417294979\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.2459372282028198\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.023300638422369957\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.4630918502807617\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.024343861266970634\n",
      "After scaling - encoder.proj.weight: grad norm 2.7250609397888184\n",
      "After scaling - encoder.proj.bias: grad norm 0.041098978370428085\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.7505940794944763\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.02759586088359356\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.155904769897461\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.022428203374147415\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.3358981609344482\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.023585308343172073\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.3642055988311768\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04514491930603981\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.1269853115081787\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03167033940553665\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.022326290607452393\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00045379920629784465\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.028241103515028954\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004731308436021209\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.021714607253670692\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00040609127609059215\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02549925073981285\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00042427293374203146\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04749326780438423\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007162866531871259\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013081604614853859\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004809498495887965\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03757382929325104\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003908861835952848\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04071081057190895\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00041105260606855154\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04120416194200516\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007868006941862404\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01964147388935089\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005519612459465861\n",
      "Are there any dead codes on this epoch?  127\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8611, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.2257122993469238\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.026174044236540794\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.4222824573516846\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.025124311447143555\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.1013293266296387\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02235165424644947\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.3845330476760864\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.023744815960526466\n",
      "After scaling - encoder.proj.weight: grad norm 2.8213303089141846\n",
      "After scaling - encoder.proj.bias: grad norm 0.040901634842157364\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.8310979008674622\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.027858905494213104\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.3477046489715576\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.024742530658841133\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.4700303077697754\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.024833418428897858\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.573981523513794\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04868833348155022\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.2522895336151123\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03458140790462494\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.020682375878095627\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004416545561980456\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.023999255150556564\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004239415575284511\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.018583567813038826\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003771564515773207\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.023362277075648308\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00040066431392915547\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.0476064532995224\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006901643355377018\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014023746363818645\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004700845165643841\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03961460664868355\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004174995410721749\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04167869687080383\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00041903313831426203\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.043432749807834625\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0008215552661567926\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.02113083377480507\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005835184128955007\n",
      "epoch: 178 ( 1 ) recon_loss: 0.27147725224494934  perplexity:  215.0410919189453  commit_loss:  0.1404644101858139 \n",
      "\t codebook loss:  0.5618576407432556  total_loss:  0.8610717058181763 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8719, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.313818335533142\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.015795636922121048\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.7417380809783936\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.017507508397102356\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.1851816177368164\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.015444077551364899\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.3788132667541504\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.015984272584319115\n",
      "After scaling - encoder.proj.weight: grad norm 2.8355154991149902\n",
      "After scaling - encoder.proj.bias: grad norm 0.02753271348774433\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.7752136588096619\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.019382894039154053\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.199310302734375\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.01585877314209938\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.1458284854888916\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.01619494892656803\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.038254976272583\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.02886468917131424\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.0837215185165405\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.0194581039249897\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02341589145362377\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.000281522108707577\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.031042607501149178\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00031203238177113235\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.02112322486937046\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0002752563450485468\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.0245742816478014\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00028488412499427795\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.050536755472421646\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0004907093825750053\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01381646003574133\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0003454569960013032\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03919781371951103\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00028264737920835614\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.038244619965553284\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.000288638926576823\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.036327362060546875\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005144488532096148\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.019314926117658615\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.000346797431120649\n",
      "Are there any dead codes on this epoch?  129\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8717, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.5432525873184204\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.027424754574894905\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.9297276735305786\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.025598742067813873\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.4241851568222046\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.023065928369760513\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.7513697147369385\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.02546502649784088\n",
      "After scaling - encoder.proj.weight: grad norm 3.3974976539611816\n",
      "After scaling - encoder.proj.bias: grad norm 0.04489874839782715\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.9294871091842651\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.02767060697078705\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.6014764308929443\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.024615919217467308\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.8347339630126953\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.025181852281093597\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.738050937652588\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04685763269662857\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.3622691631317139\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03349616006016731\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.022327546030282974\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003967772063333541\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02791900932788849\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003703587572090328\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.020604897290468216\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003337143862154335\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02533855475485325\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00036842416739091277\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.049154482781887054\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006495882989838719\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013447679579257965\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004003341600764543\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03763776645064354\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.000356139411451295\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04101250693202019\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00036432722117751837\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03961370885372162\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006779292016290128\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.019709104672074318\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004846174269914627\n",
      "epoch: 179 ( 1 ) recon_loss: 0.2732887864112854  perplexity:  215.4554443359375  commit_loss:  0.14257633686065674 \n",
      "\t codebook loss:  0.570305347442627  total_loss:  0.8717305660247803 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8757, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.6021208763122559\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.018195610493421555\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.0653750896453857\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.019629593938589096\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.3914285898208618\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.017717737704515457\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.6411335468292236\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.018233226612210274\n",
      "After scaling - encoder.proj.weight: grad norm 3.3794660568237305\n",
      "After scaling - encoder.proj.bias: grad norm 0.031072396785020828\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.922581136226654\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.022196365520358086\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.59503173828125\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.018449198454618454\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.5526123046875\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.018691129982471466\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.5628349781036377\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.03706018626689911\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.3930518627166748\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.025864511728286743\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.023751888424158096\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0002697549934964627\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.030619759112596512\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00029101420659571886\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.020628318190574646\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0002626703935675323\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02433026395738125\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0002703126519918442\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05010152608156204\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.00046065697097219527\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013677522540092468\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0003290673193987459\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.038472067564725876\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00027351450989954174\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.037843190133571625\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0002771012077573687\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03799474239349365\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005494275828823447\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.02065238356590271\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0003834485833067447\n",
      "Are there any dead codes on this epoch?  128\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8347, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.396557092666626\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.023714203387498856\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.7385600805282593\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.021420201286673546\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.285550832748413\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.01940164715051651\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.5299359560012817\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.022134019061923027\n",
      "After scaling - encoder.proj.weight: grad norm 2.8709166049957275\n",
      "After scaling - encoder.proj.bias: grad norm 0.03921011835336685\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.7352643013000488\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.02379990555346012\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.11057710647583\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.021072007715702057\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.378899097442627\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.02190229669213295\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.2429816722869873\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.039318718016147614\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.1555103063583374\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.027377638965845108\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.023868003860116005\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004052900767419487\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.029713040217757225\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003660841903183609\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.0219708401709795\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003315858484711498\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.026147527620196342\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003782837593462318\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04906569793820381\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006701246020384133\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01256611105054617\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004067547561135143\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.036071039736270905\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003601333301048726\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04065682366490364\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003743234556168318\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03833391144871712\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006719806115143001\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.019748367369174957\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00046790039050392807\n",
      "epoch: 180 ( 1 ) recon_loss: 0.24741601943969727  perplexity:  217.6658935546875  commit_loss:  0.13991139829158783 \n",
      "\t codebook loss:  0.5596455931663513  total_loss:  0.8346551656723022 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8592, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.281106948852539\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.017801789566874504\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.596134901046753\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.018251998350024223\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.0781968832015991\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.01593034341931343\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.2713310718536377\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.016258331015706062\n",
      "After scaling - encoder.proj.weight: grad norm 2.548940896987915\n",
      "After scaling - encoder.proj.bias: grad norm 0.02700386382639408\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.7324399948120117\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.020404960960149765\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.110787868499756\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.01750325970351696\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.146364212036133\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.01715797372162342\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.274651050567627\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.03505774214863777\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.211747407913208\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.024608714506030083\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.023503314703702927\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003265933773946017\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02928284741938114\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003348529862705618\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019780706614255905\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0002922596759162843\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.023323966190218925\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0002982769801747054\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04676312208175659\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.000495415588375181\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013437417335808277\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0003743514243979007\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03872472047805786\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003211165312677622\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03937740996479988\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003147818788420409\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04173097386956215\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006431727670133114\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.022230837494134903\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00045147392665967345\n",
      "Are there any dead codes on this epoch?  125\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8452, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.6028012037277222\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.026032308116555214\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.00449538230896\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.023547077551484108\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.4498162269592285\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.021217135712504387\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.7288429737091064\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.024284185841679573\n",
      "After scaling - encoder.proj.weight: grad norm 3.2238705158233643\n",
      "After scaling - encoder.proj.bias: grad norm 0.04255717992782593\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.8519262671470642\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.02525610290467739\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.3883650302886963\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.02179005928337574\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.6315016746520996\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.023115092888474464\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.464017391204834\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04221467301249504\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.2610925436019897\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.0296563059091568\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02441520057618618\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00039654571446590126\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.030534137040376663\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003586886450648308\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.022084804251790047\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003231970185879618\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02633517049252987\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00036991690285503864\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04910866916179657\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006482662283815444\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01297724898904562\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0003847219340968877\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.0363815613090992\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00033192429691553116\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04008521884679794\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003521082689985633\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.0375339575111866\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006430489593185484\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.019210008904337883\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004517494817264378\n",
      "epoch: 181 ( 1 ) recon_loss: 0.2597299814224243  perplexity:  215.10702514648438  commit_loss:  0.1394932121038437 \n",
      "\t codebook loss:  0.5579728484153748  total_loss:  0.8451970815658569 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8596, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.3058936595916748\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.018279021605849266\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.61903715133667\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.01843871735036373\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.0748807191848755\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.015887971967458725\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.2329087257385254\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.016054421663284302\n",
      "After scaling - encoder.proj.weight: grad norm 2.5711309909820557\n",
      "After scaling - encoder.proj.bias: grad norm 0.027342794463038445\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.7283759713172913\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.019827472046017647\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.1049387454986572\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.01727614551782608\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.151271343231201\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.01746259815990925\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.3626065254211426\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.03721557557582855\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.2648926973342896\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.024878082796931267\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.023689212277531624\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003315856447443366\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02936970628798008\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00033448258182033896\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019498582929372787\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00028821147861890495\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.022365249693393707\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0002912309137172997\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04664091020822525\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0004960045916959643\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013212907128036022\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00035967491567134857\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03818407654762268\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003133932768832892\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.039024557918310165\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00031677554943598807\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04285822808742523\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.000675099203363061\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.022945445030927658\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004512941522989422\n",
      "Are there any dead codes on this epoch?  129\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8460, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.6588807106018066\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.027991773560643196\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.043346881866455\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.02590770646929741\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.5307103395462036\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.023081159219145775\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.8220875263214111\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.026031600311398506\n",
      "After scaling - encoder.proj.weight: grad norm 3.336421251296997\n",
      "After scaling - encoder.proj.bias: grad norm 0.04497024789452553\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.8479219675064087\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.026164868846535683\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.3779187202453613\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.02195686660706997\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.66109561920166\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.023123444989323616\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.5387449264526367\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04276854172348976\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.3282374143600464\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03033326007425785\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02462119236588478\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00041545534622855484\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.030327457934617996\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00038452353328466415\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.02271888218820095\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.000342571729561314\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02704351581633091\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00038636234239675105\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.049519333988428116\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006674507167190313\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.012584902346134186\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0003883402969222516\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03529319167137146\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003258849319536239\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03949611634016037\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00034319935366511345\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03768018260598183\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006347728194668889\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.019713768735527992\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00045020782272331417\n",
      "epoch: 182 ( 1 ) recon_loss: 0.252805233001709  perplexity:  216.8280487060547  commit_loss:  0.14134494960308075 \n",
      "\t codebook loss:  0.565379798412323  total_loss:  0.8460273146629333 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8612, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.5624083280563354\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.018949197605252266\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.9156932830810547\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.018707500770688057\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.2580375671386719\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.01621437259018421\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.414644479751587\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.015846891328692436\n",
      "After scaling - encoder.proj.weight: grad norm 2.9783620834350586\n",
      "After scaling - encoder.proj.bias: grad norm 0.026381270959973335\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.8650875091552734\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.020109958946704865\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.425736904144287\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.017212603241205215\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.4587039947509766\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.017444683238863945\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.6887218952178955\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.03772113099694252\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.4879812002182007\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.025142431259155273\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.024493001401424408\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0002970559580717236\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.030031250789761543\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0002932670176960528\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.01972155272960663\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0002541836292948574\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.0221765898168087\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0002484228753019124\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.046690117567777634\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0004135644412599504\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013561492785811424\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00031525257509201765\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.038026921451091766\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0002698323514778167\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03854372724890709\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00027347056311555207\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04214958846569061\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005913330824114382\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.023326249793171883\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00039414383354596794\n",
      "Are there any dead codes on this epoch?  126\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8351, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.4971649646759033\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.026498625054955482\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.8740694522857666\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.025928804650902748\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.415507197380066\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02292991615831852\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.6549397706985474\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.025639399886131287\n",
      "After scaling - encoder.proj.weight: grad norm 3.0658602714538574\n",
      "After scaling - encoder.proj.bias: grad norm 0.043963536620140076\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.7940967082977295\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.026617640629410744\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.2262141704559326\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.021558351814746857\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.4284422397613525\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.022676993161439896\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.314211845397949\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04226433113217354\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.1756044626235962\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.029612140730023384\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02424704097211361\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004291532386559993\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.030351120978593826\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004199248505756259\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.02292456664144993\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00037135693128220737\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.026802251115441322\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004152378533035517\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04965253174304962\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007120028021745384\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.012860637158155441\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00043108072713948786\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03605421259999275\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00034914404386654496\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03932936117053032\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003672608290798962\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03747935965657234\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006844836170785129\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.019039271399378777\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00047957757487893105\n",
      "epoch: 183 ( 1 ) recon_loss: 0.24965214729309082  perplexity:  215.88613891601562  commit_loss:  0.139488086104393 \n",
      "\t codebook loss:  0.557952344417572  total_loss:  0.8350644111633301 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8519, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.2274383306503296\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.016510343179106712\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.4834340810775757\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.016067631542682648\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 0.9666067361831665\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.01373992394655943\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.0813452005386353\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.013437079265713692\n",
      "After scaling - encoder.proj.weight: grad norm 2.2576286792755127\n",
      "After scaling - encoder.proj.bias: grad norm 0.021663088351488113\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.6956769824028015\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.016916006803512573\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 1.969077467918396\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.014390423893928528\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 1.9535675048828125\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.014431566931307316\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.2021145820617676\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.031288474798202515\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.2264599800109863\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.021323595196008682\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.024334406480193138\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003273235051892698\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.029409615322947502\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00031854657572694123\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019163325428962708\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00027239896007813513\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.021438058465719223\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0002663949562702328\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04475829750299454\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.00042947850306518376\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013792046345770359\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00033536594128236175\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03903766721487045\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00028529533301480114\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03873017802834511\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.000286111026071012\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04365771263837814\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006203052471391857\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.024315010756254196\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004227479512337595\n",
      "Are there any dead codes on this epoch?  124\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8377, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.4457708597183228\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.026753343641757965\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.79671049118042\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.026817161589860916\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.384366512298584\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.023548156023025513\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.6321280002593994\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.025665616616606712\n",
      "After scaling - encoder.proj.weight: grad norm 3.059934616088867\n",
      "After scaling - encoder.proj.bias: grad norm 0.04365438222885132\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.8288575410842896\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.027251573279500008\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.3231568336486816\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.021751662716269493\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.4680347442626953\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.02269640564918518\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.3245491981506348\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04225430637598038\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.1762655973434448\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.02889314852654934\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.023381853476166725\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004326707567088306\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02905745431780815\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00043370286584831774\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.02238878607749939\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003808345936704427\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.026395728811621666\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00041507938294671476\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04948705434799194\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007060042698867619\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013404769822955132\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004407284432090819\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03757145255804062\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00035178064717911184\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03991449996829033\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003670595760922879\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03759397193789482\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006833613733761013\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.019023258239030838\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004672769282478839\n",
      "epoch: 184 ( 1 ) recon_loss: 0.24617348611354828  perplexity:  218.67645263671875  commit_loss:  0.1409534215927124 \n",
      "\t codebook loss:  0.5638136863708496  total_loss:  0.8377188444137573 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8433, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.3731131553649902\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.01693982258439064\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.6994308233261108\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.017033077776432037\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.109910011291504\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.014288399368524551\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.1991935968399048\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.013066110201179981\n",
      "After scaling - encoder.proj.weight: grad norm 2.6015560626983643\n",
      "After scaling - encoder.proj.bias: grad norm 0.021079029887914658\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.8125609755516052\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.018146680667996407\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.2778685092926025\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.015453092753887177\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.22711181640625\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.015107570216059685\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.3030004501342773\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.030023930594325066\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.310675859451294\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.021291404962539673\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02425609901547432\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00029924261616542935\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.030020512640476227\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00030088998028077185\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.019606605172157288\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00025240512331947684\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02118380181491375\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00023081336985342205\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04595658928155899\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0003723618865478784\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014353921636939049\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0003205618413630873\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.0402386374771595\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0002729794941842556\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03934201970696449\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0002668758388608694\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.0406825989484787\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005303739453665912\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.023153139278292656\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00037611351581290364\n",
      "Are there any dead codes on this epoch?  123\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8361, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.6737357378005981\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.02573327161371708\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.166471004486084\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.026997199282050133\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.593590259552002\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02412804216146469\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.8246114253997803\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.026381168514490128\n",
      "After scaling - encoder.proj.weight: grad norm 3.445476770401001\n",
      "After scaling - encoder.proj.bias: grad norm 0.04531467705965042\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.8697624206542969\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.02736748568713665\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.467405080795288\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.021396243944764137\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.6053574085235596\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.0229326281696558\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.366542100906372\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04139810428023338\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.2359026670455933\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.028784461319446564\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02467668242752552\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003793979121837765\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.03194131702184677\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0003980326000601053\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.023495059460401535\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00035573123022913933\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.026901116594672203\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00038895016768947244\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05079830810427666\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0006680958904325962\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.012823322787880898\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00040349189657717943\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.036378130316734314\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003154550213366747\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.038412027060985565\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003381066781003028\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03489105775952339\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006103520281612873\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018221501260995865\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0004243830335326493\n",
      "epoch: 185 ( 1 ) recon_loss: 0.2529923617839813  perplexity:  218.19752502441406  commit_loss:  0.13895457983016968 \n",
      "\t codebook loss:  0.5558183193206787  total_loss:  0.8361327648162842 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8496, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.3155696392059326\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.014805121347308159\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.733720064163208\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.015691904351115227\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.1533582210540771\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.013369781896471977\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.2684366703033447\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.012532471679151058\n",
      "After scaling - encoder.proj.weight: grad norm 2.7388808727264404\n",
      "After scaling - encoder.proj.bias: grad norm 0.020710866898298264\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.7895029783248901\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.016683286055922508\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.2449374198913574\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.014605255797505379\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.2108757495880127\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.015198377892374992\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.1643261909484863\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.02881941944360733\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.1965445280075073\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.019769689068198204\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.023311862722039223\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0002623463806230575\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.030721478164196014\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00027806012076325715\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.02043747715651989\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00023691219394095242\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.022476665675640106\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00022207506117410958\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04853289946913719\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.00036699604243040085\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01398997288197279\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0002956273965537548\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03978023678064346\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.000258804764598608\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.039176661521196365\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0002693148562684655\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03835180401802063\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005106793832965195\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.02120273932814598\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0003503184125293046\n",
      "Are there any dead codes on this epoch?  118\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8604, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.8696171045303345\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.02973821759223938\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.3711369037628174\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.03071608766913414\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.7549399137496948\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02683115564286709\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.9893782138824463\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.02857033908367157\n",
      "After scaling - encoder.proj.weight: grad norm 3.802793025970459\n",
      "After scaling - encoder.proj.bias: grad norm 0.04944871366024017\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.9640616178512573\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.029773512855172157\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.7934629917144775\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.023312320932745934\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.9381964206695557\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.024733182042837143\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.6640350818634033\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04313454031944275\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.4271929264068604\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.030209388583898544\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02473616786301136\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.000393454625736922\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.0313715785741806\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004063924425281584\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.023218918591737747\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00035499242949299514\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.026320679113268852\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003780028782784939\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.050313256680965424\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.000654236413538456\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.012755119241774082\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0003939216258004308\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.0369592048227787\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00030843616696074605\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03887411579489708\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00032723500044085085\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.035246796905994415\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005706960801035166\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01888262666761875\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0003996885207016021\n",
      "epoch: 186 ( 1 ) recon_loss: 0.25843530893325806  perplexity:  217.25424194335938  commit_loss:  0.143439382314682 \n",
      "\t codebook loss:  0.573757529258728  total_loss:  0.8603799343109131 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8480, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.7963052988052368\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.014836976304650307\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.3826987743377686\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.018364185467362404\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.5831623077392578\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.015992673113942146\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.6797468662261963\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.015011507086455822\n",
      "After scaling - encoder.proj.weight: grad norm 3.683607816696167\n",
      "After scaling - encoder.proj.bias: grad norm 0.02514524571597576\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.9904146790504456\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.019670987501740456\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.8500726222991943\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.01702922396361828\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.7008914947509766\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.017372746020555496\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.255316734313965\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.02650156244635582\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.3170565366744995\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.017749644815921783\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02514731138944626\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00020770973060280085\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.03335651010274887\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0002570887445472181\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.022163424640893936\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0002238888555439189\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.023515556007623672\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00021015306992921978\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.051568541675806046\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.000352019997080788\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013865276239812374\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00027538329595699906\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.039899490773677826\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00023840002540964633\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.0378110371530056\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.000243209142354317\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.031573228538036346\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.00037100768531672657\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018438085913658142\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0002484855358488858\n",
      "Are there any dead codes on this epoch?  125\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8377, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.5830737352371216\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.028458183631300926\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.985609769821167\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.027828752994537354\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.492786169052124\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.024309707805514336\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.742240071296692\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.026694590225815773\n",
      "After scaling - encoder.proj.weight: grad norm 3.2383320331573486\n",
      "After scaling - encoder.proj.bias: grad norm 0.047008298337459564\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.8086755275726318\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.027435416355729103\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.330967426300049\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.02174617536365986\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.575237989425659\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.023339768871665\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.4291176795959473\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04233256354928017\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.2187635898590088\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.029940923675894737\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02433651126921177\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00043748621828854084\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.03052467666566372\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00042781000956892967\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.022948523983359337\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00037371189682744443\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02678336761891842\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.000410374574130401\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04978271201252937\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.000722656084690243\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.012431727722287178\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004217632522340864\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03583384305238724\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00033430286566726863\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03958900272846222\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003588010440580547\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.037342701107263565\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0006507763173431158\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018735989928245544\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00046028028009459376\n",
      "epoch: 187 ( 1 ) recon_loss: 0.25356829166412354  perplexity:  218.1097412109375  commit_loss:  0.13919706642627716 \n",
      "\t codebook loss:  0.5567882657051086  total_loss:  0.8376938104629517 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8437, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.693541169166565\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.015467294491827488\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.2219552993774414\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.01883053034543991\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.4644746780395508\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.01632716879248619\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.553424596786499\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.015572551637887955\n",
      "After scaling - encoder.proj.weight: grad norm 3.3373517990112305\n",
      "After scaling - encoder.proj.bias: grad norm 0.026375938206911087\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.957252562046051\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.021443409845232964\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.8608152866363525\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.018587006255984306\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.7062413692474365\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.019044360145926476\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.1917786598205566\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.026471905410289764\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.253443717956543\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.017904771491885185\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.024887731298804283\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0002273023419547826\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.03265313804149628\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00027672736905515194\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.021521445363759995\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0002399387740297243\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02282862365245819\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00022884918143972754\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.049044638872146606\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.00038761223549954593\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014067473821341991\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0003151254204567522\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04204161465167999\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0002731486165430397\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03977004438638687\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00027986973873339593\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.032209668308496475\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0003890225198119879\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018420202657580376\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00026312272530049086\n",
      "Are there any dead codes on this epoch?  123\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8392, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.4371109008789062\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03195767477154732\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.6975266933441162\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.029999680817127228\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.3670622110366821\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.025868117809295654\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.662055253982544\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.02852391079068184\n",
      "After scaling - encoder.proj.weight: grad norm 3.0012359619140625\n",
      "After scaling - encoder.proj.bias: grad norm 0.050156913697719574\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.7520050406455994\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.02976854331791401\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.158421516418457\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.024403482675552368\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.5087642669677734\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.02534913271665573\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.381848096847534\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04570639133453369\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.167977213859558\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03213042765855789\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.023577474057674408\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0005243027699179947\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.027849897742271423\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004921796498820186\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.022428244352340698\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00042439650860615075\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.027267947793006897\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004679679113905877\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04923876374959946\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0008228824590332806\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01233751606196165\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004883874789811671\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03541141375899315\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00040036748396232724\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.041159193962812424\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0004158819792792201\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03907698765397072\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007498664199374616\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.019162023440003395\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005271369591355324\n",
      "epoch: 188 ( 1 ) recon_loss: 0.2417675256729126  perplexity:  218.63433837890625  commit_loss:  0.14237801730632782 \n",
      "\t codebook loss:  0.5695120692253113  total_loss:  0.8392250537872314 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8429, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.6388447284698486\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.0170675590634346\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.0802061557769775\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.01950804702937603\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.3785192966461182\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.01627419702708721\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.4437661170959473\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.014959460124373436\n",
      "After scaling - encoder.proj.weight: grad norm 3.199549436569214\n",
      "After scaling - encoder.proj.bias: grad norm 0.025218501687049866\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.9160839915275574\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.02210930548608303\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.7860004901885986\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.019408972933888435\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.6059370040893555\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.01844816468656063\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.19211483001709\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.02533652074635029\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.2642747163772583\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.017537742853164673\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02498210035264492\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0002601731684990227\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.03171009197831154\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0002973753144033253\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.021013770252466202\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00024807939189486206\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.022008376196026802\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0002280379121657461\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04877305030822754\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0003844239399768412\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013964532874524593\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00033702817745506763\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.042469024658203125\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0002958650584332645\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.039724189788103104\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00028121875948272645\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03341599926352501\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.00038622296415269375\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01927225850522518\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00026734056882560253\n",
      "Are there any dead codes on this epoch?  125\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8310, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.3496333360671997\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03268541395664215\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.55194890499115\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.029788943007588387\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.2902384996414185\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02588232420384884\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.6043206453323364\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.02900347299873829\n",
      "After scaling - encoder.proj.weight: grad norm 2.8103225231170654\n",
      "After scaling - encoder.proj.bias: grad norm 0.05085325241088867\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.7348580360412598\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.031561024487018585\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.105604648590088\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.026247011497616768\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.524200439453125\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.02729019895195961\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.386195421218872\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04898281395435333\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.0819592475891113\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03383851423859596\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.022948333993554115\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0005557626718655229\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.026388386264443398\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0005065128789283335\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.021938422694802284\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00044008714030496776\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.027278881520032883\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004931571893393993\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.047784995287656784\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0008646774222142994\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.012495075352489948\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005366442492231727\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.035802409052848816\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004462880315259099\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04291995242238045\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00046402576845139265\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.040573399513959885\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0008328736294060946\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01839696802198887\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005753692821599543\n",
      "epoch: 189 ( 1 ) recon_loss: 0.24275699257850647  perplexity:  216.42446899414062  commit_loss:  0.1401851773262024 \n",
      "\t codebook loss:  0.5607407093048096  total_loss:  0.8309966325759888 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8332, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.4167746305465698\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.01709570363163948\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.741989016532898\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.01863890327513218\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.136418104171753\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.015529008582234383\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.1827634572982788\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.014407712034881115\n",
      "After scaling - encoder.proj.weight: grad norm 2.671556234359741\n",
      "After scaling - encoder.proj.bias: grad norm 0.0243048258125782\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.8047921657562256\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.021571535617113113\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.47782039642334\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.019132936373353004\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.2914934158325195\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.017873812466859818\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 1.9445744752883911\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.025494463741779327\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.1174172163009644\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.017656473442912102\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.025068242102861404\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00030248938128352165\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.030822549015283585\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00032979456591419876\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.020107649266719818\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00027476847753860056\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.020927678793668747\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00025492836721241474\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04727020487189293\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.00043004672625102103\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014239897951483727\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00038168422179296613\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04384225979447365\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003385359304957092\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04054541885852814\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00031625715200789273\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03440707176923752\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0004510960716288537\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.019771449267864227\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00031241157557815313\n",
      "Are there any dead codes on this epoch?  123\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8225, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.4012559652328491\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03386753797531128\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.600696325302124\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.03104100190103054\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.3414825201034546\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02692587859928608\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.6505481004714966\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.029727503657341003\n",
      "After scaling - encoder.proj.weight: grad norm 2.9384875297546387\n",
      "After scaling - encoder.proj.bias: grad norm 0.052011650055646896\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.7782042622566223\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03328128531575203\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.260104179382324\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.028086159378290176\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.652322292327881\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.02851724810898304\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.4709861278533936\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.05039796233177185\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.1597509384155273\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03496120870113373\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02276238426566124\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0005501536070369184\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.026002148166298866\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0005042385309934616\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.021791409701108932\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0004373913398012519\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.026811951771378517\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00048290169797837734\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04773359373211861\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0008448914741165936\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.012641362845897675\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005406303680501878\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03671375289559364\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004562392714433372\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.043085046112537384\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0004632420022971928\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04013937711715698\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0008186782943084836\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018839312717318535\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005679194582626224\n",
      "epoch: 190 ( 1 ) recon_loss: 0.24083060026168823  perplexity:  220.5349578857422  commit_loss:  0.13861635327339172 \n",
      "\t codebook loss:  0.5544654130935669  total_loss:  0.8224713802337646 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8267, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.504237174987793\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.017325416207313538\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.860534906387329\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.019143275916576385\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.22792649269104\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.016126228496432304\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.2762327194213867\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.015066812746226788\n",
      "After scaling - encoder.proj.weight: grad norm 2.866058111190796\n",
      "After scaling - encoder.proj.bias: grad norm 0.025068387389183044\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.8635892271995544\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.02232404053211212\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.6241769790649414\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.019644049927592278\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.415553331375122\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.01799343153834343\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.0331735610961914\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.02612975426018238\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.1960557699203491\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.017942367121577263\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.025032220408320427\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0002883146808017045\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.030961424112319946\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00031856592977419496\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.020434098318219185\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0002683588245417923\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.021237965673208237\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0002507289464119822\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04769447073340416\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0004171665641479194\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.014371109195053577\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00037149747367948294\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04366929829120636\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003268993750680238\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04019756615161896\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.000299431208986789\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03383432701230049\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.00043482889304868877\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01990373060107231\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0002985814353451133\n",
      "Are there any dead codes on this epoch?  122\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8365, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.636738657951355\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03457794338464737\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.9713106155395508\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.031612277030944824\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.557658076286316\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.027787866070866585\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.891717791557312\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.031289007514715195\n",
      "After scaling - encoder.proj.weight: grad norm 3.365769624710083\n",
      "After scaling - encoder.proj.bias: grad norm 0.05527735874056816\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.8480172753334045\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.033438216894865036\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.4670186042785645\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.028409214690327644\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.8669540882110596\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.029019121080636978\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.6538026332855225\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.05180701985955238\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.2531956434249878\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.0359317846596241\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02373582497239113\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0005014459602534771\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02858775481581688\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00045843818224966526\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.022589005529880524\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00040297696250490844\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.027433505281805992\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00045375016634352505\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04881006479263306\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0008016268839128315\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.012297863140702248\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004849177785217762\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03577646613121033\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.000411987624829635\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.041576288640499115\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00042083245352841914\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03848518431186676\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007513003656640649\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018173720687627792\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005210792296566069\n",
      "epoch: 191 ( 1 ) recon_loss: 0.25195276737213135  perplexity:  217.7681427001953  commit_loss:  0.13931725919246674 \n",
      "\t codebook loss:  0.5572690367698669  total_loss:  0.8365187644958496 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8275, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.4741219282150269\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.015558299608528614\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.8331466913223267\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.017561081796884537\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.2176536321640015\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.015570377930998802\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.2889734506607056\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.01536593958735466\n",
      "After scaling - encoder.proj.weight: grad norm 2.7880194187164307\n",
      "After scaling - encoder.proj.bias: grad norm 0.0261221993714571\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.7979313731193542\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.01957835629582405\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.384408473968506\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.016603264957666397\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.243394374847412\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.016183458268642426\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 1.9042776823043823\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.023292649537324905\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.1433151960372925\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.01620674692094326\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02576189488172531\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0002718983159866184\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.03203624486923218\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00030689913546666503\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.02127983048558235\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0002721094060689211\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.022526221349835396\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00026853661984205246\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04872369021177292\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0004565140698105097\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013944723643362522\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00034215321647934616\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.04167015105485916\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0002901602128986269\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.039205774664878845\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00028282369021326303\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.033279336988925934\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.00040706456638872623\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01998068392276764\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00028323064907453954\n",
      "Are there any dead codes on this epoch?  121\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8225, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.5497499704360962\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.03430911526083946\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.8584703207015991\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.031394243240356445\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.4921174049377441\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02740938775241375\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.8022114038467407\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.03072286583483219\n",
      "After scaling - encoder.proj.weight: grad norm 3.236210584640503\n",
      "After scaling - encoder.proj.bias: grad norm 0.05437817424535751\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.8107394576072693\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.03249247744679451\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.3553943634033203\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.02783479355275631\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.745091676712036\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.028441976755857468\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.547422409057617\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.050991374999284744\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.2419168949127197\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03557583689689636\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.023473292589187622\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0005196631536819041\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.028149327263236046\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00047551299212500453\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.022600360214710236\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0004151563625782728\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02729720063507557\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00046534399734809995\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.049017272889614105\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0008236392168328166\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.012279868125915527\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004921474028378725\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.035675983875989914\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0004215998051222414\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.041578538715839386\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0004307964991312474\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03858454152941704\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007723409798927605\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01881069876253605\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005388495046645403\n",
      "epoch: 192 ( 1 ) recon_loss: 0.24247854948043823  perplexity:  219.63169860839844  commit_loss:  0.13823409378528595 \n",
      "\t codebook loss:  0.5529363751411438  total_loss:  0.822484016418457 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8262, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.8897604942321777\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.018804682418704033\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.378053903579712\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.02018597535789013\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.6182777881622314\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.018225880339741707\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.7191733121871948\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.0190946776419878\n",
      "After scaling - encoder.proj.weight: grad norm 3.596358060836792\n",
      "After scaling - encoder.proj.bias: grad norm 0.03298504278063774\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.9809569716453552\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.020635558292269707\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.8284759521484375\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.017012782394886017\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.7360379695892334\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.017626192420721054\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.349261522293091\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.027270546182990074\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.3859843015670776\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.019247664138674736\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.026300596073269844\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00026171276113018394\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.03309638425707817\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00028093677246943116\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.02252225950360298\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00025365728652104735\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.023926464840769768\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.000265748705714941\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.050052039325237274\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0004590668249875307\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013652393594384193\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00028719380497932434\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.039365094155073166\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00023677409626543522\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03807859867811203\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00024531118106096983\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.032695669680833817\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0003795357479248196\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.019289329648017883\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00026787788374349475\n",
      "Are there any dead codes on this epoch?  126\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8168, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.408704400062561\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.029918015003204346\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.7130780220031738\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.027080023661255836\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.3548036813735962\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.023744048550724983\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.6458247900009155\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.02695523574948311\n",
      "After scaling - encoder.proj.weight: grad norm 2.916485548019409\n",
      "After scaling - encoder.proj.bias: grad norm 0.04767382889986038\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.7246163487434387\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.027521025389432907\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.063774347305298\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.023558350279927254\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.4006166458129883\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.024249443784356117\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.2630670070648193\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04406756907701492\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.0733097791671753\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.030879052355885506\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.023879824206233025\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0005071588675491512\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.029039448127150536\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004590502649080008\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.022966118529438972\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00040250009624287486\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.027899397537112236\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00045693491119891405\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04943915456533432\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0008081487030722201\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01228342205286026\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004665259621106088\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.034984320402145386\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003993521968368441\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04069434106349945\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00041106739081442356\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03836265206336975\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.000747016747482121\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01819434203207493\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005234499694779515\n",
      "epoch: 193 ( 1 ) recon_loss: 0.23977430164813995  perplexity:  219.05174255371094  commit_loss:  0.13753770291805267 \n",
      "\t codebook loss:  0.5501508116722107  total_loss:  0.8168424367904663 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8152, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.4817390441894531\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.014771088026463985\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.87751042842865\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.015888039022684097\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.2741223573684692\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.014609106816351414\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.4054731130599976\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.01582043431699276\n",
      "After scaling - encoder.proj.weight: grad norm 2.890371322631836\n",
      "After scaling - encoder.proj.bias: grad norm 0.02758796140551567\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.783901035785675\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.017252866178750992\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.2830731868743896\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.014682040549814701\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.2572574615478516\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.015233195386826992\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.0001232624053955\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.02409113198518753\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.129887580871582\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.016917195171117783\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.025458011776208878\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00025378461577929556\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.03225783258676529\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00027297515771351755\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.021890917792916298\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0002510016201995313\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.024147678166627884\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0002718136238399893\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04965996742248535\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0004739935393445194\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01346834097057581\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00029642449226230383\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.039225876331329346\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.000252254685619846\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.038782332092523575\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.000261724169831723\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03436446562409401\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.00041391392005607486\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.019412795081734657\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00029065721901133657\n",
      "Are there any dead codes on this epoch?  117\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8080, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.3814290761947632\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.02994515560567379\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.6460930109024048\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.026989683508872986\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.300587773323059\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02339504100382328\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.5812602043151855\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.026229673996567726\n",
      "After scaling - encoder.proj.weight: grad norm 2.8361589908599854\n",
      "After scaling - encoder.proj.bias: grad norm 0.04622051492333412\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.7184076905250549\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.02776113525032997\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.0440096855163574\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.023505616933107376\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.3648605346679688\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.024218467995524406\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.2564759254455566\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.043956756591796875\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.101454257965088\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03074888326227665\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.023883502930402756\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.000517721229698509\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.028459271416068077\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004666241293307394\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.02248583920300007\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0004044764209538698\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.027338378131389618\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00045348433195613325\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04903430491685867\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007991056190803647\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.012420536950230598\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004799617163371295\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03533884510397911\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00040638813516125083\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04088602960109711\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0004187126469332725\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03901217132806778\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007599674863740802\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.019043022766709328\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005316168535500765\n",
      "epoch: 194 ( 1 ) recon_loss: 0.23069120943546295  perplexity:  219.08831787109375  commit_loss:  0.13760586082935333 \n",
      "\t codebook loss:  0.5504234433174133  total_loss:  0.8080298900604248 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8052, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.5215212106704712\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.01544906571507454\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.962844967842102\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.015888603404164314\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.3589141368865967\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.014642194844782352\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.5269684791564941\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.01690448261797428\n",
      "After scaling - encoder.proj.weight: grad norm 3.130099296569824\n",
      "After scaling - encoder.proj.bias: grad norm 0.029447784647345543\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.8125457763671875\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.017378192394971848\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.325575113296509\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.014614460058510303\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.2968156337738037\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.015021240338683128\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 1.9755278825759888\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.023178813979029655\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.1248706579208374\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.0165620855987072\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.025129742920398712\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0002551598008722067\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.03241873160004616\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0002624192857183516\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.022444093599915504\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00024183336063288152\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.025219710543751717\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0002791977603919804\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05169733241200447\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0004863653739448637\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013420164585113525\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00028702159761451185\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.038409654051065445\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00024137528089340776\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03793465718626976\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0002480937400832772\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03262820467352867\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.00038282584864646196\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018578583374619484\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0002735426533035934\n",
      "Are there any dead codes on this epoch?  121\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8164, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.552154302597046\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.030973203480243683\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.8975200653076172\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.028002312406897545\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.44403874874115\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02448601834475994\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.7495070695877075\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.02807052806019783\n",
      "After scaling - encoder.proj.weight: grad norm 3.13444185256958\n",
      "After scaling - encoder.proj.bias: grad norm 0.04928211495280266\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.7958715558052063\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.029207389801740646\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.2456798553466797\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.02425200119614601\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.566289186477661\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.02543431520462036\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.4569716453552246\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.04708854854106903\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.1802594661712646\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.032864298671483994\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.024334676563739777\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00048559793503955007\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.029749322682619095\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00043902031029574573\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.022639641538262367\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00038389183464460075\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.027428774163126945\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004400897887535393\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.049141786992549896\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007726449985057116\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.012477675452828407\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00045791349839419127\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03520777449011803\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00038022291846573353\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.0402342863380909\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0003987592353951186\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03852040693163872\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007382543408311903\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018504111096262932\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005152465309947729\n",
      "epoch: 195 ( 1 ) recon_loss: 0.240767240524292  perplexity:  217.13967895507812  commit_loss:  0.13720545172691345 \n",
      "\t codebook loss:  0.5488218069076538  total_loss:  0.8164107799530029 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8151, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.4802100658416748\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.017704512923955917\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.8983210325241089\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.017203519120812416\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.3293519020080566\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.01586499623954296\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.5014255046844482\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.018504248932003975\n",
      "After scaling - encoder.proj.weight: grad norm 3.002626657485962\n",
      "After scaling - encoder.proj.bias: grad norm 0.03288251906633377\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.7819871306419373\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.01908467896282673\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.244851589202881\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.01600050739943981\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.2209694385528564\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.016739262267947197\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 1.926865577697754\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.0278252474963665\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.075231909751892\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.019693579524755478\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.025278285145759583\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0003023488388862461\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.03241857886314392\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0002937930985353887\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.022702008485794067\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00027093448443338275\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.025640593841671944\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003160062769893557\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05127735808491707\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005615512491203845\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013354385271668434\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00032591860508546233\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03833645209670067\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00027324867551214993\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03792860358953476\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00028586474945768714\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.032906047999858856\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0004751856904476881\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018362274393439293\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00033631708356551826\n",
      "Are there any dead codes on this epoch?  124\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8073, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.5812227725982666\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.032012227922677994\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.9073305130004883\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.028956549242138863\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.4711887836456299\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.025150170549750328\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.781277060508728\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.02848426252603531\n",
      "After scaling - encoder.proj.weight: grad norm 3.215203285217285\n",
      "After scaling - encoder.proj.bias: grad norm 0.049588967114686966\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.8140095472335815\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.029807794839143753\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.296578884124756\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.02474348060786724\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.5983128547668457\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.025550922378897667\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.4985878467559814\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.047354914247989655\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.247812032699585\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03266235068440437\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.024308564141392708\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004921326180920005\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.029321907088160515\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00044515685294754803\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.022616982460021973\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003866403712891042\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02738405205309391\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00043789626215584576\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04942818731069565\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007623445708304644\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01251398865133524\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004582432738970965\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.0353059284389019\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003803882282227278\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.039944566786289215\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00039280124474316835\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.0384114645421505\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007279998972080648\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.019182952120900154\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005021271063014865\n",
      "epoch: 196 ( 1 ) recon_loss: 0.23367717862129211  perplexity:  219.0009765625  commit_loss:  0.13672089576721191 \n",
      "\t codebook loss:  0.5468835830688477  total_loss:  0.8072726726531982 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8072, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.8762614727020264\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.023414036259055138\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.4044582843780518\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.02202783338725567\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.7017490863800049\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02005782723426819\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.9228906631469727\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.023356715217232704\n",
      "After scaling - encoder.proj.weight: grad norm 3.8273797035217285\n",
      "After scaling - encoder.proj.bias: grad norm 0.04114101082086563\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.9751812219619751\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.023597074672579765\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.7533962726593018\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.01953043043613434\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.7837204933166504\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.02078568935394287\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.4522714614868164\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.03576541692018509\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.3676708936691284\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.02501937747001648\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.025364968925714493\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00031653177575208247\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.03250560536980629\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00029779181932099164\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.023005757480859756\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00027115954435430467\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.025995345786213875\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0003157568571623415\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05174192413687706\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005561807774938643\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013183367438614368\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0003190062416251749\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.037222858518362045\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0002640297170728445\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03763280808925629\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0002809994330164045\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03315198794007301\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.00048350871657021344\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01848939247429371\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0003382342110853642\n",
      "Are there any dead codes on this epoch?  117\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.7930, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.304748296737671\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.025481121614575386\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.6038312911987305\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.023290088400244713\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.217685580253601\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.02021602727472782\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.4817330837249756\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.02284618653357029\n",
      "After scaling - encoder.proj.weight: grad norm 2.6273934841156006\n",
      "After scaling - encoder.proj.bias: grad norm 0.039381902664899826\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.6548361778259277\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.023277122527360916\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 1.8224655389785767\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.01910538040101528\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.0646989345550537\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.019675888121128082\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.0532922744750977\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.03797111287713051\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.0012156963348389\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.026405801996588707\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.024615760892629623\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.000480734248412773\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.03025834821164608\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00043939758324995637\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.022973209619522095\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00038140147808007896\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02795480750501156\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004310227814130485\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04956917092204094\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007429904653690755\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.012354329228401184\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004391529655549675\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03438316658139229\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00036044768057763577\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03895321115851402\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00037121103378012776\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03873801231384277\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007163741393014789\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018889226019382477\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.000498179520945996\n",
      "epoch: 197 ( 1 ) recon_loss: 0.22469735145568848  perplexity:  217.9711456298828  commit_loss:  0.13545703887939453 \n",
      "\t codebook loss:  0.5418281555175781  total_loss:  0.7929756045341492 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.8024, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.308008074760437\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.016050120815634727\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.6971527338027954\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.015596703626215458\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.1702544689178467\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.014254302717745304\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.3332797288894653\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.016724804416298866\n",
      "After scaling - encoder.proj.weight: grad norm 2.6830804347991943\n",
      "After scaling - encoder.proj.bias: grad norm 0.028990890830755234\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.689488410949707\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.017579542472958565\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 1.96928071975708\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.014820179902017117\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 1.9630022048950195\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.015493339858949184\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 1.7255027294158936\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.02645387127995491\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 0.9713324904441833\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.018245218321681023\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.02514071576297283\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00030849315226078033\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.03262031450867653\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0002997782139573246\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.02249300852417946\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00027397641679272056\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02562645636498928\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00032146097510121763\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.051570452749729156\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0005572226946242154\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01325238961726427\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00033788959262892604\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03785078227519989\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0002848529547918588\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.037730105221271515\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0002977914991788566\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03316522017121315\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0005084596341475844\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018669605255126953\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0003506842767819762\n",
      "Are there any dead codes on this epoch?  122\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.7953, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.3232132196426392\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.027880707755684853\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.5932080745697021\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.025420311838388443\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.243997573852539\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.021716320887207985\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.5646229982376099\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.024835985153913498\n",
      "After scaling - encoder.proj.weight: grad norm 2.7734787464141846\n",
      "After scaling - encoder.proj.bias: grad norm 0.04260445758700371\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.708970308303833\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.025318827480077744\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 1.9656754732131958\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.02080577239394188\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.221900701522827\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.02129538357257843\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.2468080520629883\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.041984863579273224\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.085837721824646\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.02888733334839344\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.023570260033011436\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0004966362030245364\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.0283796526491642\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00045280944323167205\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.02215920388698578\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003868306230287999\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02787047252058983\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00044240086572244763\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04940369352698326\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.000758908863645047\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.012628816068172455\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0004510016879066825\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.035014376044273376\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003706111165229231\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03957849368453026\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00037933248677290976\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04002216458320618\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007478720508515835\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01934191770851612\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005145671893842518\n",
      "epoch: 198 ( 1 ) recon_loss: 0.22624443471431732  perplexity:  219.50282287597656  commit_loss:  0.13565652072429657 \n",
      "\t codebook loss:  0.5426260828971863  total_loss:  0.7953447103500366 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.7977, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.6762659549713135\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.015827549621462822\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.192314386367798\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.016329118981957436\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.5026170015335083\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.015031919814646244\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.6581873893737793\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.0168155487626791\n",
      "After scaling - encoder.proj.weight: grad norm 3.428612232208252\n",
      "After scaling - encoder.proj.bias: grad norm 0.0289433766156435\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.8657715320587158\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.01663859374821186\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.4116435050964355\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.013679442927241325\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.3219237327575684\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.014493283815681934\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.0520823001861572\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.023538528010249138\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.2176759243011475\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.016364365816116333\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.025855189189314842\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00024412851780653\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.03381486237049103\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0002518648689147085\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.023176779970526695\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00023185649479273707\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.025576341897249222\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0002593676617834717\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05288386717438698\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.00044643061119131744\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013353899121284485\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00025663827545940876\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03719785436987877\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00021099549485370517\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03581399470567703\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00022354841348715127\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03165188431739807\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0003630647552199662\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018781770020723343\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0002524084993638098\n",
      "Are there any dead codes on this epoch?  118\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.7880, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.205176591873169\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.026041630655527115\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.4473751783370972\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.02387101761996746\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.1422632932662964\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.020463062450289726\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.4501296281814575\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.023662324994802475\n",
      "After scaling - encoder.proj.weight: grad norm 2.5409774780273438\n",
      "After scaling - encoder.proj.bias: grad norm 0.04084400087594986\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.6367238759994507\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.023835478350520134\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 1.7638206481933594\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.019467687234282494\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.0468590259552\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.020154329016804695\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.070221185684204\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.03978920355439186\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 0.9650824069976807\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.027415450662374496\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.023481322452425957\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0005073878564871848\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02820025198161602\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004650962364394218\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.022255538031458855\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0003986965457443148\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.028253918513655663\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004610300820786506\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04950769245624542\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.0007957930210977793\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.01240574847906828\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00046440373989753425\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03436578810214996\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.0003793029463849962\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.039880428463220596\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00039268130785785615\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04033561423420906\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0007752417004667222\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.018803395330905914\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005341549403965473\n",
      "epoch: 199 ( 1 ) recon_loss: 0.22157683968544006  perplexity:  219.3625030517578  commit_loss:  0.13501037657260895 \n",
      "\t codebook loss:  0.5400415062904358  total_loss:  0.7879518270492554 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.7957, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.2843765020370483\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.010114471428096294\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.7186214923858643\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.01184542290866375\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.1540418863296509\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.011124958284199238\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.2911866903305054\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.012006714940071106\n",
      "After scaling - encoder.proj.weight: grad norm 2.6565608978271484\n",
      "After scaling - encoder.proj.bias: grad norm 0.020409060642123222\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.6781596541404724\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.012122809886932373\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 1.9057986736297607\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.009991900995373726\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 1.8287874460220337\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.010891919024288654\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 1.630876898765564\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.01734601892530918\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 0.9795252680778503\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.01217576116323471\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.025326918810606003\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0001994495978578925\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.03388989716768265\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00023358264297712594\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.022756818681955338\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00021937563724350184\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.025461208075284958\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.00023676319688092917\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05238533765077591\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.00040245099808089435\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013372786343097687\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00023905250418465585\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.037580884993076324\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00019703261204995215\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03606228157877922\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00021478028793353587\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.03215964138507843\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0003420501889195293\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.019315486773848534\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00024009664775803685\n",
      "Are there any dead codes on this epoch?  120\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([256, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([256, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.7902, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.2861013412475586\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.02969793789088726\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 1.508333444595337\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.02739754691720009\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.223302960395813\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.023304741829633713\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.545911431312561\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.02671203389763832\n",
      "After scaling - encoder.proj.weight: grad norm 2.72328519821167\n",
      "After scaling - encoder.proj.bias: grad norm 0.04623142257332802\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.6813411712646484\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.028046485036611557\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 1.922829031944275\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.022930433973670006\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.2583811283111572\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.023440293967723846\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 2.243046760559082\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.045066703110933304\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.0576939582824707\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.03102351725101471\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.023229297250509262\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.0005363980308175087\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.02724320814013481\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.0004948488785885274\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.022095046937465668\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.0004209254402667284\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02792193368077278\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0004824672651011497\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.04918741434812546\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.000835022481624037\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.012306245043873787\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.0005065698060207069\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.034729741513729095\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00041416482417844236\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.04079041630029678\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.0004233737417962402\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.04051344841718674\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0008139855344779789\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.019103853031992912\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.0005603404133580625\n",
      "epoch: 200 ( 1 ) recon_loss: 0.22250452637672424  perplexity:  221.1426544189453  commit_loss:  0.13533209264278412 \n",
      "\t codebook loss:  0.5413283705711365  total_loss:  0.7902145385742188 \n",
      "\n",
      "x.device:  cuda:0\n",
      "Encoder output:  torch.Size([206, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([206, 16, 26])\n",
      "x_hat.device:  cuda:0\n",
      "recon_loss:  cuda:0\n",
      "tensor(0.7897, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "After scaling - encoder.strided_conv_1.weight: grad norm 1.6066631078720093\n",
      "After scaling - encoder.strided_conv_1.bias: grad norm 0.0108404029160738\n",
      "After scaling - encoder.strided_conv_2.weight: grad norm 2.1101272106170654\n",
      "After scaling - encoder.strided_conv_2.bias: grad norm 0.013142307288944721\n",
      "After scaling - encoder.residual_conv_1.weight: grad norm 1.4154740571975708\n",
      "After scaling - encoder.residual_conv_1.bias: grad norm 0.012247405014932156\n",
      "After scaling - encoder.residual_conv_2.weight: grad norm 1.5755565166473389\n",
      "After scaling - encoder.residual_conv_2.bias: grad norm 0.012701994739472866\n",
      "After scaling - encoder.proj.weight: grad norm 3.3114967346191406\n",
      "After scaling - encoder.proj.bias: grad norm 0.021590862423181534\n",
      "After scaling - decoder.in_proj.weight: grad norm 0.8217288851737976\n",
      "After scaling - decoder.in_proj.bias: grad norm 0.012664888054132462\n",
      "After scaling - decoder.residual_conv_1.weight: grad norm 2.252624273300171\n",
      "After scaling - decoder.residual_conv_1.bias: grad norm 0.010232996195554733\n",
      "After scaling - decoder.residual_conv_2.weight: grad norm 2.1599619388580322\n",
      "After scaling - decoder.residual_conv_2.bias: grad norm 0.010913725011050701\n",
      "After scaling - decoder.strided_t_conv_1.weight: grad norm 1.9195163249969482\n",
      "After scaling - decoder.strided_t_conv_1.bias: grad norm 0.015568634495139122\n",
      "After scaling - decoder.strided_t_conv_2.weight: grad norm 1.1787000894546509\n",
      "After scaling - decoder.strided_t_conv_2.bias: grad norm 0.011243243701756\n",
      "After gradient clipping - encoder.strided_conv_1.weight: grad norm 0.026071524247527122\n",
      "After gradient clipping - encoder.strided_conv_1.bias: grad norm 0.00017590858624316752\n",
      "After gradient clipping - encoder.strided_conv_2.weight: grad norm 0.03424129635095596\n",
      "After gradient clipping - encoder.strided_conv_2.bias: grad norm 0.00021326186833903193\n",
      "After gradient clipping - encoder.residual_conv_1.weight: grad norm 0.02296907640993595\n",
      "After gradient clipping - encoder.residual_conv_1.bias: grad norm 0.00019874019199050963\n",
      "After gradient clipping - encoder.residual_conv_2.weight: grad norm 0.02556675486266613\n",
      "After gradient clipping - encoder.residual_conv_2.bias: grad norm 0.0002061168779619038\n",
      "After gradient clipping - encoder.proj.weight: grad norm 0.05373607948422432\n",
      "After gradient clipping - encoder.proj.bias: grad norm 0.00035035767359659076\n",
      "After gradient clipping - decoder.in_proj.weight: grad norm 0.013334298506379128\n",
      "After gradient clipping - decoder.in_proj.bias: grad norm 0.00020551473426166922\n",
      "After gradient clipping - decoder.residual_conv_1.weight: grad norm 0.03655361756682396\n",
      "After gradient clipping - decoder.residual_conv_1.bias: grad norm 0.00016605210839770734\n",
      "After gradient clipping - decoder.residual_conv_2.weight: grad norm 0.03504997491836548\n",
      "After gradient clipping - decoder.residual_conv_2.bias: grad norm 0.00017709839448798448\n",
      "After gradient clipping - decoder.strided_t_conv_1.weight: grad norm 0.031148234382271767\n",
      "After gradient clipping - decoder.strided_t_conv_1.bias: grad norm 0.0002526341995690018\n",
      "After gradient clipping - decoder.strided_t_conv_2.weight: grad norm 0.01912691444158554\n",
      "After gradient clipping - decoder.strided_t_conv_2.bias: grad norm 0.00018244553939439356\n",
      "Are there any dead codes on this epoch?  116\n",
      "Reset usage of embeddings between epochs\n",
      "\n",
      "Finish!!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_idx</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>codebook_loss</td><td>█▃▂▁▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>commitment_loss</td><td>█▃▂▁▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>perplexity</td><td>▁▂▃▃▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇███████████████</td></tr><tr><td>recon_loss</td><td>█▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>total_loss</td><td>█▄▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_idx</td><td>2</td></tr><tr><td>codebook_loss</td><td>0.54487</td></tr><tr><td>commitment_loss</td><td>0.13622</td></tr><tr><td>epoch</td><td>200</td></tr><tr><td>perplexity</td><td>219.82385</td></tr><tr><td>recon_loss</td><td>0.21829</td></tr><tr><td>total_loss</td><td>0.78971</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">restart-fromzip</strong> at: <a href='https://wandb.ai/annaklezovich1997/vqvae_on_ssl/runs/dejal7eu' target=\"_blank\">https://wandb.ai/annaklezovich1997/vqvae_on_ssl/runs/dejal7eu</a><br/> View project at: <a href='https://wandb.ai/annaklezovich1997/vqvae_on_ssl' target=\"_blank\">https://wandb.ai/annaklezovich1997/vqvae_on_ssl</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240705_114302-dejal7eu/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "import torch.optim as optim\n",
    "from torch.optim import AdamW, Adam\n",
    "\n",
    "# Initialize W&B\n",
    "wandb.init(project=\"vqvae_on_ssl\", name=\"restart-fromzip\")\n",
    "\n",
    "mse_loss = nn.MSELoss()\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "print(\"Start training VQ-VAE...\")\n",
    "model.train()\n",
    "\n",
    "use_amp = False\n",
    "\n",
    "# Constructs a ``scaler`` once, at the beginning of the convergence run, using default arguments.\n",
    "# If your network fails to converge with default ``GradScaler`` arguments, please file an issue.\n",
    "# The same ``GradScaler`` instance should be used for the entire convergence run.\n",
    "# If you perform multiple convergence runs in the same script, each run should use\n",
    "# a dedicated fresh ``GradScaler`` instance. ``GradScaler`` instances are lightweight.\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    commitment_beta = calculate_beta_log(epoch+1, total_iterations=epochs+1, initial_beta=0.35, final_beta=0.001, smoothing_factor=0.1)\n",
    "    \n",
    "    for batch_idx, x in enumerate(train_loader):\n",
    "        \n",
    "        # optimizer.zero_grad()\n",
    "        \n",
    "        if x is None:\n",
    "            (\"x is None\")\n",
    "            continue\n",
    "            \n",
    "            # https://pytorch.org/docs/stable/amp.html#autocast-op-reference\n",
    "        \"\"\"\n",
    "        with torch.autocast(device_type=device, dtype=torch.float16, enabled=use_amp):\n",
    "            x = x.to(device, torch.float16)\n",
    "            print(\"x.device: \", x.device)\n",
    "\n",
    "            x_hat, commitment_loss, codebook_loss, perplexity = model(x, epoch)\n",
    "            print(\"x_hat.device: \", x_hat.device)\n",
    "            assert x_hat.dtype is torch.float16\n",
    "\n",
    "            print(\"commitment_loss.device, codebook_loss.device: \", commitment_loss.device, codebook_loss.device)\n",
    "\n",
    "            recon_loss = mse_loss(x_hat, x)\n",
    "            print(\"recon_loss: \", recon_loss.device)\n",
    "            # loss is float32 because ``mse_loss`` layers ``autocast`` to float32\n",
    "            assert recon_loss.dtype is torch.float32\n",
    "        \"\"\"\n",
    "        x = x.to(device)\n",
    "        print(\"x.device: \", x.device)\n",
    "        x_hat, commitment_loss, codebook_loss, perplexity = model(x, epoch)\n",
    "        print(\"x_hat.device: \", x_hat.device)\n",
    "        recon_loss = mse_loss(x_hat, x)\n",
    "        print(\"recon_loss: \", recon_loss.device)\n",
    "\n",
    "        # for the first 5 epochs loss == recon_loss, aka. no quantisation, warm up \n",
    "        loss = recon_loss + commitment_loss * commitment_beta + codebook_loss\n",
    "        print(loss)\n",
    "\n",
    "        # Print gradient norms before scaling\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                print(f\"Before scaling - {name}: grad norm {param.grad.norm(2)}\")\n",
    "\n",
    "        # Exits ``autocast`` before backward().\n",
    "        # Backward passes under ``autocast`` are not recommended.\n",
    "        # Backward ops run in the same ``dtype`` ``autocast`` chose for corresponding forward ops.\n",
    "        # Scales loss. Calls ``backward()`` on scaled loss to create scaled gradients.\n",
    "        # scaler.scale(loss).backward()\n",
    "        loss.backward()\n",
    "\n",
    "        # Print gradient norms\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                print(f\"After scaling - {name}: grad norm {param.grad.norm(2)}\")\n",
    "                \n",
    "        # scaler.unscale_(optimizer)\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1)\n",
    "        \n",
    "        # Print gradient norms \n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                print(f\"After gradient clipping - {name}: grad norm {param.grad.norm(2)}\")\n",
    "        \n",
    "        # ``scaler.step()`` first unscales the gradients of the optimizer's assigned parameters.\n",
    "        # If these gradients do not contain ``inf``s or ``NaN``s, optimizer.step() is then called,\n",
    "        # otherwise, optimizer.step() is skipped.\n",
    "        # scaler.step(optimizer)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Updates the scale for next iteration.\n",
    "        # scaler.update()\n",
    "     \n",
    "        wandb.log({\n",
    "            'epoch': epoch + 1,\n",
    "            'batch_idx': batch_idx + 1,\n",
    "            'recon_loss': recon_loss.item(),\n",
    "            'commitment_loss': commitment_loss.item(),\n",
    "            'codebook_loss': codebook_loss.item(),\n",
    "            'total_loss': loss.item(),\n",
    "            'perplexity': perplexity.item()\n",
    "        })\n",
    "            \n",
    "        if batch_idx % print_step == 0: \n",
    "            print(\"epoch:\", epoch + 1, \"(\", batch_idx + 1, \") recon_loss:\", recon_loss.item(), \" perplexity: \", perplexity.item(), \n",
    "                \" commit_loss: \", commitment_loss.item(), \"\\n\\t codebook loss: \", codebook_loss.item(), \" total_loss: \", loss.item(), \"\\n\")\n",
    "        \n",
    "    \n",
    "    codebook.random_restart()\n",
    "    codebook.reset_usage()\n",
    "    \n",
    "print(\"Finish!!\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## vq_vae_warmup_beta.pth\n",
    "# the data for this model was trimmed from 110 beginning frames and 40 end frames\n",
    "# the commitment beta warm up was introduced (in range from 0.3 to 0.1), the previous one was 0.25 and it was too big I think\n",
    "\n",
    "## vq_vae_restart-beta-quantwarmup.pth\n",
    "# the for this model was trimmed from 110 beginning frames and 40 end frames\n",
    "# commitment beta is going from 0.35(actually less-0.26) to 0.001 with smoothing 0.1\n",
    "# no quantisation on the first five epochs\n",
    "# codebook restart for dead codes each epoch\n",
    "# Xavier initialization for embeddings\n",
    "torch.save(model.state_dict(), 'vq_vae_restart-nowarmup-from-zip-noamp-200epochs.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {\"model\": model.state_dict(),\n",
    "              \"optimizer\": optimizer.state_dict(),\n",
    "              \"scaler\": scaler.state_dict()}\n",
    "torch.save(checkpoint, \"vq_vae_restart-nowarmup-from-zip-noamp-200epochs.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = torch.cuda.current_device()\n",
    "checkpoint = torch.load(\"vq_vae_restart-nowarmup-from-zip-float16.pth\",\n",
    "                        map_location = lambda storage, loc: storage.cuda(dev))\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "scaler.load_state_dict(checkpoint[\"scaler\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (encoder): Encoder(\n",
       "    (strided_conv_1): Conv1d(112, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (strided_conv_2): Conv1d(512, 512, kernel_size=(3,), stride=(1,))\n",
       "    (residual_conv_1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (residual_conv_2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (proj): Conv1d(512, 16, kernel_size=(3,), stride=(1,))\n",
       "  )\n",
       "  (codebook): VQEmbeddingEMA()\n",
       "  (decoder): Decoder(\n",
       "    (in_proj): Conv1d(16, 512, kernel_size=(3,), stride=(1,))\n",
       "    (residual_conv_1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (residual_conv_2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (strided_t_conv_1): ConvTranspose1d(512, 512, kernel_size=(3,), stride=(1,))\n",
       "    (strided_t_conv_2): ConvTranspose1d(512, 112, kernel_size=(3,), stride=(1,), dilation=(2,))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(Encoder=encoder, Codebook=codebook, Decoder=decoder)\n",
    "model.load_state_dict(torch.load('vq_vae_restart-nowarmup-from-zip-bfloat16.pth', map_location=torch.device('cpu')))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "def test(data_loader, model):\n",
    "    \"\"\"evaluation model\"\"\"\n",
    "    wandb.init(project=\"vqvae_on_ssl\", job_type=\"test\", name=\"zipped-test\")\n",
    "    # model.eval()\n",
    "    N = len(data_loader)\n",
    "    epoch=10\n",
    "    with torch.no_grad():\n",
    "        recon_loss, loss_vq = 0., 0.\n",
    "        for batch_idx, x in enumerate(data_loader):\n",
    "            if x is None:\n",
    "                (\"x is None\")\n",
    "                continue\n",
    "            x = x.to(device)\n",
    "            x_hat, commitment_loss, codebook_loss, _ = model(x, epoch=10)\n",
    "            mse = mse_loss(x_hat, x)\n",
    "            recon_loss += mse\n",
    "            loss = recon_loss + commitment_loss * commitment_beta + codebook_loss\n",
    "            loss_vq += loss\n",
    "            wandb.log({\n",
    "                'test_recon_loss': mse.item(),\n",
    "                'test_total_loss': loss.item()\n",
    "                })\n",
    "        recon_loss /= N\n",
    "        loss_vq /= N\n",
    "        wandb.log({\n",
    "            'average_test_recon_loss': recon_loss,\n",
    "            'average_test_total_loss': loss_vq\n",
    "        })\n",
    "        print(\"epoch:\", epoch + 1, \"(\", batch_idx + 1, \") average_test_recon_loss:\", recon_loss, \" average_test_total_loss: \", loss_vq, \"\\n\")\n",
    "    wandb.finish()\n",
    "    return recon_loss, loss_vq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/project/wandb/run-20240705_114555-nim3fiwa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/annaklezovich1997/vqvae_on_ssl/runs/nim3fiwa' target=\"_blank\">zipped-test</a></strong> to <a href='https://wandb.ai/annaklezovich1997/vqvae_on_ssl' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/annaklezovich1997/vqvae_on_ssl' target=\"_blank\">https://wandb.ai/annaklezovich1997/vqvae_on_ssl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/annaklezovich1997/vqvae_on_ssl/runs/nim3fiwa' target=\"_blank\">https://wandb.ai/annaklezovich1997/vqvae_on_ssl/runs/nim3fiwa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output:  torch.Size([50, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([50, 16, 26])\n",
      "epoch: 11 ( 1 ) average_test_recon_loss: tensor(0.6316, device='cuda:0')  average_test_total_loss:  tensor(2.0421, device='cuda:0') \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_test_recon_loss</td><td>▁</td></tr><tr><td>average_test_total_loss</td><td>▁</td></tr><tr><td>test_recon_loss</td><td>▁</td></tr><tr><td>test_total_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_test_recon_loss</td><td>0.63164</td></tr><tr><td>average_test_total_loss</td><td>2.04212</td></tr><tr><td>test_recon_loss</td><td>0.63164</td></tr><tr><td>test_total_loss</td><td>2.04212</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">zipped-test</strong> at: <a href='https://wandb.ai/annaklezovich1997/vqvae_on_ssl/runs/nim3fiwa' target=\"_blank\">https://wandb.ai/annaklezovich1997/vqvae_on_ssl/runs/nim3fiwa</a><br/> View project at: <a href='https://wandb.ai/annaklezovich1997/vqvae_on_ssl' target=\"_blank\">https://wandb.ai/annaklezovich1997/vqvae_on_ssl</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240705_114555-nim3fiwa/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_recons, loss_total = test(test_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6316, device='cuda:0') tensor(2.0421, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# average losses for test batches - the test data is not normalized\n",
    "print(loss_recons, loss_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SSLC corpus data**\n",
    "\n",
    "\n",
    "* first run, codebook collapse - `tensor(0.0091, device='cuda:0') tensor(0.0132, device='cuda:0')`\n",
    "* second run, warm up beta, codebook collapse, but video reconstruction looks a little better - `tensor(0.0383, device='cuda:0') tensor(0.1051, device='cuda:0')`\n",
    "* `vq_vae_restart-beta-quantwarmup.pth`. the video for this model was trimmed from 110 beginning frames and 40 end frames, commitment beta is going from 0.35(actually less-0.26) to 0.001 with smoothing 0.1, no quantisation on the first five epochs, codebook restart for dead codes each epoch, Xavier initialization for embeddings - `tensor(0.0472, device='cuda:0') tensor(0.0862, device='cuda:0')`\n",
    "* `vq_vae_restart-warmup-normpose.pth`. the video for this model was trimmed from 110 beginning frames and 40 end frames, commitment beta is going from 0.35(actually less-0.26) to 0.001 with smoothing 0.1, no quantisation on the first five epochs, codebook restart for dead codes each epoch, Xavier initialization for embeddings. The data is normalized with `(data - mean)/std` with the help of mean pose. - test data is also normalized - `tensor(0.1960) tensor(0.3829)`, on non normalized data it's somehow better `tensor(0.1485) tensor(0.1896)`\n",
    "* `vq_vae_restart-nowarmup-normpose.pth` - the video for this model was trimmed from 110 beginning frames and 40 end frames, commitment beta is going from 0.35(actually less-0.26) to 0.001 with smoothing 0.1, no warm up, codebook restart for dead codes each epoch, Xavier initialization for embeddings. The data is normalized with `(data - mean)/std` with the help of mean pose. Test data is also normalized - `tensor(0.3448) tensor(0.9293)`\n",
    "* `vq_vae_restart-nowarmup-normpose-from-zip-float16.pth` - the video for this model was trimmed from 110 beginning frames and 40 end frames, commitment beta is going from 0.35(actually less-0.26) to 0.001 with smoothing 0.1, no warm up, codebook restart for dead codes each epoch, Xavier initialization for embeddings. Dataset is wrapped in zip. The data is normalized with `(data - mean)/std` with the help of mean pose. Test data is also normalized. Extremely fast, because I use zip archive and do everything in float16 instead of float32. The training run perplexity is **117.2675**!\n",
    "* `'vq_vae_restart-nowarmup-from-zip-amp.pth` - the video for this model was trimmed from 110 beginning frames and 40 end frames, commitment beta is going from 0.35(actually less-0.26) to 0.001 with smoothing 0.1, no warm up, codebook restart for dead codes each epoch, Xavier initialization for embeddings. The data is not normalized. Dataset is wrapped in zip. The model is trained with the help of torch mixed precision amp with float16/half. The training perplexity is **157.15**. `average_test_recon_loss: tensor(1.2258, device='cuda:0')  average_test_total_loss:  tensor(2.9373, device='cuda:0')`.\n",
    "* `vq_vae_restart-nowarmup-from-zip-noamp-200epochs.pth` - the video for this model was trimmed from 110 beginning frames and 40 end frames, commitment beta is going from 0.35(actually less-0.26) to 0.001 with smoothing 0.1, no warm up, codebook restart for dead codes each epoch, Xavier initialization for embeddings. The data is normalized with `(data - mean)/std` with the help of mean pose. Test data is also normalized. Dataset is wrapped in zip. No mixed precision amp with float16/half. Trained on 200 epochs. The training perplexity is **219.824**. - `tensor(0.6316, device='cuda:0') tensor(2.0421, device='cuda:0')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:v1xru8ak) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_test_recon_loss</td><td>▁</td></tr><tr><td>average_test_total_loss</td><td>▁</td></tr><tr><td>test_recon_loss</td><td>▁</td></tr><tr><td>test_total_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_test_recon_loss</td><td>0.38914</td></tr><tr><td>average_test_total_loss</td><td>0.66984</td></tr><tr><td>test_recon_loss</td><td>0.38914</td></tr><tr><td>test_total_loss</td><td>0.66984</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">grateful-frost-31</strong> at: <a href='https://wandb.ai/annaklezovich1997/vqvae_on_ssl/runs/v1xru8ak' target=\"_blank\">https://wandb.ai/annaklezovich1997/vqvae_on_ssl/runs/v1xru8ak</a><br/> View project at: <a href='https://wandb.ai/annaklezovich1997/vqvae_on_ssl' target=\"_blank\">https://wandb.ai/annaklezovich1997/vqvae_on_ssl</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240627_075249-v1xru8ak/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:v1xru8ak). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/project/wandb/run-20240627_080340-zfhxkw4u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/annaklezovich1997/vqvae_on_ssl/runs/zfhxkw4u' target=\"_blank\">true-sponge-32</a></strong> to <a href='https://wandb.ai/annaklezovich1997/vqvae_on_ssl' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/annaklezovich1997/vqvae_on_ssl' target=\"_blank\">https://wandb.ai/annaklezovich1997/vqvae_on_ssl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/annaklezovich1997/vqvae_on_ssl/runs/zfhxkw4u' target=\"_blank\">https://wandb.ai/annaklezovich1997/vqvae_on_ssl/runs/zfhxkw4u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11 ( 1 ) average_test_recon_loss: tensor(0.1960)  average_test_total_loss:  tensor(0.3829) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>average_test_recon_loss</td><td>▁</td></tr><tr><td>average_test_total_loss</td><td>▁</td></tr><tr><td>test_recon_loss</td><td>▁</td></tr><tr><td>test_total_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>average_test_recon_loss</td><td>0.19597</td></tr><tr><td>average_test_total_loss</td><td>0.3829</td></tr><tr><td>test_recon_loss</td><td>0.19597</td></tr><tr><td>test_total_loss</td><td>0.3829</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">true-sponge-32</strong> at: <a href='https://wandb.ai/annaklezovich1997/vqvae_on_ssl/runs/zfhxkw4u' target=\"_blank\">https://wandb.ai/annaklezovich1997/vqvae_on_ssl/runs/zfhxkw4u</a><br/> View project at: <a href='https://wandb.ai/annaklezovich1997/vqvae_on_ssl' target=\"_blank\">https://wandb.ai/annaklezovich1997/vqvae_on_ssl</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240627_080340-zfhxkw4u/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataset = utils.PoseDataset(\n",
    "    df=test_df, root_dir='./SSL_video_eaf/SSLC_poses/',\n",
    "    sequence_length=sequence_length, normalize_by_mean_pose=True\n",
    ")\n",
    "test_loader = DataLoader(dataset=test_dataset,  batch_size=batch_size, shuffle=False,  **kwargs)\n",
    "loss_recons, loss_total = test(test_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1960) tensor(0.3829)\n"
     ]
    }
   ],
   "source": [
    "# average losses for test batches - the test data IS normalized\n",
    "print(loss_recons, loss_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "utils = importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output:  torch.Size([50, 16, 26])\n",
      "x shape that is passed into Decoder:\n",
      " torch.Size([50, 16, 26])\n",
      "perplexity:  196.14186096191406 commit_loss:  0.3298075497150421   codebook loss:  1.3192301988601685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for batch_idx, x in enumerate(tqdm(test_loader)):\n",
    "\n",
    "        x = x.to(device)\n",
    "        x_hat, commitment_loss, codebook_loss, perplexity = model(x, epoch=10)\n",
    " \n",
    "        print(\"perplexity: \", perplexity.item(), \"commit_loss: \", commitment_loss.item(), \"  codebook loss: \", codebook_loss.item())\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SSLC corpus data**\n",
    "\n",
    "\n",
    "- perplexity:  30.4245491\n",
    "- commit_loss:  0.0160329286\n",
    "- codebook loss:  0.0641317144\n",
    "\n",
    "============= `vq_vae_restart-beta-quantwarmup.pth` Xavier, restart codebook, frozen codebook on first 5 steps, beta decay ===========\n",
    "\n",
    "- perplexity:  53.5902\n",
    "- commit_loss:  0.016567938\n",
    "- codebook loss:  0.06627175\n",
    "\n",
    "============= `vq_vae_restart-warmup-normpose.pth` ===========\n",
    "\n",
    "- perplexity:  66.249382\n",
    "- commit_loss:  0.055224929\n",
    "- codebook loss:  0.220899716\n",
    "\n",
    "============= `vq_vae_restart-nowarmup-normpose.pth` ===========\n",
    "\n",
    "- perplexity: 72.8709869\n",
    "- commit_loss: 0.155359\n",
    "- codebook loss: 0.621436357498\n",
    "\n",
    "============= `vq_vae_restart-nowarmup-normpose-from-zip-float16.pth` ===========\n",
    "\n",
    "- perplexity:  90.21179199\n",
    "- commit_loss:  1.4155819416\n",
    "- codebook loss:  5.6623277664\n",
    "\n",
    "============= `vq_vae_restart-nowarmup-from-zip-amp.pth` ==============\n",
    "\n",
    "- perplexity:  138.46998596\n",
    "- commit_loss:  0.4079417586\n",
    "- codebook loss:  1.631767\n",
    "\n",
    "============= `vq_vae_restart-nowarmup-from-zip-noamp-200epochs.pth` ================\n",
    "\n",
    "- perplexity:  196.14186096\n",
    "- commit_loss:  0.3298075497\n",
    "- codebook loss:  1.31923"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tensor2 = x[7].cpu()\n",
    "fig, ax = plt.subplots()\n",
    "utils.draw_from_tensor(tensor2[:,0], ax)\n",
    "ax.set_xlim(-2, 2)\n",
    "ax.set_ylim(2, -2)\n",
    "ax.set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame  0\n",
      "frame  0\n",
      "frame  0\n",
      "frame  0\n",
      "frame  1\n",
      "frame  2\n",
      "frame  3\n",
      "frame  4\n",
      "frame  5\n",
      "frame  6\n",
      "frame  7\n",
      "frame  8\n",
      "frame  9\n",
      "frame  10\n",
      "frame  11\n",
      "frame  12\n",
      "frame  13\n",
      "frame  14\n",
      "frame  15\n",
      "frame  16\n",
      "frame  17\n",
      "frame  18\n",
      "frame  19\n",
      "frame  20\n",
      "frame  21\n",
      "frame  22\n",
      "frame  23\n",
      "frame  24\n",
      "frame  25\n",
      "frame  26\n",
      "frame  27\n",
      "frame  28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video width=\"640\" height=\"480\" controls autoplay loop>\n",
       "  <source type=\"video/mp4\" src=\"data:video/mp4;base64,AAAAHGZ0eXBNNFYgAAACAGlzb21pc28yYXZjMQAAAAhmcmVlAABtom1kYXQAAAKvBgX//6vcRem9\n",
       "5tlIt5Ys2CDZI+7veDI2NCAtIGNvcmUgMTU1IHIyOTE3IDBhODRkOTggLSBILjI2NC9NUEVHLTQg\n",
       "QVZDIGNvZGVjIC0gQ29weWxlZnQgMjAwMy0yMDE4IC0gaHR0cDovL3d3dy52aWRlb2xhbi5vcmcv\n",
       "eDI2NC5odG1sIC0gb3B0aW9uczogY2FiYWM9MSByZWY9MyBkZWJsb2NrPTE6MDowIGFuYWx5c2U9\n",
       "MHgzOjB4MTEzIG1lPWhleCBzdWJtZT03IHBzeT0xIHBzeV9yZD0xLjAwOjAuMDAgbWl4ZWRfcmVm\n",
       "PTEgbWVfcmFuZ2U9MTYgY2hyb21hX21lPTEgdHJlbGxpcz0xIDh4OGRjdD0xIGNxbT0wIGRlYWR6\n",
       "b25lPTIxLDExIGZhc3RfcHNraXA9MSBjaHJvbWFfcXBfb2Zmc2V0PS0yIHRocmVhZHM9MTUgbG9v\n",
       "a2FoZWFkX3RocmVhZHM9MiBzbGljZWRfdGhyZWFkcz0wIG5yPTAgZGVjaW1hdGU9MSBpbnRlcmxh\n",
       "Y2VkPTAgYmx1cmF5X2NvbXBhdD0wIGNvbnN0cmFpbmVkX2ludHJhPTAgYmZyYW1lcz0zIGJfcHly\n",
       "YW1pZD0yIGJfYWRhcHQ9MSBiX2JpYXM9MCBkaXJlY3Q9MSB3ZWlnaHRiPTEgb3Blbl9nb3A9MCB3\n",
       "ZWlnaHRwPTIga2V5aW50PTI1MCBrZXlpbnRfbWluPTI1IHNjZW5lY3V0PTQwIGludHJhX3JlZnJl\n",
       "c2g9MCByY19sb29rYWhlYWQ9NDAgcmM9Y3JmIG1idHJlZT0xIGNyZj0yMy4wIHFjb21wPTAuNjAg\n",
       "cXBtaW49MCBxcG1heD02OSBxcHN0ZXA9NCBpcF9yYXRpbz0xLjQwIGFxPTE6MS4wMACAAAAop2WI\n",
       "hAA3//728P4FNjuY0JcRzeidMx+/Fbi6NDe9zgAAAwAAAwAAAwG5pYX/dnfziCAAAAMAdsAVAIAE\n",
       "bDm5T4SqMDiAOG6BpWXItMLKVNNKkdO1a3uDEJIUlTWvfKDBpawB5q97Df9vnkiIZDVua+Zz+PQg\n",
       "1fMfPG6FUyj/kSwl5jtT5tX9/MDbAXokIk5kVbP1jkQnieLa1yuEbmuiV2cSMpzGFR3qUUN8/EYQ\n",
       "DZjHsmMrb6QCkWE6Ux6fslG5V0umYVQsPbvxz+IzCIjPsCSyHazIwXcU4+4UTc/LeQMeihrd5pUn\n",
       "XYgnPDD25prkuajfKiinBkVPEuHEMThPSt1CBFSWmhCJmyyowXpB7zMPPO11uxr0e/MEd+H4nhE9\n",
       "90wMZmy+2mga+zNkd8V/sTIIvJ/uDYZQ66NzAFHDmAka1wr+CC+YrJPUwG3VeyURc+F7EVwN2sWv\n",
       "Vz9to79ZOGr0vp8of1ej5iYLjdJ+FwigN0CszHzQLq3EP13oiPoV9/D0Q/7bS0Z+WtNdhWB3LADt\n",
       "DFaBFBxUyjE7T92E/Eh5tAunvIH7v87Bf3ByNIB42uVNc5dIwZjGcGPHCuCE1Xyrymvc7Bl7f3DG\n",
       "f2uUtuOhom5uhQ+b/dWTPhh950fsbbQowczSc+so1tikgkqTk1vzv0fWc2jzetRamGAPNEllXgZS\n",
       "QtWL0R3cv2/k44A1Hv2Cng234T4rYSyMd8ypJuRC+mU38kEgjB9cTh+tBZyd4xFr6vo3KXZValLZ\n",
       "UdhhAMNQcuKGk02yJjlnUCGYDHaexPvWXyuHrRNCfiYLx9yXI7ET6AEK7bramRUQvtQujO2Jgm6R\n",
       "ZeAJ/I3kdWyZ3Am53LvYKh8nKJI9riGXzR4sJQiRJ7IJTxX91dGcS6LJgZrbRJ4MKHtyDlLB6Yts\n",
       "zDrC6oJVN5Z7Bq/IzLfl4mUNpcxEL6l7z+2yGMPf8CxoQbVCOUXzXXCYSb8vP71c9LChHi6oNOVb\n",
       "A9IG1GyI6g9YESrHYECzxYoIimXQiEQ0Fu+837ZfZuOpAqyXkfSwIfgVZ7CapJVjPVHGXpZYPC3U\n",
       "QxbOIH3rpqGKLJKlX5Cxx5qij5q9Ye+0CnItTz0jYFWN02Be6usV36iZMhebRW22u9+z0lHmNNWi\n",
       "wOLs3LGBPkruB/nebH2bhnkVZOijABS6UCpHWKUvU/SfFPz22mEHPRN5+dczFA88sOwILwmG02Q6\n",
       "MegB+e5LFK5zIpEpjIDwjg7r+3TCt7UhxQndg2CybKuoB4NM32qhje6BAPPzVwFLRVBPqstLf26d\n",
       "yzoDLJAxc3eal+KyScF0lICwDuWC6+HGaG/saSX/CihjzGi8me/84YiVwaBCZedptDXMF31wN0/l\n",
       "BuECW4bfg0UObMhA2FitMluhykAOZ+bosmCSXvRFJywLDuL/iLSgq5n4/PkRwb1Bc5Uqg7dC8FkJ\n",
       "SlDI9BaOxnPYbhz4gwVv0WLjuqqO2pbN34fsubjIVvIgH7OLGbdn2pFVM3IQIkIsXleT6Dc6o11L\n",
       "EHbzfyfmXdLJy++X5QT4FE5kD/I9PeNsBsegUuOrbqX1qqLmnJC9550o8upxtQEVKBAWh2krtRI6\n",
       "qTNtANy5asdrxqNeqHst9iae72WYRFbAZUXwguCwJ5+8fCwBCK1n+8rTeA0Y76vmED7TD6GKwTjc\n",
       "6OdDLbYyxTa9FGY6Ct/B3/C6bYWiAspvKjVgLoFq+gpgQlpUqIt1jasZz014hbeN9abyDVQlIGZr\n",
       "EHz3SPycdePAypkQjbkPnxBZciyuVss0tK5/zkMDEkvzPoyW/ph1EOiuluRdEok8xwPxeeLPXYYb\n",
       "0bdrJqZP5HJKzsQqraV700AvbAnNQvY1k0WMk2FmOQMAdXfviqU6E1bdj2JqqFX5HOsaBpX1JAGg\n",
       "FTonY//y0qY0Sl9V4kDpcDMTpZBbXfW+dsUySJqjUPzp8NpxvoevTLX7KYpM91QD7DgDV0IzyEni\n",
       "ywHeVXWvcSvs8+uuhe+hDG5z4eZGWhpsxH8umeP0FEGjnfXQsfK3kaUiYG5rJnTqXQcT3VVpgi91\n",
       "6b7/cRdyihWvjOaf00McMLI/TkJbIta0EYGmrPyySxzmiC4BLV1rIlPNqZZey5XQ7VITUtkSxyuE\n",
       "eKLrv0RKbjse4Fq+fPaK/rLIA2mMjLP8bH0Vq7S630ezv7DeWuWD1PD8zQs8obmIGt0JOuCDb1NC\n",
       "YfogcTjv/yRjzamQgXIgV4aer73At7VE7yGuXQq4dMjRUSnFlxPyG9RHYsXLWSwSAP7dzizBvJeT\n",
       "3rKc8jToKReEcaTBJ+X5kmTLNcKLH9f2w+N25PeTZTgs8jP0xwcZ70WsopXr85Jvwf/D3Fq7FAwZ\n",
       "PjYUT3SDqGrZCpUKpUMmZmsWz7Ydc4a+qDEJp3fiXWJsaF51ahYKU95QvIhvZGk2cS6+ch/jGY6E\n",
       "zd9oTX78e7a500Py9G2LP+xPUVj0ev6sFw/w8WETOD85sBG+P8Km5kv6gqOF9Py0D1WjB7rZxqEc\n",
       "07OTZl/tLxIW+yn1Uze7iMEcEDEelKdMmtBOQwYR4HuJyzSDvtlpL7cMwy6yAMoq3hq5r+iTAsZh\n",
       "yqWZhQ10du5Vp9ntCOhnwrXRl/fhnrVQgQpKPPSnY2ODDYkKqcpWF3Y/yGn8ZTvnnWAWYjlkIMSc\n",
       "hyt/8mAiGJzJXqD/ldjFihw9tw4A/Q/zRUXVt7bXZox8KzD8gjl7VBvvc1eNyaMdEqppjuidZvKn\n",
       "yAO5mb0I/IP8TenoqljccTfhVDNI2PT6PyfBRq0YGRtQUzyyAKWccUrSh1YNXit5D7FA6qXz6953\n",
       "mK2FbkpoMm63lG68xPCo+GdhrScY3+Ih9oENa2GBsPdPZgycp01/XXwF/DmkClGgHt6eul1kiWn8\n",
       "4zEbbVijTfhJK6OuQz9sRTQJ93Zm0GRRpvxuvlLeNzIYJ52nUH3Keaul0ULmFhUecgKkD4ggLBaO\n",
       "1xgJdHwDllYejHXDRY3BRYgdp+A07ZR1P3cRUxtcLLiAq7lEUFmi/c/Dn95V7FwySCe6AAHUGsqH\n",
       "EFZUqs0wSpo7YuLDT5VCfZWzspvKBqwNJrm0B/P4v58BLWw3CN9VczxE5Xyv5p8VcHIIe/YA3kKq\n",
       "0HpBydfyevbyN7KsY8gBDkczGc6dsjx7MAmRzTlzFqa+6m0qViNunrSQZcFLOWY96ZdFLgVvqBoO\n",
       "M+N40enaLwqtgRYnEF4r1BgfTG+JwMBSao0RMAQ2+Jq4XeodXl0yNmoc6UTkA53/9xD4ZSF1przP\n",
       "GhTPriD9lFWm0knBXnRI/bixbxzz2sS+iZypjwIavMxdKnwe8Fly5muAKnBO70huZ/CgC5UW2xNJ\n",
       "HFqncdEsHfIVI7o/+xinHGhXMWjJoHy1hxGbt4C88f4YcOdjzWG6t/t5tujr1NPO+O1v37h8sG3v\n",
       "y9zmSJyzHPZNHVZFxpPYvZZudejn6bz350y1qbeDRAEypen0jrODPVSFmgnXQgr5R14U7FaMMUnF\n",
       "vbnyXFmO/COGbKN4DAM4hJ0Vr8DYwyQ9ImMDhwUdYb/N5hOuFmFXBccNiq20nuqOhO/TlEmedNEH\n",
       "hwsOY7n9M1DrxC/Qqy27K1PXTn52gEzPhf/MwQv58xGJppFAO52jIMUkyZgVEgzr4Zu5siqWENLQ\n",
       "2k6jfLJuKAWW1j60UNdmLi25lWH/tjGfHh1QrzNqXla28eOeO4RiF87z0R89uj/G0BHh5qe4SDxj\n",
       "yZIL+4bgGNlim+gybiyNartIACpHFme+8Kv0Jq5jlFvxoSc/YGnfVbxJ1nILRkfKZUXw/716NNCP\n",
       "ZZy71bDVgbwG1dYP0oroKQO7gDKkBCTl6ZdBkE/nm0npipFkKZft+RQGQhrxBg+zxEz6hCZqThTG\n",
       "OBAdawByfU1fnfe2VCx5A84Nx49Ggvst7zqoYzWNoczcKdxP4rQ1k28OS4YNKFR830AURsaOkduG\n",
       "eOj3oelnOxoTVx/DD9DglnUXqzOecqNvjvWdZldtAIgaUk2VWV2v3A1a8cf7F1+oyDjAlygPx0AQ\n",
       "fhoyoGAnkbgEaUtvUF+C1GEmNqHz1DBMOXTzMt0P8Qd2e8qL3ZTAW2qTI3O84eLgR1tGPgvktjk4\n",
       "SvBmqBHN/HSIn0t/D8rrQJtmlkDbjHsA4p8VpVuFtm/R2UgmLnk7p3dH31s03tbvVMX30AU9rwNb\n",
       "hALlxjXuHc7nWH3rk9E0OxpZps8R1lcCCV99usTUdW75YpVLT/fDbTu6RKwfvZNdBLy4+USRLcH3\n",
       "OIGv2QEazcG75l7z3+OlEr8C46lmNC2tsnV9c/orfKWa+W7HH0nL9tgwZ+auO4B9Chv+23VCIXUx\n",
       "OKJF+BD5i0lJaZMYUfBOes5D8FqhMLCT/0TznJsx4KorabGupwYem+nd38Y1DxQiwvn1dRDs6d1b\n",
       "siFGrxJV6WjH+H+JbI7EzMU3tVsuZbRaXtfReZd/+vuwlodVoDay4EDXQGIT+tWLweEQydRvNodB\n",
       "LbHEqQ1UXmOoB0m0B7CKGQYqccFnXrXVg9Gder1Lm7Wona3+KTiTEslT3Tk6CClPfhPV0nvdLDUa\n",
       "EctRiIznDeJhxdjtVJVn6w7DUKjH7eOJ0MUoyrhcHmOQdsLXishgFhA0d8KAHFBIJZmKtvv0d268\n",
       "Bq/EaN/vsLz9Jm/vw+fGfGcUynCpGQDc0Fb24RoI+/Hntd3hibDhJxON2qWtigAMiFvsa/WT/ETU\n",
       "U2V7/D1+h/WaoBCPMq42c9xn7CP++5uKn1eUV9V+7SweQERbhKarUlFwAV1CS6s3tIF7gzFSRDD7\n",
       "c4z4H22uNWMHsxKGdDjU1dgaK18ch0iIugmoIjMBjYvQnFIIdLnDqrVfox6biNf55h9NbRBozy8k\n",
       "7QE+eyfMvoGlMNXoTNu4/Jkm0zgQnpEdn6vi+H7EYKq8safcf9pq8w7yD9c26XgDm9g4c/WX9vJX\n",
       "qsoASdAf93K8AOq2frOovgBu8usaJeJHMqBNOAWMRbpZ/4e4mg5M3gM85Wjm11qqZo/xw179Aoy3\n",
       "7NJbS8ViwJ/gBB1iMu4qiUwNd6xCqLjwUW1G7/jFwgcxTF4oFS4a9yzY41LzbAsiE++7leWwh1g7\n",
       "sg0nRlflBDV1bFeDR/agBuUiYNaujw3PWHABC1KEJxQmuo8EZvuq+T2rHrmr/WGr7iWL513staOf\n",
       "Nkl3iLomd2kk8+OAKkZN3ur5NRcb8m7gGmKEwMURjdPs21zo/qk/v1LSb/G2K2z3UAfLoir6D3LX\n",
       "XkRo5788j3tP08L9Jn+dBv9gCxMtRhr9XWHjta5ZYuS8GkTJpIKoZHFeqqhzDEaOb4Tx0bf8bicd\n",
       "1b1glhn6QnZVB0VEFzvlZPujN5wnIbe8Po/uI4r8+9mGDAw8gdSjiXaAGpcf0odO/syiJrQLz/Qi\n",
       "AS4hvr9gC5BQZUWIU196WckOkXeuP5ehPXnxi4uiw2s5HTn7y+mBNgGHtc1+FjfkyajfSzhPvDKg\n",
       "1gpTwRVLQWoeCGYePWKi9/N7GbKZH52+t9A2IP4pnBxnqNeXapjL6lBZM7Q6w3JLTXd8nA2s+wz8\n",
       "Zj95EYEdqIXqsxfRBsW5vSi6hBsGnLYUbpfFXeFvUYYcNhrFNxtfbEzhq7f9beCasd9BLEr871uU\n",
       "OieQoM/BAEMhv+0GatV/H1qIAWtIVrfR2+Ujz2gim7f+K/tltve0CU1MaXtL+tUpjH3cSdkqhSN1\n",
       "laUV+4eBwhEeB9z0QWw+9+syjyTP2sd479tZNuR81zUR5zpew3k+fCPwiD++0aZH+kvNrtW2aE+F\n",
       "1InBLhnRkGJa0ctxq/wkk9hm5KkLqJ9loMOYn9uY6XkrZZOQcJaijmK9vNCK5JVOorT3LxH696i7\n",
       "qqmggh3piN4zteMF8hQ/Z94ifKkqs+Q4Mlagrr9oToht6mz/Tl0YuqBvTDbYwEJDWXzpL2tD/+SB\n",
       "S1JU/Ph9xl4Ic5ZWhV64FGMSXtU91QFvY+Z7wZClFSRfZhd58p556aBJOVWQvdhOuENltrxJRg3+\n",
       "7ukJrcLxtNYROR2BOuu/bFXqhw+Sr0U1BDozDtXudqWCD1YngV1K59YqAIr5J18uJOH5XWQcDDNM\n",
       "W34CDedtpuqaMJPlnyXOXDRxi7ITrrcAhMtR/5uHaZDY/gsmZ+leFZETEnDm/g1jDhgL1/qMTZjR\n",
       "nHeP42fARplaFCPwzOaRcW5iuFyOnYymLxDXgah9yP5A9en/xhRcMyCsE4DWCLg428cVHyKfzm3K\n",
       "Af2253lhHJvbsadyB4lRqLyl4KjMxuHXYW3AM1RzQqCpWFbV3JZSaFDFKp38U6ch56j1Y4eT+ms1\n",
       "Ne482FA8gJkOD+X2fcvfyUEwkGohbss9/lXxfK2aH0h+Xf5o523TvHjs7+iFPcKiKCTeg5OixT68\n",
       "qoSYpmU8Ju6yYc8iYwAYYam4saRM2YF9+qQkUQtmR7ChkBuDTwmf5triQWTQ5HVf30LAGwjXfT6q\n",
       "7FFskqlPmBqm4GotLewNYE27V+AIy3tNKxZX+/UIOVgb+sg4fgUTdb47/DabBxWx15S5k43Exnbn\n",
       "OFgXmIi+Bsx39fqiw2G+DXGUAICyEVWUtx8p/QQicjQTxyBXndVMqZ5JH4N0VTnduRCnZHGo++op\n",
       "jiJ4K0ouROf4wGCD6v9TFEkEvDYWV+SmAKDYUhlMf3xpuq84nrjJhz3Hhna1U2pBK4rVyD9jtosQ\n",
       "zcohKr5c4AJpxWMMZne8VL44kyzfio4D/htiFTry5F/NhsN+FUzXqM+N6mg82+Db3gF6acgPzbVW\n",
       "DIDjJ3MHFExA/2m9Buc2JcxrlMnS4jP2lEOVr7JZFyCqdoT7rlIJYmSgTaXZnGWteBP4OqC4J5nS\n",
       "WVdV3whgYEdZrI49MWRPLjbXJK5pasW3Q1z9vdW7SA3launC1ut4RBxhCo/Clyc6N0VEdzTVtc6g\n",
       "FepnAysTZvn7+sUjUEsWujjUDbHhS5MlxkSo41GQJuFZN8jeolUkDAtDCfovKKyRDHnbySBdyOW8\n",
       "z8+Sc6YGlS83tpX8QX5a+zGjhdIjAr6+pq0z6RR8ArwaRpc/F6TKksrlnC/+4qRunMjrgdc/hmR7\n",
       "ngh0jGXQTA+c/D5eu94uAOdM6zoBFaOmGEnnHzPo7KjDJQE3wWnNZ+IlhSQvZdTT/kepW78a6kpD\n",
       "OPWRJQXdTlVGdgt1HLdH3RLS7Z5q30atX6QRDHJOCJdhQomFaPpLY/ohj4a4+PT3MoeSrSPCqXu/\n",
       "4DjK8cEI3uJreVM2Nr5Td1gD8tTpIPcjCojdqLFV76ds3GignqIDk6D0voHnc1CRf4YFiwlLlyxr\n",
       "MXGcrBGG62iI9788ShlgVcMIZjS5727zUvQfBWZf0Z2b080MY4IUU8zhyRPYh3yoiVZjNyIovm+x\n",
       "xoIRBfi6LcTB5eKtWBiaiZ8Ik/RtE36ZFFYYPyvfdsoUegTDXaASzBEiYuwS00WZZeWsjj0Wv60R\n",
       "bOcO/jXzxitlCsFugwnKj67FzepdcNUFPCXVXxGo0jMiFAg7rI3YAJq8oPAfh15kMe3a2P2D9/M1\n",
       "cDLIQPPNVmCra0XDfiXKaYQz55wJHThPXpArqGvDLpjS8jvErCYpe5YUCnY33Bfiq8mnrfFZB6Fj\n",
       "LkXu6xniW7aO1dR0yQHR+H2bTm7qyWl3MhfL7lSuMcye467upDWzTGF3rq0PJBMFbrWGrZhQKHpc\n",
       "s+kVUVM7i6FXNhK8ON6X8RL/Q/ppkvmxAOB+o5kGP8SSqNHoUFJVfqpN8cSAGtgh1RvkFd9gbDOr\n",
       "k+B7DsNzLc457dH25i+x3YnXfRvadtzKwLnWy242O3Thn1wicyupXddZ96jd9TBl9LxdQLdOjXy6\n",
       "vZ9O6oHGLZ3JYGtk6mXZaCe0fxWZNH2cSy7UTRl+or6EaZaNbCdQSDwkqOTejiOKtYRQ+vA0uPwP\n",
       "eohCMHEfbKaMm6HRfSyMrJiOhgGXL/8wzBmh0Gaq57uiEhvlyXH7OJ1ehhQN+7t3JJGb85pgocXD\n",
       "feyu8wQBjwhHJ8P7ws+CWZTSEyQLCeIEHqa3URSFebh67g5UE4xrtOEVygr87w94M6mzdYbvi01p\n",
       "GzqFN16Gm8d/0WCqdBzpIgEoA21RayZsem8La05HPJiodeoG0h4l6WGTZvoKzdV2TjTJXwwZ01rO\n",
       "pDda+Ju5qUJT3iT3H/VxDEfh2Y0dgvCHLX3d8aPlAJvTFymTmX3Ekl9+MjmamTV3lkv2v0KP3+Wh\n",
       "OqM+vPxhd3MvNB/7zFepjYOtXuu9QL5r3xeqGD1wjK6Hi8bFiHmDuJ8p6rTJQyGQTbl6ksUkgTOV\n",
       "0hc2tiBt5UbRbx3WtjVb/GXeDetashyud6Y/fldDHZW9EgdAJVnYK2xk+/MXs9ePyO+tSmG388wO\n",
       "K2IDL8SnGKvWTN34qoYOhdDFF9YMSdNOnU53wP2NxGNgyvT5SqNQ5i09AKZa4LgOJwgdua6rQL+Z\n",
       "2h5VtkhkyxsX0dgVgISbEzABE4HycrL/2itEE0co7caZkBtaXRTuRrUHNS4yDOFT0J2pOCIBGPTr\n",
       "2OXal92kiQTuscgrT+8bMYXEEanZF+IAbnbamNV2Fohlsn4sqs17LKvEzl68dmxu3NLJDdi8MHdD\n",
       "fgCTKmD7197PWl57xXIhEpB0hN/cMm7LstQCjy1ukCmxEyE2bQxeuglVC9ClN/IT4ctLoWNG5IFw\n",
       "C32UWHtLwuaSZzmo9jQDsom+lwwYEdULQGys8NF7f9jx0YTwheVI/lM5h4aqLJw2dNca3LbJ/bVP\n",
       "pHo69D4hiAOFwqOudsn5vLYeFbfQSFSA2gq3+18xULlJESJWKJ5ASYOh4fC3v3JR+26K5QAiRWRd\n",
       "xjO7Bta7rcuuV7D6WWqiQTtVGNql9rrslHJ+EVxoz4Go97Iu8Efpwxyojeb54rnBnNwXiJ4VK3vh\n",
       "D0tDqXOJQQ0PFCYFmqU7o2kFfm1ZbvMsDZfNFRty4x80bY9l07UM8x2RNa668xEl4f6AdEl7CjQF\n",
       "qpYcuf+p4/Q2XdaZSgNhuB9EjBgOLHG9Ec5xJvLguulJseQZm7eD0tsMlbxSgaYT3QaK+2o9FoWj\n",
       "zRELIvWJ8x3n/IKb5C+TGK3NZd+5uoYZCMu8vCxIgL0u5oaCQ8UWEqgRQZWkNB4QDFZiHKGTaziL\n",
       "mOcwuRYIaVSgIjcFWVB8v1uERR/Dd5ePQkypW3nW7O5OaudMSATvR2Arh1YY7tcIyqWMt7xeqz8f\n",
       "dgNnNZpCL1h0pST2VuxD3+Fb5fxxPdV7/GAGDAfZCrmWFl5aFifxN/T8wraZIimWNLWIXDkTIMEJ\n",
       "8PXMBI5s9Kh4acl4txtvf/XIr3sQjttZYX6ASi3L8jZ7cIaGNGidV6B+MGEGi1pQosFJrzZ0j7+4\n",
       "n2snY8ssPPdw4vxee+kwjoOIc5Q7mWJ17dglKqW3VPw2JQa/NnopFmvP58tOjwvKUBMeIl0zVu6r\n",
       "ul3VVIECpQdyHxjBqz07w00PADUuogNI1THzXB5KEpjDVXLrZUEXB2HOMb+HqgSiDL5JJpO9olIe\n",
       "zoL6D0GXyhHjEQfkHIO9h/xY0IEOgauTvJmouZbfi2DOuDS8sXKq6lzSgELbBlTSuf8938LvfFw6\n",
       "L+bF75vQ7BAPQCig/YUZxT+8iP4fwNiFZ76vORUAdPa0Z6UnIPuedWryGQF7IoSSLPBu6QJKxcGu\n",
       "2mladEk5jA8PxdfpSGiR8eDF3FY61BSUTo3f5jNatRmjDK7ZGVBUmz14S7STrJdpZOfHqykB93QK\n",
       "HMmrOGyj5Z4gieae3+YlciKPeDk0j/CKo0qdJC7lOhVekq9IRoxRNskWVkiqmX9+6eFJIMv1Xn7o\n",
       "YcY9XFeTFRD1Y/sQ+iM/7QMYT0BPbwGHD1rVclbhzWgoOcjeEifQBw71KT9e9zQLLZV5bQi4hdw5\n",
       "TzSQqQeIyVyYncGb1QPU8JIqVOK40RA+zRlVO2Heoro6UCroVeLtZDAyCNPF1OAloPMMOgI18tPh\n",
       "NcmfjwJu46uIcINq+nPyV0gSic+s5h8yUKB1+8HsoL2XwdwP93SBOcENc4JCwUgr/r443GBNhpY/\n",
       "55yQYoNfO8rX7L+fb51KHDRRuvBBUeSb1rryhkYh5oYlgfw3GIv74opF3jKluu1aDjdwtEWVNoDb\n",
       "hBe1Ldff/RXotR6aGCcTk0DM1B+1MpLGv10ifeAWijAgbOTVtbxs5ETsrgVbDEMzQJtUOFc0Xqbj\n",
       "P4xwN7HIVKYFbqKNWkqnqZ+YAeSxzDVBhCircS2X3TWcEs5GPFnGF5q4xj8YcJvXv/vOb+nWQ32/\n",
       "2a5pgIHFng+80/q1ly8gp34KHpHF47AEFhRNohqMaeVwDq3i+sqbwxYYbEY76uQRj5tLwguutr6x\n",
       "LLS18YdllB+sSrCaTtQqLA7L7XLX1P71ZEJFIoKBU0K/ubtO6S5cp36ibqVyLt7hilFpOnJPPZrI\n",
       "E56jVZMbBe4DQfHlPkN3a7sT4bPszM9iiTxzwhYxj+q1wwYu4UOewhPgglNXdAXK248nsFgbO1kg\n",
       "QhoZsqDaY7bzB2L4v7CDnaOGlM7yEqVkOHK8k8jH0lhOIzSimAk89jIMVDjX/HzHFG6FpI6VCH6T\n",
       "XcYA7fSem0kHOnlio4VpU96Do+DNpDrnLuliWjDSpnan3P1m/ON9VjB7Gv3d2jRoDKELeDKySxoD\n",
       "Mp6rbvkYlBVYIqhTFnUslhpdZBorurRPiKLG2OMAu6ffC8DukrCNMLUx9tToZwHSzmmL7efS7fhw\n",
       "Z9mrRWOmv2q3UY4bwrWP/SFY/khzUNuRh+8KY2stk1xgka7AViOwOokvryMV5i9MyLgYsK2MsEwS\n",
       "eo7sSpWSgiRvrfe5QYDLMTOyMDZu6hvNO2UfphknYgdiuQbOkAbfopLmrYaIKegnkdp1ILJWrc/V\n",
       "A3nybwr2O9XnbZLC6tdctrawMRoCuHR2QyJa96QcZ0a4aO4owSj9E8V8tl6OEVJ7XP03vHq1nTWM\n",
       "+iFWzvyscuXeqX+v51/Gd+TI42s8YkjCeo4Y5pJOoY88QN1rCERP31vxSJWvzEkw22TlblLqx8OV\n",
       "uWyeYqLaa6TYjwAYu2Tr8uxTIJcKa74jPTJx6aUQD27tBbIefklpR5bwtCGH107cHSlF1XIZ/E1n\n",
       "1YBlXmPnRV5k8tX08iO1piPU6O2FueSluJYR0jqvMfe/QiWrnIWLLd7JGBJPEyLGyjNC9Ls7wl89\n",
       "rtmO4tC+vnbl5avYKRR93gcAw/VKuwRS9iXnTHM7RGizGQViIS5cBr3kx8T3gRFIGYXlFkA/1Sai\n",
       "R+xJYTWYDGNjje1BkN/88swzlkfo52APo0VLtHVdl3HwJGKAA7hgb+O8uj7zJMTxNA35PhZYDQT5\n",
       "QvcrkKDV0L0rHUksq1+sK+8UbgjV9vBHgq0T5035KdANoGBJGdEus7F0IbQAa4ztONbtID/dCgtd\n",
       "YSZ9qr6PL4vaYn79BOzykZHJKpRivzqcXekUTEYCowf1Hc9F8abwW/Q/suN593BSiPPY5TbCRuKk\n",
       "BZ2jdG8wRhDbQUI8qg2yP9dEpTI7/SFEel69nq6nbEOd9vBeMPv5zMPQiWAbo+wgf+r1WdDQYaNH\n",
       "0Em+hjS8fyOkLsGop8EfO7wSi1/DUaA/hAoqbt7203tmiK/FfqYNr8uRHQ97e88E3UFpeQW6lsDk\n",
       "BKZQrJ0Ob37+ofxOQyCOkRlbDQoAQaUytZoWEpe2NCbHyR6k8Xy0w/rXDqwpnXngjQwy20KMSYHU\n",
       "d53+tfI0U/Z9octloTc4R8SOW23rnSlLxzJSasCuPm1gpXaRVi47WPEzMoWSS3nwR8XOi4x6RNzo\n",
       "XXTKKgZIo9rQAqlvkzn0Ry7OeUEamou2K/0vyHVcbdHByC70KQGCBdGSTK8pOsNeuhV0JXl2sgnS\n",
       "gIL8lv6grji4N+VxjU2QUEHlYz8sd5mTAF4uLywI1u9LGV6s+p8LoLNe5bwWLrTcHptP6aQrDobc\n",
       "HRFIlSNhejpspNZgyDf+Qr+AT+l9HHHpxSGmek1iTbLyp4SYeKPzfIsIINe/HL8etIzSUBtK4tN+\n",
       "N0ADx75Zm1QcFRyHkD0lgF855pJ7RGZlfQZ45C27DprNL31IF9avTY92E3Y9bPLjdkoY132abcUV\n",
       "my8bQI+lAay9hVZypZD89Yfo2AeNZELYSQex/kF5q/tqE/dfCf9H1bnhSUfGVMOEGNHCBX0209r+\n",
       "6a96Zqo0unBP+opsHfnTFdew54aOmXLq3f/qBx6w2x8nK7bPAJHI8tzxWVr0/qEw02cn5QgyzEXP\n",
       "MRHPwdUvz+AbUFFUpwT/nkJArDQ3OjZPwN+wj6TIdApK60x4JfxEvq3pA0zoxkk+w2D14HiVWYWS\n",
       "BUH4/ML20KkQqgXRHqVwwwEy6syj1htGtL+H31dXjr3XKkDtEO6s///EAWsm7Xzw5KcesE2BCgpO\n",
       "1tYEb6YttMoxFBoAO8hsxZzlLJv+8oWp2P/fQJEvKETAJjAqUKbDp/lO2XBwVJnRENakZyZ/Hu8+\n",
       "WgjaR762baqSTmSrKuZ1bLEceCOhDYVj9BnKsxToLO/KSokA/OZUswR4Lw1YQ4RskFEBz5bZ+63K\n",
       "ufT/yZrs9RANZZK2fzFupOdpCqo/xHaRUv0mjk76EQwRD/PZ2ynxjn8e4a3kzGmrIPZPOypdbZb4\n",
       "w6QDPnoPJ7dGTBEV6zVHvD7FY+V2TFdcLz9y/aQqwTnPvLqm9I0fg3imeNI0ZAxREbM9b/PsUSpZ\n",
       "JBDO1CffKI1kJNPmjP+lfXh/lBhabvvU7bB7re//E1ibEuIexf5EjHdc8SUnM00zUV/QkeKyUOMd\n",
       "0E8zZhiyKjpiNNORbm6iPRSFVwMRyu/ozkvZMVcsW3FgfYm8zsTW2ufLrj5AoFlObN6JY0/cjlRH\n",
       "cUv0D3wZHn46E30j649o6nvkgZ0kgfIkrj1h1mtDfIRZl7C/ElPT0FyBjPJAkoJfU85Z/1jMNQfd\n",
       "U/NIqh8AaH+xolsOA9kKt2cVNYqW4u3f7GjaBzCyTrMJjr+afy/P0Cu2GSMc15ecCWPV5ignNVqW\n",
       "jlqzVDWhN1yQvK+AcvPRAV6kmakp+eczg8Ythp4fXTxcycrujBFf96UoVgVHXOXzqWsiGZ7WNS9q\n",
       "OOtniN0Aw8gctKZ3guenY8HNVTv4U5UjB+q7fM8yokMlpMVYhpsToF85svvQEbq44VjHAop3qE7/\n",
       "zkbRCDc2gesn//Vcmffk81X9cb1oqeLY3hSQi5QyYsxDlVsiJHDv8Na9Mbwa+VR8Q3SWKAPDUPvT\n",
       "60orBoAtpmlIV7YDWNGhoJFAJ/xSPKPZ6xPDYPkszmTMwoD67IlUv+eTGy6pZwZ/Jk5nMVvlQy4S\n",
       "iJdc8mnGLQ7aKHafs9ie+ORDLfYRqpKa0dDheJQrWjFj4Q4YuJa8KIN2+RDIkw+fIzjSnK341ee5\n",
       "KuugUjDIYrQ3/h5ChEGMJcWHME+cZZdpRgOS+BHcIgaxLUhLH3AAOWe5eplFvhCRbidEo6xjPntt\n",
       "fWtAgEgcP3LLHpGUqUm/iCI/Xr3qP3t/ITTuqGjyJVnCz56p3io+epxg65fQJ0UB/u0QbUttv3uG\n",
       "5OHNXqfEvWWgNGm5sv9tIx7jt9G6XpKihtKaw5Pl/IWTCzxhyZqMurQF82soPhle02Go7PMu9rud\n",
       "CQgw/SbRKJlNOLqoe2M1EWe2eb2sTL5xXw51oHofqRzeLA/NivcNHOANFiOSuo9leQcgHTb+SROf\n",
       "R9mSiLmCdtf5Lg3TTv4A7dG1QRDCisN5U5PGPQA7n78jwtydVR7aXXnOSfsYFFUhBE4dSkkRDBkw\n",
       "f0O8zS7vyN5Bk+7/qc9Yzcp7cAiY3/kJgTk2mcdJW1kfYk3wPbWAAZ+HZtGJNFKjetP17Xw5kXdX\n",
       "I8o71yCaD2CjgmrkmReKAAADAAADAAADAAADAAClgQAACOhBmiRsQ3/+p4QAACxB1EgDkZb7Bn+r\n",
       "fhPIOyJA3KoOIQqe8ED63WSiut0N2b/Q/lTp4d8viy2tGq78qlidJFjsXajYCqCxUyFSTfOLsgWk\n",
       "7Mh0HqejfNCWog7GV9dWmpIERAGX0LMOKmk2LspVPFJRVij1Nkpx2Vg/gm6/cxNt0fX83pJN6rDr\n",
       "+El+J1YFvl7bHddaL/Rv4COfiRnEPRYWLDYapoNGKxM8Ge5UX79cbSZNfEBybnbgy6TMXkYFHuy6\n",
       "CeeQsyjXPKsvwF0/+4QG/Ccd5XXS8+sUHlulAi/cCImXxeneYpioIY4FvjVJvF2FdWk9q+RX0fYX\n",
       "B28Voq39MMYq4j6N3r9U5Q86bcSIj1kWMu9zf+W7jMt9GGRZXF0WNHiDBNLcZv8hqETUy/hs50Zs\n",
       "dMHZmu0u7XHTRD81jMNkCNckVqEK40U10tYpCieEE1fCaRWTtRaHeIdCnJKygqKKftJma/0ZQTgz\n",
       "JKP6J9oXLkGx4HU6ZRq+CKsooQlG8wNoJRvM/+/vaUfA9oElwqpXzFozWkBVq+n2FSvBCfrsDBpi\n",
       "eePYUWQJ4ITTxJymr8urEjEkZS/EeOrFC2LIiVBNLjjTR62LSHcNT4XbJBjjAXB8AJV2tuWTy3nE\n",
       "bnBrbpCLmDbnU3Zd0fq6zk+9snkW7oD/qpcj6gSGRRsaPVj8EFLxoNBYhlHtTwbXhR3GE5gJnHIN\n",
       "YSXBjOchCvFF8u/oLWepZbL6BbjVfx3CSxC9dNuKBBPPNd4EDkM0VM9IvkxnfLjEwK41EVgAHfKY\n",
       "ydNGhbWez5VmcqpLoukshd7/2AHnp3hV9GVqOVBBzz7ZbT43T7EMQAuT4DOvkVQmvf4GGafybBLh\n",
       "Z2oGty/WvDyp17jsgwUihLP63rtVvJDCAbu3/BNObAcGL1EW0Dlw9cd8DEzBKs75MLREOJrwlPP3\n",
       "/akCFAjgI90EvfLGfRBYAQEzYoIQ0igG5l3llQY6dexKX+ujW+TyQNL7Ncib9pXkIRyrBYBVuZv1\n",
       "Ngh3lnAcnEf6Z39MA6TALQqiKFbwjyNUsUjNrcRP3Ll7jC987zG/dyeu+/tCov3V0Ip5SEf8+tcW\n",
       "27wrjAisGsFDiMHlfUuE9RsEaBVpct6QkNjB9AE6114YmhYGE6qL3G7QpeHvonbDv/PqPTCSDmp0\n",
       "XjfO0hrjmsTf207jJFtFPE7BRMXlcM/QT2eytQbPscWMH4EEVucy8sRfYf2JoGIJaURtrg6FGuuL\n",
       "tSOnaaXqKenzmjuEW2bfoRHORx5LEUA6CdwUN1tiAMTqohzA0wTU3M1RfNidxW95C/MhclYIeIOu\n",
       "C74ANlUqfFrSHMZDZHWdgKXMPvRBI7T80Bwca1GL3USeLLUGS1dAW7nGlTczWc+KEc394OoKlTNu\n",
       "QA58MlA6vAjFVDAqe8iqAoD+zRBXM9fZMQIQ6Al1fQQIXclh1wx7dGpw1C2YIiQW1v9ZLZAdS7e2\n",
       "8A3P/t20RVlvzmv2In9KLyNz1drto0VWLrS7bgCcOGUuQkRQOPeqiL3A4wD70gSS22+nUeuGBdS4\n",
       "CDTDWyQNHO2Gb7FUIvl9ZsEgzlMfDrtpWUfdowd6Ci6w0BOMeu26sIw8WsPrcqWOZ/m8UGyQoJop\n",
       "hDN8uMxmb2Qwb8x5CghoKm8YdIEzaxvTsy4PPp5CnSoqc7mWDv/ayK8cgBq9YaAiXILjLbAl/KFU\n",
       "d6cOtQTW9FMDGfDaasaKu2jfczIIy6g/3rB36m7ziWLUq8NKEZ+PzP3ZCa4k65PCOmbHmY5nJHXO\n",
       "k/l7ZVWI66x+m1XNRIlcw8S7FDK0cRN5YDOnoxA7C3iqY/gJVkpMfKBZefjp4yuFgvgoUJirh/H8\n",
       "JWWMMNsZwg6L2s/Osj5UXMxrOTbfffQS8/VveaGPNwJYLjRZ08sMWjmXgxpvDqbjEbxic+iEDSJa\n",
       "QCrlks8s+qXuw9QaC3OE0ewYNloZ1bNJ7jR4/RiDvvbz3CP5yxFCfXMx0Xzd4sZZqLAglHcllmPq\n",
       "y3zCx4qMkJigR6PjKAwJyomI/Q3ic2Z8uGHdnOU82IhwFFMjYge8z5mtHpFJei+d800ygfUbpHjj\n",
       "ccMzrQv+H2sRJXdJJHjjMEJshTs4rl8j3XahyeCVa2Cmx7M3SRdSyfVptMyq5D8512Xg6T/i/hp0\n",
       "BdusXz2o05W7Klfxs9Lt3K9+0z51II5WnwlbPOgoU1iXMNSx0jgsdhbo55/lqnfKvOnL6L67bqZQ\n",
       "ckyDmLV/PzYuXOfmj2R6qOtmVpvcbaiX+TPrP2bMQOFrhJ9UpmclMRp7tl1YgduyxN8JyilVNhzO\n",
       "gE5BoeQ0BXrZ9pUw3DJwrGQqPH966ZJ/1C9eWCttiAlDvY6zTFrRWL00qwcOzsNhjyZFmZGfksLi\n",
       "pLOxE58+j0s3/lPlwCAPeij9M0LcncWjHIZW46i0FBFaPqMPc5DUQWv64q8s7/lwAbV54p8nT8X6\n",
       "qzf2Fp4f8r8zTk1mICeavm26gPftC4N6gVnZKJFq+IUHxAhyAwRyv29srbjQsRzmz3Qpiidt+txz\n",
       "dh3ZRKNqDUYLlK6Mfweu9WnltWv+muZKJsfDqgUZlahT9J7Ia7Krw/Gmhw9fndimJ+EiZ+JRfHK2\n",
       "AZF15aehgaryLnCEqCKH/7ABGTM93E57IlvmOihXRxi+9BE3ohsuv/B6KlFRqwhJEUifhfzFN+Vd\n",
       "oC1V+ACPN36qGr6a1P5ybIeuMs66DCEY/1xDDW7kzQNiFGyWInhRHItNRkSFSHTQeRUMTUQkZGU5\n",
       "IYiqjnBrn8FqUw++b96+il9j/5feDkoAOxN6B3pUawH2wLREgUdnGOkhzGDGUQQjEKY9CpEfUuoY\n",
       "2BPV3NvRhCtXgMwZhhtpeWrDQYY9btapDKAAGUg9Zl1Gb8vUNcIigWihw9fv3p6tsRcpmbpA3YV+\n",
       "vHzU+pfIG5yEhtx+RE40o6b49GBL/baTFZJiC6GtMH0oQrGdg0rO2kKdRxy6zoQw07KhB88ch3Wr\n",
       "BJGHndz2IM/YpsmeeDwt60iIUx39/LrMaljws5GNXIQAM+AAAAN0QZ5CeIV/AAADA5vqvTdXAF2o\n",
       "T165tV0KWrKYANdhhCmROE1EBxeDphmaskHgtkde/MBHGwr3doN0uBVEuhcNERMXwyAfstuKpI7q\n",
       "RmULs8MHdo+2usz9MMVB+KGx8aUzPg+IssK5RrYGk9X8xCf1x1ki89yHC6RYQVc58cM1Gn5ro86L\n",
       "aVb+yHsTgsavHWe8XgO5uaS8J8grFRt6WRGr/aNyX1tYPzlQRDpBIPOjWnok2ykZ4W8kuj0Ol7KD\n",
       "3RIaBEH5t/42sqQfGkBB7+lfW9vobqplIsjWZueznncfSZvpRrRcIICqaZNLUwwyfXH+RkBwYCCb\n",
       "0epM6RHBX2Whsb2Vf3XbvcwDs3ZcKJmy7wx1w0doJY8EqlruabUqxikNm35rRXrB7nJkkJCOOuE5\n",
       "vU7kV1kU6rFtTPm9CpUcwXoscovy8BM2svhSn+hfbTrMHnNl+pP8OPju1p0inbjomDju9QBPWJkJ\n",
       "TIHKkKCi17iP4SSaGJ+Uaemb/1djiUwxq3DXK6i0ccy1Ei3YQOu6HLZOtwSdT6dMWV1BjcwAliX5\n",
       "u+fmiAehAYEuln51h1TIRcTGPt424uTyllN81lfkh6qfjVmVnZr1KHbi4XKux+6blqJGHI4wVFg9\n",
       "rwYj83MkY4gSWKsPj2lRe1Ny7MfLfah6f9gvXSix2Q9Ea5QnMBc1d7tP4XaIKhYJKgLCXsgiLvqL\n",
       "4XL5ErVphge/SYka058HdRQtDzG0PNHhD/zk/al254VtwjrhsMqb74LBKTKMndTobu0dgmTTsbgJ\n",
       "Tg4gbtuTphyW0mMibDnhgy5tGnaiIiHahagFRIIEttSW+oWc7U1ar1jkw1tld3VTKFapCrnrP+sg\n",
       "uP2NppoMFxpnB5iBbLFSrYGhfPAYh0hIoGhaB2EVZdwLt+bnE2nrogr2//QCDISPq52cwTQWb//C\n",
       "qn2qZYLYIpPlLM8tTa8arvPZqtPEQiSqsILVQCZuJ4a3CSA4A6gVQ/nWouw48AbLu5FDz/mH7cDF\n",
       "tI4ljXAvkj920voCgFsxIg/7pAo0yfT2QlD2K18TVtXoNwo04l6iFLdzwudUL9NfU9PoZgcsV607\n",
       "3vKuAuAGfCNRpfOr9ZDu9yG9v5W+WeVuL0dtEVCRYxU5gS4PMkKM1RmWB/W948YCiR8xIEvmptTq\n",
       "ycJLWWiypJ4MB00AAAJSAZ5hdEJ/AAAEuHXHg5Q5RebnqPKVoroAAWxIzxg5uoyczxZaPlbmCna9\n",
       "uhU7GUlpEvGU8UCJFEGNgQ8fCfoV12MtIBLXgSX7cwTE9jbXh8nTgfxU5x1lLdv2VqNFMc2lTNBX\n",
       "g6+EpnQQXOV8WEUaYfDc6XEn6tgbqJAch++EWX9u33rmgX8gjHv6/ppIeKfk4PrRdJWe7f6OeBQc\n",
       "hfNzb1irb0q3ja5qMUhY6cGnO1WnKpJBnS3IFuOJ1bps5ff1SUIyTbkJdNOtrwTUQb7SaoMesOlC\n",
       "5pM2QHCriauqfgy2MKtI3GAH2C/+FhF6d4hy/UVFH1CoouO0FFmeXHHOsTv1vBMj0ttG5XFxU/nW\n",
       "OSPSoZxTpYAW8Yek8hbpPxqTqSVmTSBG0Z4RJlbKP15/5X6WEp513OvKdFLu8KhvPZ33UCn6gmKg\n",
       "DwlPeyy7oc7ioFtj3dUY2PMEjV/mnJx9WHLOP1IrUGMioZJIhQ/7zYCQZqy82E2cqqhO1RQ2NnQQ\n",
       "k51QKthQ1ViQMqsff+uR9WuW1Q5pmOMv8Kt8DaZH5qTYy93quAnkXjzfJXokOmbcE1dX3XwyRqz1\n",
       "ixbAHF5NE27ANOPpruUfPmHRu/wdFyw7CdO7sW5o12AcS9RYiT5GnUaF44tc9s08jcY18O5B+yBv\n",
       "2Gj2k7l0HL2fWsH2vH1GjHYPJB60JLIEnuVYHiLPXytKf60tcUxJRj5H7tE/Nj717+QyqWr7a7i1\n",
       "ZEhOszqKTb82QU6eCminqJXEws863PAibPZbhNqcWIsNNo1UIAGLAAACZAGeY2pCfwAABLdw5KeJ\n",
       "8xEznsaS25vz+G/blnfAAObTu/ia3AONbK2SvPpnvSY+6wgAvmusTWRcdI4ZIlELPeADJsbrmF/O\n",
       "l80z5Enk6Pa334dyOs/LKKoFyxIsA12TTy/t+/+8+LGUC/Jeg53S2Mumg4CWR3gUV2syZQsc9/YO\n",
       "8XvGO99+vnpA4dcJywftJHrWg9ygMZTA7HtTBVoavdaWs9WxkuN3mA+9ZVPorAugUbvO47K6teMq\n",
       "nt1te2E27rb4FDLdhZ40Hjqmcx1ct5MY7EqQBdgMh6lbftRjraeBwdJ/ic8zo39cUIhNShpATx8Y\n",
       "+M9ppoAOzl+64qJHZXLvKIZJLrJkA9A2xrQlDru1G6LGHp8BTZW3Hu/58nZ590k3XFGr7lJZeKF6\n",
       "PvJc0sJEd0uO6CKrY0dTATILhHpo/ukkTlHssehS4gqn+FUQy+GZCWLcbnT+uRas0tZoeVCl6UrM\n",
       "qfIuEL7fgorUCbxYpxsquFzmC9qmUAdla02NXQbe8n/UEph8pzRj9ey33r0yXrSkgaeCuGfae+wm\n",
       "pFSfTaMsMY3Pw7kKeZuhhX6fMwBD7Mhf/vPr4++wIQEVqb1RJaHwURXBLUVTyyXfXqgu6cLC0k+f\n",
       "cy54V3CntcbOCIoua+0/0mHZQbMSTHy12hUSN2YckjuCr3yDezFkaR3vP51gyXvLQFmEqwl3UapI\n",
       "1Srkn9AtXnFuEjDQVfh2u1Ksn83k0wqP2V9FfTyTpDI5yBycY54JTqXHD/k1OXnmcpxkujgj9EBv\n",
       "IYFoPxvDAWLmYhXIj1sHJhlcfL4FjulqUKoQMQAABrZBmmhJqEFomUwIb//+p4QAAAR1V//ZYjER\n",
       "1dwDVuamNXnKRGvnDXglQ4vCJ9iVy+xYg2ekuDVaRaym2Ogpyg+Si7T5o5afrQCVH03XQ0OnPPhn\n",
       "SqLprrKpShfKmnK8UbasxcSgRkpsZ2R3JukbL//TcP5R5k8n8yt0da2WKkPJ6TNmJkxg35XO8CMf\n",
       "vGu9wPUCw6RYm7PGwspWZSg2ZpuH3mzVOPxxu6wEhlOcxUdHuyJw/fiMSidZu2iPWziQxJSQuUWn\n",
       "UYKbAp42jfk6vKbKej6Sz9WcciSl3OyWZJL/WS4+2CYzRtJiE+JADFhVIy74ZCzpNnFIdBsFTZHN\n",
       "V/pOCw88HmICwPi2GVqjk/LeC/Ar2/PEAL1sFVo2Ku9+kpSALObFmG/4pN54X/2o4Y9lwpQgafOf\n",
       "49dd65YbBKIfoJ/9lsm5HiuVQzlPVwq1BcAUneeSPRmFR1bYZKFW6JL+8cFLtqTOjFeTKlz7U2iO\n",
       "RtvujfY+33uAahpGcSYgRO2BwGS9Jz7TVEBJ96tnju03ju/N5f+v+YBfoDYMluD4thKdEIm2Rayl\n",
       "HzImxRD6e2q3zmk6WD7/usGj6qlYczD+S1MJl1tqVNDwx6ZhiAbVWiXIKOUviYHdRplgDKfu4C5G\n",
       "PW8KKuxhxdT/d2B+Rzez1YZem138TZ3gdUHhN1mmVBVHV01znRyaYOajZFU7HJNy5nPntI0PPnXX\n",
       "ZPBLvumiBFGjcLWPZEuyWmbSLtIywbFfTPo2BEtm3rpJu47dN8s8JQNeU1P1qws165TwSaG3LKpV\n",
       "Sd/jjKXu91njMNZCjD1r6Uk9ZqxcUkWLPm4Xfn+eRY8LFPr/7oAPCRiUlaGP3GVlYts9kb18KJtM\n",
       "pcvup23sfM9YhvAv+quWnoWdN6VanP7//Ppd+VfLZwMIJUls0EU/E1q+dUBw5W8+ThXgB0wu5Ua3\n",
       "U4dHezRuP1YnbysrfWDqamSZDm+LIOMYZCX/7ARBii0548kjsXgNpnzShjKMsy2+1N+VMWUO5Eev\n",
       "YCfryu88G1pafPagds3oC6mQl3PMUiMLzu0dEuBsNq4R4kayGRnHq7OsG7dmUyJvaBtPSxuMt0kr\n",
       "4XzgHVrYkJVJAqL4p42nLaB0La9wet0n4oImxG3FOz76s35cnWF1kK4WT+9Ki8mQ/OsPGurA2qJh\n",
       "UrSin1GEhRYKgtXjLEm+yFTC8oZv3tj1BCMZrr+44VNmaopvdUSqtZxAIXerFKoItEcW6EDQp7XP\n",
       "BIo8gaOUEbBrScmPFjbuQRklijAcNQYSkcX4J6kEQUl1zV4DO3x15CvC5+aubviSnxRNBkv82O9n\n",
       "FHyOndnTRD4fpdz2OrMRHgEaFCQ2/WhwJZYFcYyYDPmKg9NaFm7d69OFEuNNqjHGj10IKWhcFSrG\n",
       "SuldzIpI4Hg6/GvcjLPbwnFuxXsGQROJIorABl6fIVMQtPvlbCUf36iOF7iP9UgrAFFTHBRzQjM3\n",
       "SrPAfmn4YxC1gH0r+hzFcMLM+gZcWqTEg4uSiHc+a/mkkeoPKXF1FkMwSYcGt4Z2e5IZZ08FucSt\n",
       "7EkAQXlQKTRX/qCGAES9SckSPfSO+QVfXQndfK9Fvuel39lHqFrK6aiJ+JwUtcn09M2MY3yLXsir\n",
       "s8kQLVW1GkiZkePRbsv22QR0fPeW2RA10QgIXjqwgS6V9vi84I6VHqbpgnkwkdWMz6GWi8spjd9f\n",
       "HbvCDlgWNeQwmZXybHLVO4ASCOEm+AvxaxvA2tEp9cgJx4l1FO1a7gV+9279q70UncVut60bxGbR\n",
       "VH3mOlmc2+M71vwvmXnXxHTKf9GCaNYQxnv/U3F0buhvTwy41dOl2nDnhNvHYq76sHB+yuexsv9U\n",
       "IypufsnWs4ZAXINzpp+T4oNTbssm0FW1X5tdm/4eZE9WJCMmnFKBjD+tUYH8yfEndyvLymiCX5pt\n",
       "JM9T9xCVReeDelE4YVSCLbIRYDZgyEYoGtp7SOWBA/8sReKouDKSgaMNdL9/6BFSErn3rV+kZcJh\n",
       "ZCrtJ+4U+Mh2F5T+P93hniKhs6KJb2V4QEnxZmrNVOnwD+D85xgpZ0/uACzT1n3sAt3jrhLFiur9\n",
       "oVWwYQo2hpw7UZC6S5AzTbbwb4EVBgmPLZwsEfQxBvpayUuTcQrMicXVPqq6vGo2OaIysPz/8LHG\n",
       "mQmtCdWUlnGLhgL08Ik1L5kzEq7NXvfvJo8qi4nr9U2TwxDekP4YGpcQfK32MukkKBtReJ8ZS/l9\n",
       "g1Lr++2j1AfcmvawIc7A3GAGBVrOv/mC6oh+9d347nBFyF2dFaOBUwAAAhxBnoZFESwr/wAAAwO2\n",
       "yM2FedDTM6eGSfoVcOyr31X60c+Z3O3ah3jV0a0gPanSpVSRmGf0VCzbUhiAC6iR4hUAyCwOxezn\n",
       "7Ir3T9nzSXFaUfIXyRAEgqtz9ijztzchepm4+qW/rXabO48FSxwhbGp2fkbrBo3c60U1c6ev51o+\n",
       "bT4kSQfMpxHQIB/ba2IvU+DCROh//Mwn/1vyy9JO7fNwrfzEU4Gr0W9IazNr+48Yj1UAAYYowczF\n",
       "f1zfJbZ9u1uwRjLB0IklUayPSumRaFKGWBsB6Tm+Ge33KqERsf8h3poPkvdprYLVUZBC3Ck9Tyel\n",
       "Nif0VouhC12MzmFKEISlCj7oNSY3USvuzm4l5m90kuI+7gjQV4JjOsmC15x4MJbinx+Xvq2ioKpX\n",
       "yRUFMxW1EFvpLYJcvTUyiBwjR+CujLhn5OffzHgVYWH89ngcMsyrodLsI56P+jdAj6FShYawx6SK\n",
       "WdfxPm9goOWXoq1MHANCsd8IlLTlUw+4usvagtGc89Nbr3+Gr5E1+pLkLG1UZ0YT7ry705SwbXgU\n",
       "V2qJ9UCtuMhIhZDvVuDZ6TAvhG+jOa0eDM8zDGa189cJ3QIXQYFOvn5a03TACmwwsfEQBngxtvhV\n",
       "2frYwQcG8W1DSrs6kflsKkRAErPh8w6JEHSA/V9RpOWCn34RekhPpnDrrUU8lG4YalCU2cwHh0vq\n",
       "mvC/ORcvDeFZt1moh4EAAAGsAZ6ldEJ/AAAEtapXIT4o6SMmABHOgVCbJQTfj21U5TtRerqIWd5o\n",
       "jAL05UYYQe+VstsXUuu9KgPLFqjtlGQR+w7NVcwbAAqJTl20ibRq4OHmkB9rB7O9DBG49LiuS6kQ\n",
       "Lp2/O4OT1tfIfCqqKtmjO6iyZpMpaSqa1ji9/16DMTnVv4hwQBPFlC64noz1dbPWMF7Mjkj9h302\n",
       "/R1nbRLpPYaKr4jx4AArK03irNP2d/QY5VZ7qL2JBDC31Y9yCHvOT8ylEBOCjcfJdrK0qZ4jmOQL\n",
       "kk3Kwe7F8v3zbkuACBnrXBQA0DepKV7RiRJ+MDzfLe1XfwDMCxVjbckNeVY1vKxGUs2cnVuXHbbQ\n",
       "OvKnv1n4fMK32Il5gPMJhy8ukJNZEicpnUX82z9u/bYameh9+rOd8O0bJyQzbS6UuzLGZ5AlTfZs\n",
       "qEBBQLzq/8168KJJXejeqRPrrS4BgAptkHIk87Qv/8wlDaKUwfy0FCsAcYZQq0sUbqSTrU/NnUY/\n",
       "QdtsizXtAXMmVLl/0evcWX85b+M2U3187Jag9uSKa/lO7mxdvgEBrkfQ6jmeEHEAAAFfAZ6nakJ/\n",
       "AAAE13DGSVf/f1kwAGqgRWxM5/XjFd8VDRAp+ngSddy51Q+cVRPll1mlIFfHrD7eY8JcdTF/ymfI\n",
       "PYiSBa1p9piaVKAssi4Bu99Cy/N3zZZ5Jc6XwAAzZnEYOMXKZKSbhbobpFonb71KoOKnJL5M7WDW\n",
       "lV02ljI2hew36uozKrpPqFst/3dxJQkhsgrPis5ejoaRIAy4gjPkZ0jsSxpYxTzYPjCHpeaX2z23\n",
       "Mnax0m11mnP0Q4DBNpZCf4vyEXxniBw5lo64D1jL6ofsOvg4vVRVo2FzrLdrM8gq63TiKHjZL4TU\n",
       "BQUd5BuDfWNDedQZHarYhJlo1KrvPGVm9UqDxGElPPGVaLTvVBSRtkCdGuJcjhBeV5HOSoIxWNty\n",
       "EtxLMkjYe+UP0J5Ln+CPMF61BI7nJxcLDcuOrrnST3s+D2Ckz+Fd0dp3bfJw/60zRh531vgi1CnJ\n",
       "ALaAAAAEzUGarEmoQWyZTAhv//6nhAAABJvkbHBRXaKBNd4XohUxvOawwGHFGEicuxNiC+PBbY6c\n",
       "klXmbbIbj6d0NgiX5VSY7X68D7DK+vUCpM2GesmTrvAd7vbH2bTfxUzjaO7nfAiqH3bErMFTnxmA\n",
       "+OX7HTAUsbjNq8uCPsK6ZymDStX1sutWyebUi0m5kaSD3dBKSeVOaPVM/9zQZ9jH8/+P1CEtP422\n",
       "DZTekBIn/9FrWD5o69rAixUlt2nBiCVEJed1poPa8NmhsEYtb6ohwStyIWBSd1e92aoVfRa7pjiC\n",
       "PsGIGWDmyN7VkWILIcHOGjwaGv/uq5/fbkhgC1Ooc/+PQzwvZi/pIJgQjg7Jqq26B0ONb06BT40J\n",
       "PwcDOkdchvm4gn6TthcKfznGhjOjsLKuXd2V/fWkiMMFJ+kD0ynjBO3Fb4Xf04qMwr79xWlXdcOu\n",
       "OUgDE5CUuSMDCVZ5G1UuJUVW/Wol/O0bgkCtcpm3g/RH+vj12rocJKNpRQVZn5u9rSgvTT3KzyP5\n",
       "+dfD3CJJubkXdGhqdT5KF1Za9fXsR9SYgv/nF5CGYRuz0LnPlJRppGCwjxCDJe+Wb9vtXGYEo4OL\n",
       "U4gdOz0aCrGLGsp7c8PTWHMgtNspEIZaMod8ZD4hnGtI2nfiDIttgsRvniHsS9l8TUC51+nMO225\n",
       "efXsNnFTHATaiv5MbRFU1QD1ImPmMH0x8WM42a2hbifagD/NHGnaq1O0Htrqp1oHOrVFCUvgjxSA\n",
       "UslC9lanNBOhpetWAkNVJab0Jbeom2fTsdA09Oz3+khJEMI/tG5B3GMd7NzXmy55PLdIq2EQaPSH\n",
       "43cABVgjEZblKQUhGy9+ruakCEKs/C6xMBwv0gb0h1xzespFYlwiv7e9VrXi5ec2hPK6whilfC/N\n",
       "6m2Ev9Id6mbMVrYCm5ViGOGAmmpaPzC9cdKT7thzhzMnDolyKC7rw2T0pKPEZ2im8zKNrT4qOPCR\n",
       "iQs4m49mWYRVDnDLyxmimpiDQeSc7GZ5LFtlPcEJ2thqd/irj79s8xV56Sh7AL7uPUho5SoX7Dc9\n",
       "48WdeysGLA0vVGIBNmi/V5wunZkrQgzPNR9ZCsmCwjYeuqmki5uICoIZfJp1HPeXMe1JulGvsbld\n",
       "Hg4y79VIDMOx/Hanfnv8prCifl/VJLndwox+3xLshoyx8puHLcCbE3AptVJTIciIowPA6sH5jxG0\n",
       "X9ykCAV2/kfaNdZiun/EBkQIaEFGsJAZ59UB9o7IDXJa9mwCsCV1BxevobviP8UFuJqqYiRJjfNf\n",
       "N4aisA2OO0DYNJImtaJ1t2gmQg3BxQIjOhwVPAaAgwE6pr4+H0OlZRpnyhv6t/4vxKnWmCGk8StH\n",
       "3k0zRdVLTc4+sjG0dPhxJf8oDeAJAkcKPrX3Hcb4aAC3h+qnfulKXxhyIMU0GpCRy/vZ7UXorvXw\n",
       "DeNR22RypqiYRElQGDXwqJSSuXs5jmjb2VNIO3TW7K59cwRK3a9wkqI2xMTnLst+8GX3KwS3pshg\n",
       "weaHBF4FB6r7oL1FOPm2LJv7jniLGpIQ2aypuiUx7Rk0T75bm//rQipvbyDLe+0dunuyhiMBKboH\n",
       "lhJ25D40O0llXxMg9TKS8auBjVn2Q8a7Xu2l9U2EvzKCU50yvdyOAAABIUGeykUVLCv/AAADA7YH\n",
       "haHs/yutd7X7MAFsCW55zBQ9sUr0spdjmS0pNXnBiSngg49CEXiHGlCSIoZJZST/wO436kTVUFwa\n",
       "nBnW71hXmjeIaw07MuKhEF0jgh9kDs7G1UpCcv617qoSLxKIkeK0crEfg0J2h3yFT7miCq3OZ1VC\n",
       "IBH3A2jsCe46mSBsyWIrWJdS2HMDzDfgHBdIe+roZ+fT80m83/675q89bWcQADF6SxX8EA6235a/\n",
       "BuObl+tYwFvM2Q0I3HR0EoAHODnnxy6RUqVmg/clrdUgUPvQJbCRQx+OgOXAoYnRDnEdKMUCC+FU\n",
       "ciYyb2OulTX2jHo8/3G8f5kyRFJygRapDzI3d7mI1+G0KgIn5la9449VJBFg/IEAAADVAZ7pdEJ/\n",
       "AAAE1akoCCGsBlFbOAADXMc3ZE2hgP1ihl3VsWZ4HLLZ0FC38y0wOVrD08olc24Njm/Tl+LSJOX+\n",
       "QyQVhVnJdQ1qoC/g+VJ7dNx+NygfzHIVviic8vFiQzYV8vpYAWkQLcvCeUEJnLpsEInwVEEjK7KT\n",
       "jfPwFbC7g5O+CYyd8cam6y7Ve4NQ7NWgWU7m8VvozrteJIRR5XR2ek0nSMudTYlkmvPwmz3qYf5a\n",
       "ff7IsoWrLZ1pjqxvayrYkSlsgpH5ZCcKgUa+SFdilVFrYIeAAAABBAGe62pCfwAAAwHFdTb7lh/i\n",
       "XPtP8gAQuim7L/OA2mtENy9jl+SZGSlUwGcpG228QQNBTgbzr7f+DPIVL2wwfLq6NtEtg/cYcycK\n",
       "T5eznZ8WufPHpIR++lxENl3Ntti0+o5QeZHC3hMlC26YhD/iRHEaSKE6v07jsQTNhf0576AgG9zK\n",
       "3bAI4o/VX53Kh4eHuC1m7EQj+qEc80bPHWC9EBXk24vLr+dwKgCr1akDSbHG5ETbS94f/S+gBVMD\n",
       "RjBhdwOZAxLOzoZKqZ4mvn+bwd/9n+UCBXH8nNBvzobWKmoT/IRMvqFKYFpzvzF4WM8jJyLh6jLK\n",
       "v8RgcOgJlT1zDagYmAcsAAADnkGa8EmoQWyZTAhv//6nhAAAAwL7WK4c+E98ioaK4Eu71wA3Yli+\n",
       "Djh39qTkuB85DPG4sd1wMyKlOJQ0UpyXJCcZusVq+OWD7ehGndlsWdpmTH+u/soq5mw0yAluMYiu\n",
       "xUYNzEnckmo0yyzhQfsAq2+6BWKkX7OdNZKRCPfA8OxQoV462IDUEawUrXzIzS5mF0hkRn5BJQwb\n",
       "kw7UZq6bJn+krM9Ks/4RyQTwQkOk6Ra0smHQe/g5cZ23jJXHf9SsqM7fx34rF0DcFcR5Si6pX2uO\n",
       "bcEEt4mt93rbm473yrLN84LYonZJXBcN2G44NG2nxF/S8PZJy+qphQlGEhiE+fIKE/ptSVvAoENQ\n",
       "/5vHCPAKWQ/o31fAZP4uX8/aKwOr68miF2G6AIhtZ/fGmTNJPbuK8MQeHrtlSlKfKdzmWVNhjovu\n",
       "Sid9Wl3ck9enhxZ2OAPZcKnu0Rif7CYv+LP1ThmTpfy0qOb1Jc4cN2M8Az02QQ2zQyH8lzap/G0c\n",
       "40QGaRXdghn/00thtydZnOGZgCeuVLnraFy/Qhcz1JusGt9n8ac2PA8bJGySxihmxhRXkzoDp4K9\n",
       "ihVQCo184ixQ+UisZTPRABvMe1gsm4aeTLnbUZ51p5oi+jsCA/rIK1AZLhQFoTdyWgI47YN2bZzs\n",
       "f7sBkZHZLd68lYtwO0iXaH2wPxoetYUTrSqHRe72YsJFgfMDISq9KHe3JMBH+T4BZ8tI84oYJ04x\n",
       "cCo/HLqekl/yi+UV/7LhIajDUricoGgSDinFRFL5JOnQzN42JwZ60e8XBqSu9OVFnQoCzuFRoeD0\n",
       "SBp3AKsmv6y5mco6zh9kYm+PKUQs4O/ElT8T2Rq//1vwB7DqLbGXibkrUrFAUidaNGXXvpxIZqV4\n",
       "Z04qdx4Vbhfc59j1O2FbKWFPMyMRbYhlKQpsu74/I7CoCoVJjfLshuNPbN1/nLtGUquXtLJiol68\n",
       "3rjtSWXIUpQMs9nJV1R3kzop2jURjqGJmMwNwP7P157bYK0wpTig0Xacy6azXbpMxnrGJjl2uuep\n",
       "JpLtIn1jCgiKVfHiUXZ2nFlfANBaSZhPlg8xEHun71qBSRdk7RsVpKHCvMqPo9Y4ig0c8pjBxL4z\n",
       "X1nYp8Q4+urL2CDeRAlPNHJqCKWjvjqFEsB79PHgXXItYHJAHjlVOkueIvL711Lt9Gk+6xfPAuIs\n",
       "FEbt1oAJAYJqY8aZL2HZu7elzLIM3XvqRhwelO/4AHzBAAABKkGfDkUVLCv/AAADA7bK05xBkaSI\n",
       "lKO1FTAAfnqGAAmbZf8JBcXh/XezA/QLSs6YoAqJFcCvvVogXJtYfTlopSmeHVw/LpCDfHM9EViB\n",
       "jia7LpE+QeHi7xUcd8WGXBTYPR56p3TvFhdu2h5InJVYqVDP1COgrMeAEFr4wVMWD12DCyMz8iXn\n",
       "Djr/skDxfZdljQctZW5880RALc+dizd9MF+yTiYzoftkHhCAbj24Rz8Bgi868YUBE/cabOaq/7JD\n",
       "dr6wqR1MPhmsbkgf/v/GhUZ7R3aBqTHRvLg/qlXAwfl8RyvU5zf8dHJ4tjbH+rhMOh/EnZx2RgIr\n",
       "ai2dRWaTIxMlFBQ+d88kbgwguQjNJXr+QWfeo/LcOpr88dS2qzUZRQ8miVMEPMmId0EAAABoAZ8t\n",
       "dEJ/AAADAyPk+hIpk9zaKzu3xuxPV9HDsgveeeAEAyhmeV6bUuGAHwMMLAHc1mUE9lP02qO3yMEr\n",
       "BAYwY+PWwoF5YWcK0bB2U9yhbzW4rwQfv/iJX39uCuwtC48MbxlfYXRAAxcAAACWAZ8vakJ/AAAD\n",
       "AyOza44jgSV4eC3r1Gh6sw/ZF7dvzk2uADP3B0AYicZGEAbhQaFQCHnkx3PTIz/pg2mU992jTFIa\n",
       "07cTXyp5FUK1suShyKgL4eJvBYhldkr4izTUWF9lnmHfc8WxSGTUMtkMM58IoywYatRqhhAI5WJp\n",
       "43bQs6nMf/c2KxzshokJFvn0Wqo1XaFkWAz4AAACV0GbNEmoQWyZTAhn//6eEAAAC53UMgAra92o\n",
       "Muu/1a3RpCHqkSuhOowfAV4A3ZLI2sqrIScbo6NGA7DH6K0wGjTPbg/X7OkKzU+pVrEcoZvr/GTO\n",
       "SpUzCdeOCd6wtIYpnKKxzjhUKl3hj8Rld7aBa5XaWkIrnV1lAReofhIxlSQjkCfwLT/GpRqWIt5q\n",
       "CQJdwHTF3FpWQPAGbuFLTy1bgmnm53a0ZPdVur+77eL0iZRrt/jAzRKCPUoF+f7PuxyLFbav++gW\n",
       "UUqcjm1Lp24qt5da62ly69ouwHjfCx8pQY9yNE6NUHoZZOAqTU3pA5yM6k/ZafmdmM7iXh+37WTq\n",
       "INChXOE5xc3nXRgSRd/Mv1qV3mHbTRCbsao6WhtduG+eLtPqeAdL6ODCWeMvbJcZrWZwpwBSH/sc\n",
       "khgys1CmTewma6yNeIPEmQKLVkwO/wP5ixmjvQJd6Key3BvBa+BOZBUlH1Nz7PfafQOvNqf1mYQo\n",
       "Ujawglhof2jpyOMZ9BxgWSMAQG6JLoWHF/zusk4gyek+4y+VE8pfFjd9X9AWxpn9fD0NbjfMkfqE\n",
       "bwXjKJyvBBsifTJU7XU7unrMGT21rAIPz0p6Ww6yYt5idq8Ysm6pZeMBnzlbPI393Z23BR21oPyM\n",
       "FrMhgISawvokOXWymp4JrKIPbAPwdnQstJS3GdjynUIi9uYBFT60erfJXwcZPER1smpdn9Bh/b5e\n",
       "RkEuyHxJpiV73mXa5ZoGvA5rEqZjLJ14qYjOHilWPMoGYaIaUvnmUkLOe0xqxsoXKFGS7StxShmd\n",
       "AAIuAAABKkGfUkUVLCv/AAADAmsIJv/+eo0vhxCxh7G1NC8aaZqegdFgBIbqw3xZ4HKB7YJURIFN\n",
       "v5n20/IHjKa328lmDO4hPZwLZViZ94YH/rza7+NpTeKgRDW3PIt0DewpvuXycXnP7yobv4WSIz2y\n",
       "9nFGYiqzHJewn83m4SdT+7ifXbJvfO37QHMlBiskCgXt8ol/dpOVIJp7LaDsUCbN75z9F1OtfQl1\n",
       "PozOgbE4p8hgg0FCRyOIYAg7d50MmlsHxHnAgCSqpHEX2zzctThPsfptXxov/A7p4pHL/49C3XuI\n",
       "isiJpdJDhfsofdTSw4qYlMRq6oSs/B63QEM4rz1kSHpXDcoerD1uNGZjdu5xWgENnukLKo/LSOqC\n",
       "IfnqWmIZpw3InbB53PFDJWDPDZkAAAC8AZ9xdEJ/AAADAR1qcs+8ZzpxknJ/Ah4Qts83nUZgBs9X\n",
       "PWsUn1oR+b2fz76t954Q3lCybp6xU5tkeCmLgs0h4FgRzNPCkGPcf4FygHw+VeLjEB7ZPnRjYo3i\n",
       "ziWWTvi7U7SUe0pp6YUYRHw4yrFPMtfUO7UiD0Gt1q4YmzgNKPKpXXCPqM/lfwwm0EDATu3FQ95p\n",
       "Qk6jY0FvQ34Y7J5v+EHa43llLr9aqTkrDFT5VzLClMr3m9uYBSAAh4AAAADWAZ9zakJ/AAADAxDw\n",
       "+hrzTYYq8NwXrfPhIANgTxj64lgaAR49uT+FgCEG370K2UkZs2+rXdVmL+cpZ63+YB4qtm1AHRxh\n",
       "tsvgFUKbVIHc/uLfzpX1TY/10MhUXNEnLu4vt4gaI1NRpXCw7tbs9s7mQqnU3qffoRYgddm4+oIu\n",
       "ywKlxcw84Um7P6WLWDh37Uz2KnDijjcx4EVzF6KPNK3ukXzyxG3vwZJrFySPXFpPYUQWNwpmYysG\n",
       "bA4hC+S5+1ijdJK6ObBg/5nY2lZ25Zq/ELLAkIACpgAAA5ZBm3hJqEFsmUwIX//+jLAAAAvHTQPl\n",
       "XaJmPr7FpEw4P2T4A/aplfi4fKZuAT1sr1wC4+MYGsmea7JbUGA84IJzCfSxmhgdZlhLG0oHvV9u\n",
       "6ubpmmNAbJZhGkGU6WvujV+iAlKA5ro62kmV3P0f7hgPGyMO1ByE/EozsVwuFGOgZryGA8tcvMGO\n",
       "RrDVoxLFVoS6EXcokrK5OCBO94x3q+atu4kWXz2NIRQzWWgzK0Bxp3U9UYZM5oW1kpCXF0xbdJs1\n",
       "bmnfT/4+zC/4sU2uSIVtGZQ/+x33HHM1dmMsW0ntvRrX7IUk2MNEiD2KQHZvPaogu90eeUlj4xTB\n",
       "jqJ+PGBycQf0Kx0W8hfKgtXWzDQCExk3fYMCstesp4PL+jo8RJtr1+ifIHDIVd56en4sY6Ds2kPL\n",
       "bVUbe7c0Jp446tI4vxpMXom00ltSyIXaKAPkqXVQGuVWgtYqPhvKBRUl/RZiabO7MMuIfS2MNu7o\n",
       "VYlp/PnX21rWUXP3v9/qwYJ1ATFFip2hf5FhRZYu6Pnz0vejuLaNDzeqom0RPSgv0H2kpOTwNBDn\n",
       "peqOWnSdD7zE3WSjpNvO+px4mnTwoHSvzyhN2VuOqNRNpUpkCyUIw/d3WgoUYfFGcMXpEyVvRhub\n",
       "1w1AuuElB/T//ksWzOKWJ/u1xoX4gO/HJWb6OHh1aEHzLVV9Aa6/7PhKenaaXSg3qHIASddR8C3S\n",
       "7MxiBT8C5MTCUE+hsuV9k2gAfF1fCiQTwQ2SoBE5icF2VPY1BnQGFDSrnk3p9ifS6YME1mKBpffV\n",
       "mZr4FM0hBYMYFIZ45xPoUEv2OIE7d3Tz5QCqJ88wapSeqj4cFOHYQn0Ed29qzAXUDTH+Vbg9Y01k\n",
       "3xcBRn7YWZsgo9O0engyqPIbRO6F2VSsqFk3vseYMcdGYLaU191UR9jnLFbkTv/SAtanLDe/s2hy\n",
       "iYVmxp7/9yqQOWbK+4XYXf64Kb5jJMwhZFWAnQ6eykVCIukmpJtNy6F1LB4wDB4HzegQ2A4RKLqE\n",
       "XHhzomm87CVhhaPC9LTlpw54sUReVKt1dYEWS4omo2efqCVnpUHIjjdzO1YwxuWi+CIu87bnhrnH\n",
       "o6BA8S8W/gJTIHQVLyTvnrV5s8D6MtSAvuCO+QwVnxgbuHUF/5yRw3ickFDdbGPTCN0pVZmtlemf\n",
       "3Jg5SmmimT3ePtUfkmkG5bZA26Lz1blJNJpvaJm9OUrqJiI8M1ZH2sEAAAHvQZ+WRRUsK/8AAAMD\n",
       "gKOnCrnJba4Gwd+IVMJIQAjB3fQskZ3f/HpoiULZeEdIAgg3qU+WUEtNZWqteqZgGuVzxL0oKW2f\n",
       "/wzZa3BCxrLZzEgQsmk5MHCv5dZP5Ebj4OZhAGPBHfp+vEW4LmEt4fKgUo/FTHWlU7+X9xnIb7ra\n",
       "O3Xnk4lKCq0Wftb7n94qdn7XYtYOcPFK+cu7Luc2cn0scCkJ1xW4r+2JkFYbPlbK7eiJUIzAbbnv\n",
       "krYpBp9OWMxzj7BkCcNElvQTdsDZGe95dH4qST/p+Hgr9wXbpQZdM3RReN+MkpTKXPzckQFcWipw\n",
       "5RiST+zIbQ4rpsr5G+0MHhLy9bgrP9/v9JqgF2/ihmz6dWFSIIqqffphk19VLrOpAJJzfuCP+Ang\n",
       "uBt4kt1Ha6zEpfVVkwABLjkP+AwaNfUuhu3K5COsnxtmZVEmvNH17viJpZ+Odr8G0Gs+r64KAZgJ\n",
       "Euex8s2zaz4EftRTb7f3dR7o6nLtYnGyUTxeX5fvcVE4OmmgkTGFzthWypCo89wSVBS4l6ThCAXO\n",
       "nquqSMB3AuJZpPs0fLECmJJsI4WRMHeLzhmfQDbIRDz0eKBAssMdrLmuPv33kC8zvdahr1HgDMZz\n",
       "2NE8iM7QV0jvuxb3rw5xxXRsAabg1N6rgAlYAAABKAGftXRCfwAABJWqVH0Gakq9Cn2zIAFku/z/\n",
       "A0j3lfbwvpDl8f4kWYBe69BEIP0ONWvca2zX9f/Ejqlj0guh7yXkdse5p1kusVYE6H9KycZAWvVl\n",
       "OR0UP78ReJfNZ6cUgmjzucqbqG3N2CRpjiee2CQhHoh95wImY0xoi8ZooEwJ8NiaHwtJsHbQrXAZ\n",
       "FDpcLR6CEZQn3YVRfxNqSxvumu65DQxbR+Mu6M6E9pPpb90yh9YSTkrQ0rugAj8tqoPzBxpGpE8O\n",
       "CA3PcP5i8TgoOjb4h7F1evClR05n0pPeTWP7rlnNhhl6MISmkWyFD5ZSn3X4bCm0jwku+ImMY3Zh\n",
       "AOMclnA8aAnOxZXxdpQyShsW2qHMeNUtw/PI7ipqUXtDLtJQKsr7agHdAAACMQGft2pCfwAABJdx\n",
       "FBz1Km0mmKioW8lbBFLAB1NQBi+jF9DyLAuTGyK0/a00EzWSFYjLS9DGDSVrMihKk5Tb3PbjxKEP\n",
       "JfE2VAArbGeEjOgVCGU9QSM9GaCgsfzSrlCYlgofEbWzf65mzd9rbS8HEd6eVLvLs1xfsjE4vgII\n",
       "KmxojX7dj8Z1c4orTlugO4n6mEmTM7ikw699W/HgWImTHvs0/57xtFUhzpT6xspD+pX+Dm4BYjFb\n",
       "2ef7OathOHX4JnF3ZlgENvF642V6dg/qI2YqB1oEeQ0CLuLc1u+CE9bVht0wiJZfMLxxuUd3o6l2\n",
       "KcbPdNZ9fUSzEQjU4Ob26YF2IrjXGqcmygAuFyDKKOgFM5hn3pM2besSMumUNgoLesmLMb/FdFXo\n",
       "ytreQ5O5wj+zSRimhWXbRH+sN1gpouNOtVgPqY0TwEKL7T2LzSaMDH/sYdoZ4PiOv7rI1RgtDHjr\n",
       "fx/b0YRQGQjO/ZW44RVwwnQRhbSj1IfbUREmIQKmGV2oQ2nmsNknl17Oq7gTeZ3hvrv91bZBUluQ\n",
       "glajUptxyfjeV2fU35i3aM3Iy9FTXyTct+Bk+NmW8Lc8oE5y8DIEfLHjri6r2wzYrSWDT/XEy4KR\n",
       "BGWo+tePqj628y3XRIWoZwYWrZf5z9p0TYxIMni0HPtauQobbQhLrPifIlDBjV+hvDaobN177iRS\n",
       "swpmGSu7e4E6PUGBbeibzV28AJVymQNsuhHv9THx2S3kPQCtgQAAAvVBm7xJqEFsmUwIT//98QAA\n",
       "AwBu/TdwO8hQ98McZELT1tV3oAIgz1P1nc5YMtUrsFVsHcyDZPceVqnnR0elc1woCklThIlwBKPq\n",
       "Tx6v2hpRh6PTVaY9jzNbve5giR8NEz+uDj6OkRnacdi99NlTRhCpH6Ul7jKKxSojNICcBzrL+dpy\n",
       "IedW4Jes8RtKh+CCgpbXlQUHRFTgYMvqtk6hxuIV6Spa4LvbJ2+OVCqT7Eg7fxEooRcA3EOS1wm1\n",
       "nqkn6QJw3DXIKZRxkZoPg3ubCRPgAMns5kmgdGYG06kacxlSQit5+D4uMOl083wo3ne+WcQAbOfD\n",
       "D5rl2FaQCt9qVd1JKnbdXOwo2c0UJk+LIEnoKvi5cEVDieTcduNsxnUCZQFayfl/AQeDXonYtBbz\n",
       "63MvTudzlm/Qq96t7H1Gc+ns34bCLd9m46/PFNc914rWab9Q8d+F3GTHldByaj/yaPaHCO+cbJei\n",
       "lYBiWh1cYkBcbH32YGp8wke7KmL34juAnGFeh18T+rrl3nbedEg35smObbjsb3fRtfXu+gVYbkn2\n",
       "s3XBVis5iBCmxb6g/zc027gpOqCycuuJJtmgL2FhPCoj0e7Osq9Q+5NnYumWA9Os35qihewBLSIk\n",
       "Ml388y/iZnI5cFuKL/ITIdi4XptgF2ZYKB+0YtksuOwKJYGgPMefv/Iabzm+PZcr14zSCwgcblUk\n",
       "VGrdSntltAxQ0EQtaLFpBgtRuv3RMc6CUWNScfw+w09c2P6QtPTkd4eRRFWRwts4PQakxyIhEYY4\n",
       "lLzyCWPCR072E2bLj1HflWsS8fPMytuvNl22G67kN8gFj+1+U5bfmJkn6+Y2RnN/+dQWSnKNoTGF\n",
       "5833JDIoHLPPzrGykhOAynB7XuPMeArLQrvJQe25RERizgikhiNyGXiQJgtcgO1N7f0XKlDlSnCt\n",
       "gbwDt3j8SkYct08JpltKuS/AjLUko7dh3MbZP8W40SY0qS0VMKMk1Kb5MKQ5aSSlaYUo7UP3kA9I\n",
       "AAACvEGf2kUVLCv/AAADA4teu/RkwmM5NR6AC7SEj+hl0dBCZ7f2hsIMYI5AwITLy3UTl8/MwghY\n",
       "ENXsA016n4UNNrpxIhVv9mrWJBqmg/GkF6E6TRfx8Pfwe2inSa/kcAxsxzOvc7JFC7M96duA3kj7\n",
       "NisfrR6z+om9za1ZKI4QcnoIkvTLQUBeNuluc4FJI210nomJ5gLvN+U9lFD3gCVHbmKkegXuIeu0\n",
       "oTAoakdBHPQ2KX9RR3YHMuQjoDX9dUD3N3MvrHrFAINvFVORxjV4HkQLm86smCl7neRL8X+xlm1n\n",
       "JZWOGt+MKw0Vsj9cCgGkKKsDKxlP7ACqdFEHdFpu/gHYysb47AuRwHs/J8oh584qPgqJRauSnJ+h\n",
       "qXy8oocfhP9l3mYy5OJo/nhI7OFjv4DLKaxJGhDN5/74NwqoCS7z/q2iMjSJbW5j11ckM2C+s3Jn\n",
       "j8iOOU7/CXe5A+tl7gkU+5eaeDIqY6DJls48cAj0Bem2/nGAO/Ms0l1V4WAAHGknIHrpwicgOby8\n",
       "braWsboZi9um1rczhGFj0KrtXed93NpnOMO3Ft7QpxdGZRjuy1aJeI0q5FlGLthYmNj0vV+DLz2A\n",
       "Bt0aPux1xarQ3sEPNlNaYTYmjG6ffwsPN+p2yCqkj7XNrUvLvYODhpa1QnVG6bomvZeBj5bOCjcc\n",
       "oStuqpsEjWb0cLKVdyZEPkHj9iKCZu2cYfS6ranF4YBK0R68DVWdKaqCKj/krOl3gcSTumR4OL+q\n",
       "Ndi/RfNh9fjzp+3Xja8hkmHiREvO6Jr0Qi1Fqt/jo6GVzE95k8fZ9pkvL20ZivHKdlzuxmRHLVe4\n",
       "+uy/VjJtzPH/10LzMK1CvCIRjRSzaCBr1S04yThjoAJrk59UtM44IqYAt6k1KVVjmlAmXMc2qRk1\n",
       "Orc1aRwJV6BKSz0Ut3fxZwuAwIEAAAHTAZ/5dEJ/AAADAySCpwweDec/O4hy+NKdkD+oAL1vUOCF\n",
       "rzd/6JZCrhiIVa383GNtZWq/x+bUKtmKaR5AjWEHHJIlTSIMvrbgbs9oHj85Kk+//jhGbmdSSaE1\n",
       "Hums/X1tEE6LzTirnkjQhCYj5fo8M91gMHaBW2o1lDfDV8tHgMz+2WY+1Qj1pQsLN3fux0qR8WTU\n",
       "xNYKOTfIT9CeaTGRGicocoWptkDeM+8H4llR3dW2tXTwzVle2yRtrVX6jNv1zXAglFjHjGUYZPpg\n",
       "yKi6x/iPjjzR1ttleoz15fIvBHm0EzXWSNVzpJ5D6gXrDpD9iFcCU+YsjfW+uZ5WajzBfxO+V78Z\n",
       "fICu1sEXY109g3lK4IBFkagr/TFeXp2OcvbN79UXuSsmvBPw/Xc7w3pfEvsckZeh61wFdX+sOKD2\n",
       "iUNC1OmXQg0AvMULXHYw3qXoxMCgyGE7en9sBr8yfbk7CYjiQS4xhtTZSDW4LFRFacBg7/QXkpaM\n",
       "wkEsJingZD+NR4l3VqRAMmK7Po20sgBSO0BzYmlfv0tyRF5dq+rJcKjyoCcTJiRdW8HQnPJh0WsK\n",
       "4AvxjyKvdQHGGqVrg9L0s0i1zCWkq8nkoZpOq7IHPpAAZ8AAAAHbAZ/7akJ/AAADAbmOpahYCTtk\n",
       "wAJIj6d0bvw63h6vUaLXaNzB0Y2y201VXO+ejHXy00EQe7NjuRU8O3fr0pfB1aNUFbAAqLjxFAx4\n",
       "wP0PCNzJsNhlbfQOxWwUTBoPMOyWgQy4fP0o6xMDbMyIEsFUZnqQ4aoMwmIJCq/xfKZLZvWbAIQu\n",
       "KbO4WKhe2sEQUg/GGViF4HfqBOvTLKUVDEhDOEfe+ES472yEfycj+9MplrwOfd08F3ZCZ7slVreO\n",
       "pkZck2QgdSOjz7aVeFHDVl4ADaot3BPjlpbFyDg+e4GHaDJKWb0P/1vd2wkt7rJo4C5Mq+g8wBin\n",
       "hsQqo0N0MIaYGERczfaUoBObGzYM/AVc+hH/Mgoh1c9wz/bgyIhCXWsYC6qIQ0Be+iynXgAItIUZ\n",
       "tlKuwoR4etL6go/DhtRW0nAw6xkbiI+jW8ztuvqsL0TINSShtDIThKAS/yG7xoU4Y4TcL2UowIhG\n",
       "pN/2Hjbj58hlS3uH5dVfUCg/XNWePkitSfMnXAClxpfss+VKrm+k2RyJaCSku3FCm8lygvNq477U\n",
       "27aHFJqYAEVsN6iq/Ubq1DRK4qBLUoEfPxE8pQl8Q0/yDGdKhmga+SeGVpgnB/fckh2pVpMvjIBd\n",
       "wQAABIptb292AAAAbG12aGQAAAAAAAAAAAAAAAAAAAPoAAADZgABAAABAAAAAAAAAAAAAAAAAQAA\n",
       "AAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
       "AAACAAADtHRyYWsAAABcdGtoZAAAAAMAAAAAAAAAAAAAAAEAAAAAAAADZgAAAAAAAAAAAAAAAAAA\n",
       "AAAAAQAAAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAEAAAAACgAAAAeAAAAAAACRlZHRzAAAA\n",
       "HGVsc3QAAAAAAAAAAQAAA2YAAAMAAAEAAAAAAyxtZGlhAAAAIG1kaGQAAAAAAAAAAAAAAAAAADIA\n",
       "AAArgFXEAAAAAAAtaGRscgAAAAAAAAAAdmlkZQAAAAAAAAAAAAAAAFZpZGVvSGFuZGxlcgAAAALX\n",
       "bWluZgAAABR2bWhkAAAAAQAAAAAAAAAAAAAAJGRpbmYAAAAcZHJlZgAAAAAAAAABAAAADHVybCAA\n",
       "AAABAAACl3N0YmwAAACzc3RzZAAAAAAAAAABAAAAo2F2YzEAAAAAAAAAAQAAAAAAAAAAAAAAAAAA\n",
       "AAACgAHgAEgAAABIAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAY//8A\n",
       "AAAxYXZjQwFkAB7/4QAYZ2QAHqzZQKA9oQAAAwADAAADAMgPFi2WAQAGaOvjyyLAAAAAHHV1aWRr\n",
       "aEDyXyRPxbo5pRvPAyPzAAAAAAAAABhzdHRzAAAAAAAAAAEAAAAdAAABgAAAABRzdHNzAAAAAAAA\n",
       "AAEAAAABAAAA+GN0dHMAAAAAAAAAHQAAAAEAAAMAAAAAAQAAB4AAAAABAAADAAAAAAEAAAAAAAAA\n",
       "AQAAAYAAAAABAAAHgAAAAAEAAAMAAAAAAQAAAAAAAAABAAABgAAAAAEAAAeAAAAAAQAAAwAAAAAB\n",
       "AAAAAAAAAAEAAAGAAAAAAQAAB4AAAAABAAADAAAAAAEAAAAAAAAAAQAAAYAAAAABAAAHgAAAAAEA\n",
       "AAMAAAAAAQAAAAAAAAABAAABgAAAAAEAAAeAAAAAAQAAAwAAAAABAAAAAAAAAAEAAAGAAAAAAQAA\n",
       "B4AAAAABAAADAAAAAAEAAAAAAAAAAQAAAYAAAAAcc3RzYwAAAAAAAAABAAAAAQAAAB0AAAABAAAA\n",
       "iHN0c3oAAAAAAAAAAAAAAB0AACteAAAI7AAAA3gAAAJWAAACaAAABroAAAIgAAABsAAAAWMAAATR\n",
       "AAABJQAAANkAAAEIAAADogAAAS4AAABsAAAAmgAAAlsAAAEuAAAAwAAAANoAAAOaAAAB8wAAASwA\n",
       "AAI1AAAC+QAAAsAAAAHXAAAB3wAAABRzdGNvAAAAAAAAAAEAAAAsAAAAYnVkdGEAAABabWV0YQAA\n",
       "AAAAAAAhaGRscgAAAAAAAAAAbWRpcmFwcGwAAAAAAAAAAAAAAAAtaWxzdAAAACWpdG9vAAAAHWRh\n",
       "dGEAAAABAAAAAExhdmY1OC4yOS4xMDA=\n",
       "\">\n",
       "  Your browser does not support the video tag.\n",
       "</video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting video for comparison\n",
    "\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "#for i in range(5):\n",
    "sample_id = 20 #i\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)  # Create subplots with 1 row and 2 columns\n",
    "\n",
    "def unnormalize_all_frames(tensor):\n",
    "    unnorm_frames = []\n",
    "    for i in range(tensor.size(1)):\n",
    "        column = tensor[:, i]\n",
    "        unnorm_frame = utils.unnormalize_mean_std(column)\n",
    "        unnorm_frames.append(unnorm_frame)\n",
    "    # Stack the normalized columns back into a tensor\n",
    "    unnormalized_tensor = torch.stack(unnorm_frames, dim=1)\n",
    "    return unnormalized_tensor\n",
    "\n",
    "def draw_sample_video(n):\n",
    "    ax1.clear()\n",
    "    ax2.clear()\n",
    "    print('frame ', n)\n",
    "    artists = []\n",
    "    \n",
    "    tensor = x.to(dtype=torch.float32)[sample_id].cpu()\n",
    "    tensor = unnormalize_all_frames(tensor)\n",
    "    \n",
    "    # Plot the first data\n",
    "    utils.draw_from_tensor(tensor[:, n], ax1)\n",
    "    ax1.set_xlim(-2, 2)\n",
    "    ax1.set_ylim(2, -2)\n",
    "    ax1.set_aspect('equal')\n",
    "    ax1.set_title('Original')\n",
    "\n",
    "    tensor2 = x_hat.to(dtype=torch.float32)[sample_id].cpu()\n",
    "    tensor2 = unnormalize_all_frames(tensor2)\n",
    "    \n",
    "    # Plot the second data\n",
    "    utils.draw_from_tensor(tensor2[:, n], ax2)\n",
    "    ax2.set_xlim(-2, 2)\n",
    "    ax2.set_ylim(2, -2)\n",
    "    ax2.set_aspect('equal')\n",
    "    ax2.set_title('Reconstructed')\n",
    "    \n",
    "    #plt.show()\n",
    "    return artists\n",
    "\n",
    "ani = FuncAnimation(fig, draw_sample_video, frames=29, interval=30, blit=True)\n",
    "\n",
    "from IPython.display import HTML\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# It is much faster to draw single videos so keeping this\n",
    "def draw_gen_sample_image(n):\n",
    "    ax.clear()\n",
    "    artists = []\n",
    "    tensor2 = x_hat[sample_id].cpu()\n",
    "    utils.draw_from_tensor(tensor2[:,n], ax)\n",
    "    ax.set_xlim(-2, 2)\n",
    "    ax.set_ylim(2, -2)\n",
    "    ax.set_aspect('equal')\n",
    "    #plt.show()\n",
    "    return artists\n",
    "\n",
    "ani = FuncAnimation(fig, draw_gen_sample_image, frames=29, interval=30, blit=True)\n",
    "plt.show()\n",
    "\n",
    "from IPython.display import HTML\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'draw_gen_sample_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m#plt.show()\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m artists\n\u001b[0;32m---> 12\u001b[0m ani \u001b[38;5;241m=\u001b[39m FuncAnimation(fig, \u001b[43mdraw_gen_sample_image\u001b[49m, frames\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m29\u001b[39m, interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, blit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     13\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTML\n",
      "\u001b[0;31mNameError\u001b[0m: name 'draw_gen_sample_image' is not defined"
     ]
    }
   ],
   "source": [
    "def draw_gt_sample_image(n):\n",
    "    ax.clear()\n",
    "    artists = []\n",
    "    tensor2 = x[sample_id].cpu()\n",
    "    utils.draw_from_tensor(tensor2[:,n], ax)\n",
    "    ax.set_xlim(-2, 2)\n",
    "    ax.set_ylim(2, -2)\n",
    "    ax.set_aspect('equal')\n",
    "    #plt.show()\n",
    "    return artists\n",
    "\n",
    "ani = FuncAnimation(fig, draw_gen_sample_image, frames=29, interval=30, blit=True)\n",
    "plt.show()\n",
    "\n",
    "from IPython.display import HTML\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below is unchanged but "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_sample_image(x, postfix):\n",
    "  \n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Visualization of {}\".format(postfix))\n",
    "    plt.imshow(np.transpose(make_grid(x.detach().cpu(), padding=2, normalize=True), (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([112])\n",
      "torch.Size([56, 2])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([112])\n",
      "torch.Size([56, 2])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([112])\n",
      "torch.Size([56, 2])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([112])\n",
      "torch.Size([56, 2])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([112])\n",
      "torch.Size([56, 2])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(40):\n",
    "    draw_sample_image(x[i], \"Ground-truth images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([112])\n",
      "torch.Size([56, 2])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_sample_image(x_hat, \"Reconstructed images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo: Fix so we can sample from latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_random_sample_image(codebook, decoder, indices_shape):\n",
    "    \n",
    "    random_indices = torch.floor(torch.rand(indices_shape) * n_embeddings).long().to(DEVICE)\n",
    "    codes = codebook.retrieve_random_codebook(random_indices)\n",
    "    x_hat = decoder(codes.to(DEVICE))\n",
    "    \n",
    "    tensor2 = x_hat[0].cpu()\n",
    "    fig, ax = plt.subplots()\n",
    "    utils.draw_from_tensor(tensor2[:,0], ax)\n",
    "    ax.set_xlim(-2, 2)\n",
    "    ax.set_ylim(2, -2)\n",
    "    ax.set_aspect('equal')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-3, 2], but got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdraw_random_sample_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcodebook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[31], line 4\u001b[0m, in \u001b[0;36mdraw_random_sample_image\u001b[0;34m(codebook, decoder, indices_shape)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdraw_random_sample_image\u001b[39m(codebook, decoder, indices_shape):\n\u001b[1;32m      3\u001b[0m     random_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloor(torch\u001b[38;5;241m.\u001b[39mrand(indices_shape) \u001b[38;5;241m*\u001b[39m n_embeddings)\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m----> 4\u001b[0m     codes \u001b[38;5;241m=\u001b[39m \u001b[43mcodebook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve_random_codebook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     x_hat \u001b[38;5;241m=\u001b[39m decoder(codes\u001b[38;5;241m.\u001b[39mto(DEVICE))\n\u001b[1;32m      7\u001b[0m     tensor2 \u001b[38;5;241m=\u001b[39m x_hat[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\n",
      "Cell \u001b[0;32mIn[12], line 28\u001b[0m, in \u001b[0;36mVQEmbeddingEMA.retrieve_random_codebook\u001b[0;34m(self, random_indices)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mretrieve_random_codebook\u001b[39m(\u001b[38;5;28mself\u001b[39m, random_indices):\n\u001b[1;32m     27\u001b[0m     quantized \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39membedding(random_indices, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding)\n\u001b[0;32m---> 28\u001b[0m     quantized \u001b[38;5;241m=\u001b[39m \u001b[43mquantized\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m quantized\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-3, 2], but got 3)"
     ]
    }
   ],
   "source": [
    "draw_random_sample_image(codebook, decoder, indices_shape=(batch_size//2, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
